{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Введение в Natural Language Processing (NLP)\n",
    "\n",
    "# Что такое NLP?\n",
    "# Natural Language Processing (Обработка Естественного Языка) - это область\n",
    "# искусственного интеллекта (AI) и лингвистики, которая занимается\n",
    "# взаимодействием между компьютерами и человеческим (естественным) языком.\n",
    "# Цель - научить компьютеры \"понимать\", интерпретировать, генерировать\n",
    "# и манипулировать человеческим языком.\n",
    "\n",
    "# Цель NLP:\n",
    "# Преодолеть разрыв между человеческим общением и компьютерным пониманием.\n",
    "# Позволить машинам выполнять задачи, связанные с языком, такие как:\n",
    "# - Извлечение информации из текста.\n",
    "# - Перевод между языками.\n",
    "# - Ответы на вопросы.\n",
    "# - Генерация осмысленного текста.\n",
    "# - Анализ настроений и мнений.\n",
    "\n",
    "# Почему NLP - это сложно?\n",
    "# Естественный язык по своей природе неоднозначен и сложен:\n",
    "# - Лексическая неоднозначность (омонимия): Слово \"лук\" может означать растение или оружие.\n",
    "# - Синтаксическая неоднозначность: \"Казнить нельзя помиловать\" - структура предложения неясна без знаков препинания.\n",
    "# - Семантическая неоднозначность: \"Он съел пиццу с друзьями\" (с кем? или пицца была с начинкой \"друзья\"?).\n",
    "# - Прагматика (контекст): Значение фразы зависит от контекста диалога, ситуации, знаний о мире.\n",
    "# - Сарказм, ирония, метафоры: Трудны для буквального понимания.\n",
    "# - Разнообразие языков, диалектов, сленга.\n",
    "# - Ошибки и опечатки в тексте.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Основные Задачи NLP\n",
    "\n",
    "# --- 2.1 Текстовая Предобработка (Text Preprocessing) ---\n",
    "# # Цель: Привести \"сырой\" текст к структурированному и чистому виду,\n",
    "# # пригодному для анализа моделями. Часто является первым шагом.\n",
    "# # Основные этапы:\n",
    "# # - Токенизация (Tokenization): Разделение текста на отдельные слова или символы (токены).\n",
    "# #   Пример: \"Привет, мир!\" -> [\"Привет\", \",\", \"мир\", \"!\"]\n",
    "# # - Приведение к нижнему регистру (Lowercasing): \"Привет\" -> \"привет\". Помогает унифицировать слова.\n",
    "# # - Удаление стоп-слов (Stopword Removal): Удаление часто встречающихся, но малоинформативных слов (предлоги, союзы, артикли: \"и\", \"в\", \"на\", \"a\", \"the\").\n",
    "# # - Стемминг (Stemming): Грубое отсечение окончаний слов для приведения к \"основе\" (stem). Может порождать несуществующие слова. Пример: \"бежал\", \"бегущий\" -> \"беж\".\n",
    "# # - Лемматизация (Lemmatization): Приведение слова к его словарной форме (лемме) с учетом части речи. Более осмысленный процесс, чем стемминг. Пример: \"бежал\", \"бегущий\" -> \"бежать\".\n",
    "# # - Удаление пунктуации и специальных символов.\n",
    "# # - Обработка чисел.\n",
    "\n",
    "# --- 2.2 Языковое Моделирование (Language Modeling - LM) ---\n",
    "# # Цель: Построить модель, которая предсказывает вероятность последовательности слов (предложения).\n",
    "# # Используется как основа для многих других задач (генерация текста, машинный перевод, распознавание речи).\n",
    "# # Пример: P(\"я иду домой\") > P(\"я иду домом\").\n",
    "# # Классические подходы: N-граммы.\n",
    "# # Современные подходы: Рекуррентные нейронные сети (RNN), Трансформеры (GPT).\n",
    "\n",
    "# --- 2.3 Классификация Текста (Text Classification) ---\n",
    "# # Цель: Присвоить тексту одну или несколько предопределенных категорий.\n",
    "# # Примеры задач:\n",
    "# #   - Анализ тональности (Sentiment Analysis): Определение эмоциональной окраски текста (позитивная, негативная, нейтральная).\n",
    "# #   - Классификация по темам (Topic Classification): Определение темы документа (спорт, политика, технологии).\n",
    "# #   - Определение спама (Spam Detection).\n",
    "# #   - Определение языка (Language Detection).\n",
    "\n",
    "# --- 2.4 Распознавание Именованных Сущностей (Named Entity Recognition - NER) ---\n",
    "# # Цель: Найти и классифицировать именованные сущности в тексте (люди, организации, локации, даты, денежные суммы и т.д.).\n",
    "# # Пример: \"Иван Петров работает в [компании Google] в [городе Москва] с [даты 2020 года].\"\n",
    "\n",
    "# --- 2.5 Определение Частей Речи (Part-of-Speech - POS Tagging) ---\n",
    "# # Цель: Присвоить каждому слову в предложении его грамматическую часть речи (существительное, глагол, прилагательное, наречие и т.д.).\n",
    "# # Пример: \"Мама [NOUN] мыла [VERB] раму [NOUN].\"\n",
    "\n",
    "# --- 2.6 Машинный Перевод (Machine Translation - MT) ---\n",
    "# # Цель: Автоматический перевод текста с одного языка на другой.\n",
    "# # Подходы: Статистический машинный перевод (SMT), Нейронный машинный перевод (NMT - сейчас доминирует, часто на основе RNN или Трансформеров).\n",
    "\n",
    "# --- 2.7 Ответы на Вопросы (Question Answering - QA) ---\n",
    "# # Цель: Предоставить ответ на вопрос, заданный на естественном языке, на основе предоставленного текста (контекста) или базы знаний.\n",
    "# # Типы: Экстрактивные (ответ - это фрагмент текста), Абстрактивные (ответ генерируется моделью).\n",
    "\n",
    "# --- 2.8 Суммаризация Текста (Text Summarization) ---\n",
    "# # Цель: Создание краткого и связного изложения основного содержания длинного текста.\n",
    "# # Типы: Экстрактивная (выбираются ключевые предложения из оригинала), Абстрактивная (генерируется новый текст, передающий суть).\n",
    "\n",
    "# --- 2.9 Генерация Текста (Text Generation) ---\n",
    "# # Цель: Создание нового текста на естественном языке (продолжение текста, написание стихов, диалоги и т.д.).\n",
    "# # Основывается на языковых моделях.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Представление Текста для Моделей\n",
    "\n",
    "# Модели машинного обучения не могут работать с текстом напрямую. Его нужно преобразовать в числовой формат.\n",
    "\n",
    "# 1. Bag-of-Words (BoW) - \"Мешок Слов\":\n",
    "#    - Принцип: Представление текста как неупорядоченного набора слов (мультимножества). Порядок слов игнорируется.\n",
    "#    - Реализация: Создается словарь всех уникальных слов в корпусе текстов. Каждый текст представляется вектором, где каждый элемент соответствует слову из словаря и содержит его частоту (или бинарный флаг присутствия) в тексте.\n",
    "#    - Недостатки: Игнорирует порядок слов, не учитывает семантику (синонимы - разные векторы). Высокая размерность вектора.\n",
    "\n",
    "# 2. TF-IDF (Term Frequency-Inverse Document Frequency):\n",
    "#    - Принцип: Улучшение BoW. Вес слова в векторе текста зависит не только от его частоты в этом тексте (TF), но и от его редкости во всем корпусе (IDF).\n",
    "#    - TF (Частота термина): Как часто слово встречается в документе? (log(1 + count) или count / total_words)\n",
    "#    - IDF (Обратная документная частота): Насколько слово уникально для всего корпуса? (log(total_docs / (docs_with_word + 1)))\n",
    "#    - TF-IDF = TF * IDF. Слова, часто встречающиеся во многих документах (как стоп-слова), получают низкий вес. Редкие, но важные для документа слова - высокий вес.\n",
    "#    - Недостатки: Все еще игнорирует порядок слов и семантику (частично).\n",
    "\n",
    "# 3. Векторные Представления Слов (Word Embeddings) - Статические:\n",
    "#    - Цель: Представить слова плотными векторами низкой размерности (например, 100-300), которые захватывают семантические отношения между словами (близкие по значению слова имеют близкие векторы).\n",
    "#    - Обучаются на больших текстовых корпусах.\n",
    "#    - Примеры:\n",
    "#      - Word2Vec (Google): Два алгоритма - CBOW (предсказание слова по контексту) и Skip-gram (предсказание контекста по слову).\n",
    "#      - GloVe (Stanford): Основан на матрице совместной встречаемости слов.\n",
    "#      - FastText (Facebook): Расширение Word2Vec, учитывает информацию о подсловах (n-граммах символов), что позволяет генерировать векторы для неизвестных слов и лучше работает с морфологически богатыми языками.\n",
    "#    - Недостатки: Статические - одно слово имеет один вектор независимо от контекста (\"лук\" всегда будет иметь один и тот же вектор).\n",
    "\n",
    "# 4. Контекстуализированные Векторные Представления - Динамические:\n",
    "#    - Цель: Генерировать векторное представление слова, которое зависит от контекста, в котором оно используется. Вектор слова \"лук\" будет разным в предложениях \"Я люблю зеленый лук\" и \"Он натянул лук\".\n",
    "#    - Основаны на глубоких нейронных сетях (RNN, Трансформеры).\n",
    "#    - Примеры:\n",
    "#      - ELMo (Embeddings from Language Models): Использует двунаправленную LSTM.\n",
    "#      - BERT (Bidirectional Encoder Representations from Transformers): Основан на архитектуре Трансформер (только энкодер). Обучается на задачах Masked Language Model (предсказание пропущенных слов) и Next Sentence Prediction. Очень влиятельная модель.\n",
    "#      - GPT (Generative Pre-trained Transformer): Основан на архитектуре Трансформер (только декодер). Обучается на задаче предсказания следующего слова. Отлично подходит для генерации текста. (GPT-2, GPT-3, GPT-4).\n",
    "#      - T5, BART, XLNet и многие другие.\n",
    "#    - Преимущества: Учитывают контекст, достигают state-of-the-art результатов во многих задачах NLP.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Методы и Модели NLP\n",
    "\n",
    "# 1. Правиловые Системы (Rule-based Systems):\n",
    "#    - Основаны на наборах правил, созданных вручную лингвистами или экспертами.\n",
    "#    - Пример: Простые чат-боты, некоторые системы извлечения информации.\n",
    "#    - Плюсы: Интерпретируемость.\n",
    "#    - Минусы: Трудоемкость создания правил, хрупкость (плохо обобщаются), сложность поддержки.\n",
    "\n",
    "# 2. Статистические Методы Машинного Обучения:\n",
    "#    - Используют статистические модели, обученные на размеченных данных.\n",
    "#    - Требуют этапа извлечения признаков (Feature Engineering), часто с использованием BoW или TF-IDF.\n",
    "#    - Примеры:\n",
    "#      - Наивный Байес (Naive Bayes): Простой вероятностный классификатор, хорошо работает для классификации текста.\n",
    "#      - Метод Опорных Векторов (Support Vector Machines - SVM): Мощный классификатор, часто дает хорошие результаты.\n",
    "#      - Логистическая Регрессия.\n",
    "#      - Скрытые Марковские Модели (Hidden Markov Models - HMM): Использовались для POS-tagging, NER.\n",
    "#      - Условные Случайные Поля (Conditional Random Fields - CRF): Улучшение HMM, популярны для задач последовательной разметки (POS, NER).\n",
    "\n",
    "# 3. Глубокое Обучение (Deep Learning):\n",
    "#    - Автоматически изучает признаки из данных, устраняя необходимость в ручном Feature Engineering.\n",
    "#    - Использует нейронные сети с несколькими слоями.\n",
    "#    - Доминирующий подход в современном NLP.\n",
    "#    - Основные архитектуры:\n",
    "#      - Рекуррентные Нейронные Сети (RNN): Обрабатывают последовательности, учитывая предыдущую информацию.\n",
    "#        - LSTM (Long Short-Term Memory) и GRU (Gated Recurrent Unit): Варианты RNN, решающие проблему затухания градиента, лучше работают с длинными зависимостями. Хороши для языкового моделирования, перевода, генерации.\n",
    "#      - Сверточные Нейронные Сети (CNN): Могут применяться к тексту (1D свертки) для извлечения локальных признаков (n-грамм). Эффективны для классификации текста.\n",
    "#      - Трансформеры (Transformers): Архитектура, основанная на механизме внимания (Self-Attention). Позволяет модели взвешивать важность разных слов в последовательности при обработке каждого слова. Параллелизуется лучше, чем RNN. Основа для BERT, GPT и др. State-of-the-art для большинства задач NLP.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Популярные Библиотеки NLP (Python)\n",
    "\n",
    "# 1. NLTK (Natural Language Toolkit):\n",
    "#    - `import nltk`\n",
    "#    - Старейшая и наиболее полная библиотека. Отлично подходит для обучения и исследований.\n",
    "#    - Содержит инструменты для большинства базовых задач (токенизация, стемминг, лемматизация, POS-tagging, NER), а также корпуса текстов и лексические ресурсы (WordNet).\n",
    "#    - Может быть медленной для продакшена.\n",
    "\n",
    "# 2. spaCy:\n",
    "#    - `import spacy`\n",
    "#    - Разработана для промышленного использования. Быстрая, эффективная, имеет свое \"мнение\" о лучшем способе решения задач.\n",
    "#    - Предоставляет предобученные модели для разных языков для токенизации, POS-tagging, NER, синтаксического анализа зависимостей.\n",
    "#    - Легко интегрируется с фреймворками глубокого обучения.\n",
    "\n",
    "# 3. Gensim:\n",
    "#    - `from gensim.models import Word2Vec`\n",
    "#    - Специализируется на векторных представлениях слов (Word2Vec, FastText, Doc2Vec) и тематическом моделировании (LDA, LSI).\n",
    "#    - Эффективная работа с большими корпусами.\n",
    "\n",
    "# 4. Scikit-learn:\n",
    "#    - `from sklearn.feature_extraction.text import TfidfVectorizer`\n",
    "#    - `from sklearn.naive_bayes import MultinomialNB`\n",
    "#    - Библиотека общего назначения для машинного обучения.\n",
    "#    - Содержит отличные инструменты для векторизации текста (CountVectorizer, TfidfVectorizer) и классические модели ML (Naive Bayes, SVM, Logistic Regression), которые хорошо работают с текстовыми признаками.\n",
    "\n",
    "# 5. Hugging Face Transformers:\n",
    "#    - `from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification`\n",
    "#    - Де-факто стандарт для работы с Трансформерами (BERT, GPT, T5 и сотни других).\n",
    "#    - Предоставляет:\n",
    "#      - Огромный хаб с тысячами предобученных моделей для разных задач и языков.\n",
    "#      - Унифицированные API для загрузки моделей и токенизаторов (`AutoModel...`, `AutoTokenizer`).\n",
    "#      - Инструменты для fine-tuning моделей на своих данных (`Trainer` API).\n",
    "#      - Простые в использовании `pipeline` для стандартных задач (sentiment-analysis, ner, translation, etc.).\n",
    "#    - Поддерживает PyTorch и TensorFlow.\n",
    "\n",
    "# 6. PyTorch / TensorFlow:\n",
    "#    - `import torch` / `import tensorflow as tf`\n",
    "#    - Фундаментальные библиотеки для глубокого обучения. Необходимы для создания или модификации архитектур нейронных сетей (RNN, Transformers) и их обучения.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Пример Задачи - Анализ Тональности (Sentiment Analysis)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Классифицировать отзывы о фильмах из некоторого датасета (например, IMDb)\n",
    "# на два класса: \"позитивный\" (positive) и \"негативный\" (negative).\n",
    "\n",
    "# --- Решение (Концептуальное, с использованием Hugging Face Transformers & PyTorch) ---\n",
    "\n",
    "# Шаг 1: Загрузка и Подготовка Данных\n",
    "# # Предполагается, что у вас есть датасет в виде списка текстов (отзывов)\n",
    "# # и списка соответствующих меток (0 - негативный, 1 - позитивный).\n",
    "# texts = [\"Это был ужасный фильм!\", \"Лучшее кино, что я видел.\", ...]\n",
    "# labels = [0, 1, ...]\n",
    "# # Разделение на обучающую и валидационную выборки\n",
    "# # train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, ...)\n",
    "\n",
    "# Шаг 2: Загрузка Модели и Токенизатора\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"bert-base-uncased\" # Пример: используем базовый BERT (без учета регистра)\n",
    "# Или можно взять модель, уже дообученную на анализ тональности, для инференса:\n",
    "# model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# num_labels соответствует количеству классов (2: positive, negative)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Шаг 3: Токенизация Данных\n",
    "# # Токенизатор преобразует текст в формат, понятный модели (input_ids, attention_mask)\n",
    "# train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
    "# val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Шаг 4: Создание PyTorch Dataset и DataLoader\n",
    "# class IMDbDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, encodings, labels):\n",
    "#         self.encodings = encodings\n",
    "#         self.labels = labels\n",
    "#\n",
    "#     def __getitem__(self, idx):\n",
    "#         # Возвращаем словарь тензоров для каждого элемента\n",
    "#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "#         item['labels'] = torch.tensor(self.labels[idx])\n",
    "#         return item\n",
    "#\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "#\n",
    "# train_dataset = IMDbDataset(train_encodings, train_labels)\n",
    "# val_dataset = IMDbDataset(val_encodings, val_labels)\n",
    "#\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "# val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "# Шаг 5: Fine-tuning (Дообучение) Модели\n",
    "# # Используем оптимизатор AdamW, рекомендованный для Трансформеров\n",
    "# from transformers import AdamW\n",
    "#\n",
    "# optimizer = AdamW(model.parameters(), lr=5e-5) # Типичный learning rate для fine-tuning BERT\n",
    "# num_epochs = 3\n",
    "#\n",
    "# model.train() # Переводим модель в режим обучения\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "#     for batch in train_loader:\n",
    "#         optimizer.zero_grad()\n",
    "#         # Перемещаем батч на нужное устройство\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "#\n",
    "#         # Прямой проход\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs.loss # Модель возвращает лосс, если переданы labels\n",
    "#\n",
    "#         # Обратный проход\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print(f\"  Training Loss: {loss.item()}\") # Лосс последнего батча\n",
    "#     # Здесь нужна валидация после каждой эпохи для оценки качества\n",
    "\n",
    "# Шаг 6: Оценка Модели\n",
    "# # Переводим модель в режим оценки\n",
    "# model.eval()\n",
    "# total_eval_accuracy = 0\n",
    "# with torch.no_grad(): # Отключаем градиенты\n",
    "#     for batch in val_loader:\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "#\n",
    "#         outputs = model(input_ids, attention_mask=attention_mask)\n",
    "#         logits = outputs.logits\n",
    "#         predictions = torch.argmax(logits, dim=-1)\n",
    "#\n",
    "#         total_eval_accuracy += (predictions == labels).sum().item()\n",
    "#\n",
    "# avg_val_accuracy = total_eval_accuracy / len(val_dataset)\n",
    "# print(f\"Validation Accuracy: {avg_val_accuracy:.4f}\")\n",
    "\n",
    "# Шаг 7: Инференс (Использование обученной модели)\n",
    "def predict_sentiment(text):\n",
    "    model.eval() # Убедиться, что модель в режиме eval\n",
    "    # Токенизация одного текста\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    # Перемещение на устройство\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs) # Распаковка словаря inputs\n",
    "\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=-1)\n",
    "    predicted_class_id = torch.argmax(probabilities, dim=-1).item()\n",
    "\n",
    "    # Предполагаем, что 0 - негативный, 1 - позитивный\n",
    "    sentiment = \"positive\" if predicted_class_id == 1 else \"negative\"\n",
    "    confidence = probabilities[0][predicted_class_id].item()\n",
    "\n",
    "    return sentiment, confidence\n",
    "\n",
    "# Пример использования\n",
    "# review = \"This movie was absolutely fantastic, highly recommended!\"\n",
    "# sentiment, confidence = predict_sentiment(review)\n",
    "# print(f\"Review: '{review}'\")\n",
    "# print(f\"Predicted Sentiment: {sentiment} (Confidence: {confidence:.4f})\")\n",
    "#\n",
    "# review = \"A complete waste of time, terrible acting and plot.\"\n",
    "# sentiment, confidence = predict_sentiment(review)\n",
    "# print(f\"Review: '{review}'\")\n",
    "# print(f\"Predicted Sentiment: {sentiment} (Confidence: {confidence:.4f})\")\n",
    "\n",
    "# --- Конец Примера ---\n",
    "\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Введение в NLTK (Natural Language Toolkit)\n",
    "\n",
    "# Что такое NLTK?\n",
    "# NLTK - это одна из старейших и наиболее комплексных библиотек Python для\n",
    "# обработки естественного языка. Она предоставляет удобные интерфейсы\n",
    "# к множеству лексических ресурсов (корпуса, словари, WordNet), а также\n",
    "# набор библиотек для выполнения различных задач NLP, от токенизации до\n",
    "# синтаксического анализа.\n",
    "# NLTK отлично подходит для обучения, исследований и прототипирования.\n",
    "\n",
    "# Установка:\n",
    "# pip install nltk\n",
    "\n",
    "# Загрузка данных NLTK:\n",
    "# NLTK требует загрузки дополнительных данных (корпусов, моделей) для работы\n",
    "# многих своих модулей. Это делается один раз после установки.\n",
    "import nltk\n",
    "\n",
    "# Попробуйте выполнить эти команды в Python консоли или Jupyter Notebook:\n",
    "# nltk.download('popular') # Загрузить популярный набор данных (рекомендуется для начала)\n",
    "# Или можно загружать пакеты по мере необходимости, например:\n",
    "# nltk.download('punkt') # Для токенизации предложений и слов\n",
    "# nltk.download('stopwords') # Списки стоп-слов\n",
    "# nltk.download('wordnet') # Лексическая база данных WordNet\n",
    "# nltk.download('omw-1.4') # Open Multilingual Wordnet, нужен для WordNet в некоторых версиях\n",
    "# nltk.download('averaged_perceptron_tagger') # Для POS-tagging\n",
    "# nltk.download('maxent_ne_chunker') # Для NER\n",
    "# nltk.download('words') # Словарь английских слов (используется в NER)\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Основные Задачи Предобработки с NLTK\n",
    "\n",
    "# --- 2.1 Токенизация (Tokenization) ---\n",
    "# # Разделение текста на предложения и слова (токены).\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text_example = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue.\"\n",
    "\n",
    "# Токенизация предложений\n",
    "sentences = sent_tokenize(text_example)\n",
    "# print(\"Sentences:\")\n",
    "# print(sentences)\n",
    "\n",
    "# Токенизация слов (для первого предложения)\n",
    "# word_tokenize обрабатывает пунктуацию как отдельные токены\n",
    "words_in_first_sentence = word_tokenize(sentences[0])\n",
    "# print(\"\\nWords in the first sentence:\")\n",
    "# print(words_in_first_sentence)\n",
    "\n",
    "# --- 2.2 Удаление Стоп-слов (Stopword Removal) ---\n",
    "# # Удаление часто встречающихся, но малоинформативных слов.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Загрузка стоп-слов для английского языка\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "# print(\"\\nEnglish Stopwords (sample):\")\n",
    "# print(list(stop_words_en)[:10])\n",
    "\n",
    "# Фильтрация токенов (из первого предложения)\n",
    "filtered_words = [word for word in words_in_first_sentence if word.lower() not in stop_words_en]\n",
    "# print(\"\\nWords after stopword removal:\")\n",
    "# print(filtered_words)\n",
    "\n",
    "# --- 2.3 Стемминг (Stemming) ---\n",
    "# # Грубое отсечение окончаний для приведения к \"основе\".\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english') # Snowball поддерживает разные языки\n",
    "\n",
    "words_to_stem = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n",
    "\n",
    "# print(\"\\nStemming examples:\")\n",
    "# print(\"Word\\t\\tPorter\\t\\tSnowball\")\n",
    "# for word in words_to_stem:\n",
    "#     porter_stem = porter.stem(word)\n",
    "#     snowball_stem = snowball.stem(word)\n",
    "#     print(f\"{word}\\t\\t{porter_stem}\\t\\t{snowball_stem}\")\n",
    "\n",
    "# Применение к нашему примеру (отфильтрованные слова)\n",
    "# stemmed_filtered_words = [porter.stem(word) for word in filtered_words]\n",
    "# print(\"\\nFiltered words after Porter stemming:\")\n",
    "# print(stemmed_filtered_words)\n",
    "\n",
    "# --- 2.4 Лемматизация (Lemmatization) ---\n",
    "# # Приведение слова к его словарной форме (лемме) с учетом части речи.\n",
    "# # Требует загрузки 'wordnet' и 'omw-1.4'.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet # Для определения части речи\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Простой пример лемматизации\n",
    "# print(\"\\nLemmatization examples:\")\n",
    "# print(f\"rocks : {lemmatizer.lemmatize('rocks')}\") # По умолчанию считает существительным (pos='n')\n",
    "# print(f\"corpora : {lemmatizer.lemmatize('corpora')}\")\n",
    "# print(f\"better : {lemmatizer.lemmatize('better', pos='a')}\") # 'a' - прилагательное (adjective)\n",
    "# print(f\"running : {lemmatizer.lemmatize('running', pos='v')}\") # 'v' - глагол (verb)\n",
    "\n",
    "# Лемматизация требует знания части речи (POS tag) для точности.\n",
    "# Функция для конвертации NLTK POS tag в формат, понятный WordNetLemmatizer\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # По умолчанию возвращаем существительное\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Сначала нужно получить POS-теги (см. следующий блок)\n",
    "# lemmatized_words = []\n",
    "# words_with_tags = nltk.pos_tag(words_in_first_sentence) # Получаем теги\n",
    "# for word, tag in words_with_tags:\n",
    "#     wnet_tag = get_wordnet_pos(tag)\n",
    "#     lemma = lemmatizer.lemmatize(word, pos=wnet_tag)\n",
    "#     lemmatized_words.append(lemma)\n",
    "#\n",
    "# print(\"\\nWords after lemmatization (with POS tags):\")\n",
    "# print(lemmatized_words)\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Анализ Текста с NLTK\n",
    "\n",
    "# --- 3.1 Определение Частей Речи (Part-of-Speech - POS Tagging) ---\n",
    "# # Присвоение грамматических тегов каждому токену.\n",
    "# # Требует загрузки 'averaged_perceptron_tagger'.\n",
    "import nltk # Импортируем снова для ясности\n",
    "\n",
    "# Используем токены из первого предложения\n",
    "# words_in_first_sentence = word_tokenize(sentences[0])\n",
    "pos_tags = nltk.pos_tag(words_in_first_sentence)\n",
    "# print(\"\\nPOS Tagging results:\")\n",
    "# print(pos_tags)\n",
    "# # Теги соответствуют Penn Treebank Tagset (например, NNP: Proper noun, singular; VBP: Verb, non-3rd person singular present; JJ: Adjective)\n",
    "\n",
    "# --- 3.2 Распознавание Именованных Сущностей (Named Entity Recognition - NER) ---\n",
    "# # Поиск и классификация именованных сущностей.\n",
    "# # Требует POS-тегов и загрузки 'maxent_ne_chunker', 'words'.\n",
    "# # NLTK NER работает через создание дерева разбора (chunking).\n",
    "\n",
    "# Используем POS-теги, полученные ранее\n",
    "# ner_tree = nltk.ne_chunk(pos_tags)\n",
    "# print(\"\\nNER Tree:\")\n",
    "# print(ner_tree)\n",
    "# # Результат - это дерево. Сущности помечены (например, PERSON, ORGANIZATION, GPE - Geopolitical Entity).\n",
    "# # Можно итерироваться по дереву для извлечения сущностей.\n",
    "\n",
    "# Пример извлечения сущностей из дерева\n",
    "# continuous_chunk = []\n",
    "# current_chunk = []\n",
    "#\n",
    "# for node in ner_tree:\n",
    "#     if type(node) == nltk.Tree: # Если это узел сущности\n",
    "#         current_chunk.append(\" \".join([token for token, pos in node.leaves()]))\n",
    "#         named_entity = \" \".join(current_chunk)\n",
    "#         if named_entity not in continuous_chunk:\n",
    "#              continuous_chunk.append((named_entity, node.label())) # Добавляем (сущность, тип)\n",
    "#              current_chunk = []\n",
    "#     else: # Если это не сущность, сбрасываем текущий чанк\n",
    "#          current_chunk = []\n",
    "#\n",
    "# print(\"\\nExtracted Named Entities:\")\n",
    "# print(continuous_chunk) # Может найти 'Hello', 'Mr. Smith' как PERSON\n",
    "\n",
    "# --- 3.3 Частотное Распределение (Frequency Distribution) ---\n",
    "# # Подсчет частоты встречаемости токенов.\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Используем все слова из исходного текста (приведенные к нижнему регистру)\n",
    "all_words_lower = [word.lower() for word in word_tokenize(text_example) if word.isalpha()] # Берем только слова\n",
    "fdist = FreqDist(all_words_lower)\n",
    "\n",
    "# print(\"\\nWord Frequency Distribution:\")\n",
    "# print(fdist)\n",
    "# print(\"\\nMost common words:\")\n",
    "# print(fdist.most_common(5))\n",
    "# print(f\"\\nFrequency of 'is': {fdist['is']}\")\n",
    "\n",
    "# Можно построить график\n",
    "# fdist.plot(20, cumulative=False, title=\"Top 20 Word Frequencies\")\n",
    "# plt.show() # Нужен import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: WordNet - Лексическая База Данных\n",
    "\n",
    "# WordNet - это большая лексическая база данных английского языка.\n",
    "# Группирует слова в наборы синонимов (синсеты), предоставляет краткие определения и примеры.\n",
    "# Требует загрузки 'wordnet' и 'omw-1.4'.\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Поиск синсетов для слова 'car'\n",
    "syns_car = wordnet.synsets('car')\n",
    "# print(\"\\nSynsets for 'car':\")\n",
    "# print(syns_car)\n",
    "\n",
    "# Информация о первом синсете\n",
    "first_syn = syns_car[0]\n",
    "# print(f\"\\nName: {first_syn.name()}\") # Имя синсета (слово.часть_речи.номер)\n",
    "# print(f\"Lemma names: {first_syn.lemmas()}\") # Слова, входящие в синсет\n",
    "# print(f\"Definition: {first_syn.definition()}\") # Определение\n",
    "# print(f\"Examples: {first_syn.examples()}\") # Примеры использования\n",
    "\n",
    "# Гипонимы (более конкретные понятия)\n",
    "# print(f\"Hyponyms: {first_syn.hyponyms()}\")\n",
    "\n",
    "# Гиперонимы (более общие понятия)\n",
    "# print(f\"Hypernyms: {first_syn.root_hypernyms()}\")\n",
    "\n",
    "# Семантическое сходство между словами (на основе пути в иерархии WordNet)\n",
    "# w1 = wordnet.synset('ship.n.01')\n",
    "# w2 = wordnet.synset('boat.n.01')\n",
    "# print(f\"\\nPath similarity between 'ship' and 'boat': {w1.path_similarity(w2)}\")\n",
    "# w3 = wordnet.synset('car.n.01')\n",
    "# print(f\"Path similarity between 'ship' and 'car': {w1.path_similarity(w3)}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Пример Конвейера Предобработки NLTK\n",
    "\n",
    "# Задача: Взять текст, токенизировать, удалить стоп-слова и пунктуацию, лемматизировать.\n",
    "\n",
    "def preprocess_text_nltk(text):\n",
    "    # 1. Токенизация слов\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # 2. Удаление стоп-слов и пунктуации\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Добавим базовую пунктуацию к стоп-словам или проверим isalpha()\n",
    "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "\n",
    "    # 3. POS-теггинг\n",
    "    pos_tags = nltk.pos_tag(filtered_tokens)\n",
    "\n",
    "    # 4. Лемматизация с учетом POS-тегов\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        wnet_tag = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wnet_tag)\n",
    "        lemmatized_tokens.append(lemma)\n",
    "\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Пример использования конвейера\n",
    "# sample_text = \"The quick brown foxes are jumping over the lazy dogs.\"\n",
    "# processed_tokens = preprocess_text_nltk(sample_text)\n",
    "# print(f\"\\nOriginal Text: '{sample_text}'\")\n",
    "# print(f\"Processed Tokens: {processed_tokens}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Замечания и Ограничения NLTK\n",
    "\n",
    "# - Производительность: Для некоторых задач NLTK может быть медленнее, чем более\n",
    "#   оптимизированные библиотеки вроде spaCy, особенно на больших объемах данных.\n",
    "# - Современные Модели: NLTK не предоставляет прямого доступа к современным\n",
    "#   трансформерным моделям (BERT, GPT) и их контекстуализированным эмбеддингам.\n",
    "#   Для этого лучше использовать библиотеку Hugging Face Transformers.\n",
    "# - Предобученные Модели: Хотя NLTK имеет некоторые предобученные модели (POS-tagger, NER),\n",
    "#   их качество может уступать моделям из spaCy или Transformers для некоторых языков/задач.\n",
    "# - Фокус: NLTK исторически больше фокусировался на лингвистических исследованиях и\n",
    "#   образовании, предоставляя доступ к разнообразным ресурсам и алгоритмам,\n",
    "#   в то время как spaCy и Transformers больше ориентированы на создание\n",
    "#   эффективных промышленных NLP-приложений.\n",
    "\n",
    "# Вывод: NLTK - отличный инструмент для изучения основ NLP, экспериментов\n",
    "# с различными техниками и доступа к лингвистическим данным. Для построения\n",
    "# высокопроизводительных систем или использования state-of-the-art моделей\n",
    "# часто предпочтительнее использовать spaCy и/или Hugging Face Transformers.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Не забудьте выполнить nltk.download() для загрузки необходимых пакетов!\n",
    "# Пример команд для этого скрипта:\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Введение в spaCy\n",
    "\n",
    "# Что такое spaCy?\n",
    "# spaCy - это современная, быстрая и эффективная библиотека Python для\n",
    "# продвинутой обработки естественного языка (NLP). Она разработана с фокусом\n",
    "# на промышленное применение и предоставление готовых решений для\n",
    "# распространенных задач NLP.\n",
    "# spaCy является \"мнениющейся\" (opinionated), то есть часто предлагает один,\n",
    "# но хорошо оптимизированный способ решения задачи, в отличие от NLTK,\n",
    "# который предоставляет множество альтернатив.\n",
    "\n",
    "# Философия spaCy:\n",
    "# - Производительность и Эффективность: Написана на Cython, быстрая.\n",
    "# - Готовность к Использованию: Предоставляет предобученные модели для разных языков.\n",
    "# - Интеграция: Легко интегрируется с веб-фреймворками и фреймворками глубокого обучения.\n",
    "# - Простота API: Предлагает объектно-ориентированный подход.\n",
    "\n",
    "# Установка spaCy:\n",
    "# pip install -U spacy\n",
    "\n",
    "# Загрузка Языковых Моделей:\n",
    "# spaCy использует предобученные статистические модели для выполнения задач\n",
    "# (POS-tagging, NER, парсинг зависимостей). Модели нужно загружать отдельно.\n",
    "# Модели различаются по размеру и возможностям (например, наличию векторов слов).\n",
    "# Соглашение об именовании: [язык]_[тип]_[жанр]_[размер]\n",
    "#   - язык: en (English), de (German), es (Spanish), ru (Russian), xx (Multi-language) и т.д.\n",
    "#   - тип: core (основная модель), dep (только зависимости), ner и т.д.\n",
    "#   - жанр: web (веб-тексты), news (новости) и т.д.\n",
    "#   - размер: sm (small), md (medium), lg (large), trf (Transformer-based)\n",
    "\n",
    "# Пример загрузки маленькой английской модели:\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "# Пример загрузки русской модели (если доступна):\n",
    "# python -m spacy download ru_core_news_sm\n",
    "\n",
    "# Проверка установленных моделей:\n",
    "# python -m spacy validate\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Основные Объекты и Концепции spaCy\n",
    "\n",
    "import spacy\n",
    "\n",
    "# 1. Объект `nlp` (Языковая Модель):\n",
    "#    - Центральный объект spaCy. Загружает языковую модель и содержит конвейер обработки (pipeline).\n",
    "#    - Создается вызовом `spacy.load()` с именем установленной модели.\n",
    "\n",
    "# Загрузка маленькой английской модели (убедитесь, что она загружена!)\n",
    "try:\n",
    "    nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Loaded 'en_core_web_sm' model.\")\n",
    "except OSError:\n",
    "    print(\"Error: 'en_core_web_sm' model not found. Please run:\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")\n",
    "    # Создадим пустой объект для продолжения примера, но он не будет выполнять анализ\n",
    "    nlp_en = spacy.blank(\"en\") # Создает пустой пайплайн для языка 'en'\n",
    "\n",
    "\n",
    "# 2. Объект `Doc`:\n",
    "#    - Результат обработки текста объектом `nlp`.\n",
    "#    - `doc = nlp(\"Текст для обработки\")`\n",
    "#    - Это контейнер, содержащий последовательность токенов (`Token`) и их аннотации\n",
    "#      (POS-теги, зависимости, сущности и т.д.).\n",
    "#    - Сохраняет исходный текст и информацию о структуре документа.\n",
    "\n",
    "text_example = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "doc = nlp_en(text_example)\n",
    "# print(f\"\\nProcessed text: '{text_example}'\")\n",
    "# print(f\"Type of result: {type(doc)}\")\n",
    "\n",
    "# 3. Объект `Token`:\n",
    "#    - Представляет отдельный токен (слово, знак пунктуации).\n",
    "#    - Доступ к токенам осуществляется итерацией по объекту `Doc`.\n",
    "#    - Имеет множество атрибутов для доступа к лингвистической информации.\n",
    "\n",
    "# print(\"\\nTokens and their attributes:\")\n",
    "# for token in doc:\n",
    "#     print(\n",
    "#         f\"  Text: {token.text:<10}\"\n",
    "#         f\" Lemma: {token.lemma_:<10}\" # Лемма (словарная форма)\n",
    "#         f\" POS: {token.pos_:<8}\"      # Простая часть речи (NOUN, VERB, ADJ)\n",
    "#         f\" Tag: {token.tag_:<8}\"      # Детальный POS-тег (NNP, VBG, NNS)\n",
    "#         f\" Dep: {token.dep_:<10}\"     # Синтаксическая зависимость\n",
    "#         f\" Shape: {token.shape_:<10}\" # Форма слова (Xxxxx, dddd)\n",
    "#         f\" Alpha: {token.is_alpha:<5}\" # Является ли буквенным?\n",
    "#         f\" Stop: {token.is_stop}\"     # Является ли стоп-словом?\n",
    "#     )\n",
    "\n",
    "# 4. Объект `Span`:\n",
    "#    - Срез (slice) объекта `Doc`, представляющий фразу или последовательность токенов.\n",
    "#    - Имеет доступ к атрибутам токенов, входящих в него.\n",
    "#    - Именованные сущности (`doc.ents`) являются объектами `Span`.\n",
    "\n",
    "# Пример Span:\n",
    "# span = doc[1:4] # Токены с индексами 1, 2, 3 ('is looking at')\n",
    "# print(f\"\\nSpan example: '{span.text}'\")\n",
    "# print(f\"Span root dependency: {span.root.dep_}\")\n",
    "\n",
    "# 5. Конвейер Обработки (Pipeline):\n",
    "#    - Когда вы вызываете `nlp(text)`, текст проходит через последовательность компонентов.\n",
    "#    - Компоненты: токенизатор, теггер (POS), парсер (зависимости), NER и т.д.\n",
    "#    - Состав конвейера зависит от загруженной модели.\n",
    "#    - Можно посмотреть компоненты: `print(nlp_en.pipe_names)`\n",
    "#    - Можно добавлять/удалять/заменять компоненты.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Доступ к Лингвистическим Признакам\n",
    "\n",
    "# После обработки текста (`doc = nlp(text)`), spaCy предоставляет доступ к аннотациям.\n",
    "\n",
    "# --- 3.1 Токенизация ---\n",
    "# # Выполняется автоматически при создании `Doc`. Доступ через итерацию.\n",
    "# print(\"\\nTokens:\")\n",
    "# for token in doc:\n",
    "#     print(token.text)\n",
    "\n",
    "# --- 3.2 Лемматизация ---\n",
    "# # Получение базовой формы слова. Атрибут `token.lemma_`.\n",
    "# print(\"\\nLemmas:\")\n",
    "# for token in doc:\n",
    "#     print(f\"{token.text} -> {token.lemma_}\")\n",
    "\n",
    "# --- 3.3 POS-теггинг ---\n",
    "# # Простые теги: `token.pos_` (NOUN, VERB, ADJ, PUNCT...). Универсальные теги.\n",
    "# # Детальные теги: `token.tag_` (NNP, VBG, JJ...). Зависят от языка и обучающих данных.\n",
    "# print(\"\\nPOS Tags:\")\n",
    "# for token in doc:\n",
    "#     print(f\"{token.text:<10} {token.pos_:<8} {token.tag_:<8} {spacy.explain(token.tag_)}\") # Объяснение тега\n",
    "\n",
    "# --- 3.4 Синтаксический Анализ Зависимостей (Dependency Parsing) ---\n",
    "# # Анализ грамматической структуры предложения. Показывает отношения между словами.\n",
    "# # `token.dep_`: Тип зависимости (nsubj, dobj, amod...).\n",
    "# # `token.head`: Токен, от которого зависит данный токен (его \"голова\").\n",
    "# print(\"\\nDependency Parsing:\")\n",
    "# for token in doc:\n",
    "#     print(f\"{token.text:<10} {token.dep_:<10} {token.head.text:<10} {[child for child in token.children]}\")\n",
    "\n",
    "# Визуализация дерева зависимостей (требует Jupyter Notebook/Lab)\n",
    "from spacy import displacy\n",
    "# displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 90})\n",
    "\n",
    "# --- 3.5 Распознавание Именованных Сущностей (NER) ---\n",
    "# # Поиск и классификация сущностей (люди, организации, локации...).\n",
    "# # Доступ через `doc.ents`. Каждый элемент - это `Span`.\n",
    "# print(\"\\nNamed Entities:\")\n",
    "# if doc.ents:\n",
    "#     for ent in doc.ents:\n",
    "#         print(f\"  Text: {ent.text:<20} Label: {ent.label_:<10} ({spacy.explain(ent.label_)})\")\n",
    "# else:\n",
    "#     print(\"  No entities found by this model.\")\n",
    "\n",
    "# Визуализация NER (требует Jupyter Notebook/Lab)\n",
    "# displacy.render(doc, style=\"ent\", jupyter=True)\n",
    "\n",
    "# --- 3.6 Определение Предложений (Sentence Boundary Detection - SBD) ---\n",
    "# # Автоматически выполняется компонентом `parser` или `sentencizer`.\n",
    "# # Доступ через `doc.sents`.\n",
    "# print(\"\\nSentences:\")\n",
    "# for sent in doc.sents:\n",
    "#     print(f\"-> {sent.text}\")\n",
    "\n",
    "# --- 3.7 Проверка на Стоп-слова ---\n",
    "# # Атрибут `token.is_stop`.\n",
    "# print(\"\\nStopwords check:\")\n",
    "# for token in doc:\n",
    "#     if token.is_stop:\n",
    "#         print(f\"'{token.text}' is a stopword.\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Векторные Представления и Сходство\n",
    "\n",
    "# Некоторые модели spaCy (обычно `md` и `lg`) включают векторы слов.\n",
    "# Это позволяет вычислять семантическое сходство между токенами, спанами и документами.\n",
    "# Модель `en_core_web_sm` НЕ включает векторы по умолчанию.\n",
    "# Для работы с векторами загрузите модель побольше:\n",
    "# python -m spacy download en_core_web_md\n",
    "# nlp_md = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Проверка наличия векторов в модели:\n",
    "# has_vectors = nlp_en.vocab.vectors.shape[0] > 0 # Проверяем для nlp_en\n",
    "# print(f\"\\nModel 'en_core_web_sm' has word vectors: {has_vectors}\")\n",
    "\n",
    "# Пример (если бы векторы были):\n",
    "# doc1 = nlp_md(\"I like cats\")\n",
    "# doc2 = nlp_md(\"I love dogs\")\n",
    "# doc3 = nlp_md(\"The weather is nice\")\n",
    "#\n",
    "# # Сходство между документами\n",
    "# print(f\"Similarity(doc1, doc2): {doc1.similarity(doc2)}\")\n",
    "# print(f\"Similarity(doc1, doc3): {doc1.similarity(doc3)}\")\n",
    "#\n",
    "# # Сходство между токенами\n",
    "# token_cat = doc1[2] # 'cats'\n",
    "# token_dog = doc2[3] # 'dogs'\n",
    "# print(f\"Similarity(cat, dog): {token_cat.similarity(token_dog)}\")\n",
    "#\n",
    "# # Получение вектора\n",
    "# print(f\"Vector for 'cats': {token_cat.vector[:5]}...\") # Первые 5 компонент\n",
    "# print(f\"Vector shape: {token_cat.vector.shape}\")\n",
    "# print(f\"Has vector: {token_cat.has_vector}\")\n",
    "# print(f\"Is OOV (Out of Vocabulary): {token_cat.is_oov}\") # Неизвестное слово?\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Rule-based Matching (Поиск по Правилам)\n",
    "\n",
    "# spaCy предоставляет мощный инструмент `Matcher` для поиска последовательностей\n",
    "# токенов на основе правил, описывающих их атрибуты (текст, лемма, POS, теги и т.д.).\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Инициализация Matcher со словарем модели\n",
    "matcher = Matcher(nlp_en.vocab)\n",
    "\n",
    "# Определение паттерна: ищем \"Apple\" (лемма) + любой глагол\n",
    "pattern = [{\"LEMMA\": \"Apple\"}, {\"POS\": \"VERB\"}]\n",
    "\n",
    "# Добавление паттерна в Matcher\n",
    "# \"AppleVerb\" - ID паттерна, pattern - сам паттерн\n",
    "matcher.add(\"AppleVerbPattern\", [pattern])\n",
    "\n",
    "# Применение Matcher к документу\n",
    "doc_match = nlp_en(\"Apple is looking for talent. apple pays well.\")\n",
    "matches = matcher(doc_match)\n",
    "\n",
    "# Результаты: список кортежей (match_id, start_token_index, end_token_index)\n",
    "# print(\"\\nMatcher results:\")\n",
    "# for match_id, start, end in matches:\n",
    "#     string_id = nlp_en.vocab.strings[match_id]  # Получить ID паттерна (строку)\n",
    "#     span = doc_match[start:end]  # Получить найденный Span\n",
    "#     print(f\"  Match ID: {string_id}, Span: '{span.text}'\")\n",
    "\n",
    "# Другие атрибуты для паттернов:\n",
    "# {\"LOWER\": \"apple\"} - текст в нижнем регистре\n",
    "# {\"POS\": \"PROPN\"} - имя собственное\n",
    "# {\"TAG\": \"NNP\"} - детальный тег\n",
    "# {\"DEP\": \"nsubj\"} - зависимость\n",
    "# {\"OP\": \"!\"} - отрицание (токен НЕ должен соответствовать)\n",
    "# {\"OP\": \"?\"} - опциональный токен (0 или 1 раз)\n",
    "# {\"OP\": \"*\"} - 0 или более раз\n",
    "# {\"OP\": \"+\"} - 1 или более раз\n",
    "# {\"ENT_TYPE\": \"ORG\"} - тип сущности\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Настройка Конвейера (Pipeline)\n",
    "\n",
    "# Можно посмотреть текущий конвейер:\n",
    "# print(f\"\\nCurrent pipeline: {nlp_en.pipe_names}\")\n",
    "\n",
    "# Отключение компонента (например, для ускорения, если NER не нужен)\n",
    "# nlp_disabled_ner = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
    "# print(f\"Pipeline with NER disabled: {nlp_disabled_ner.pipe_names}\")\n",
    "# doc_no_ner = nlp_disabled_ner(\"Apple is a company.\")\n",
    "# print(\"Entities with NER disabled:\", doc_no_ner.ents) # Будет пусто\n",
    "\n",
    "# Добавление кастомного компонента\n",
    "# @spacy.language.Language.component(\"custom_printer\")\n",
    "# def custom_printer_component(doc):\n",
    "#     print(f\"Custom component processing Doc with {len(doc)} tokens.\")\n",
    "#     # Нужно вернуть doc\n",
    "#     return doc\n",
    "#\n",
    "# # Добавляем компонент в начало конвейера\n",
    "# nlp_en.add_pipe(\"custom_printer\", first=True)\n",
    "# print(f\"Pipeline after adding custom component: {nlp_en.pipe_names}\")\n",
    "# # Обработаем текст снова, чтобы увидеть вывод компонента\n",
    "# # doc_custom = nlp_en(\"This will trigger the custom component.\")\n",
    "# # Удаляем компонент, чтобы не мешал дальше\n",
    "# nlp_en.remove_pipe(\"custom_printer\")\n",
    "# print(f\"Pipeline after removing custom component: {nlp_en.pipe_names}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 7: Пример Задачи - Извлечение Сущностей и Лемматизация\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Обработать текст, извлечь все именованные сущности типа \"ORGANIZATION\" (ORG)\n",
    "# и \"GEOPOLITICAL ENTITY\" (GPE). Для остального текста вывести леммы слов,\n",
    "# не являющихся стоп-словами или знаками пунктуации.\n",
    "\n",
    "# --- Решение (Полный Код) ---\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Убедитесь, что модель загружена\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Loaded 'en_core_web_sm' model for the task.\")\n",
    "except OSError:\n",
    "    print(\"Error: 'en_core_web_sm' model not found. Please run:\")\n",
    "    print(\"python -m spacy download en_core_web_sm\")\n",
    "    exit() # Выход, если модель не найдена\n",
    "\n",
    "text_to_process = \"\"\"\n",
    "Microsoft Corporation, often called Microsoft, is an American multinational technology corporation\n",
    "headquartered in Redmond, Washington. It develops, manufactures, licenses, supports, and sells\n",
    "computer software, consumer electronics, personal computers, and related services. Its best-known\n",
    "software products are the Microsoft Windows line of operating systems, the Microsoft Office suite,\n",
    "and the Internet Explorer and Edge web browsers. Its flagship hardware products are the Xbox video\n",
    "game consoles and the Microsoft Surface lineup of touchscreen personal computers. Microsoft ranked\n",
    "No. 21 in the 2020 Fortune 500 rankings of the largest United States corporations by total revenue;\n",
    "it was the world's largest software maker by revenue as of 2016. It is considered one of the Big Five\n",
    "companies in the U.S. information technology industry, along with Google, Amazon, Apple, and Meta.\n",
    "\"\"\"\n",
    "\n",
    "# Обработка текста\n",
    "doc = nlp(text_to_process)\n",
    "\n",
    "extracted_entities = []\n",
    "lemmatized_non_stopwords = []\n",
    "\n",
    "# Итерация по токенам\n",
    "for token in doc:\n",
    "    # Проверка, является ли токен частью сущности ORG или GPE\n",
    "    if token.ent_type_ in [\"ORG\", \"GPE\"]:\n",
    "        # Добавляем всю сущность только один раз, когда встречаем ее первый токен\n",
    "        if token.i == token.ent_iob_ B-ORG or token.i == token.ent_iob_ B-GPE: # IOB схема: B-egin, I-nside, O-utside\n",
    "             # Проверяем, что это начало сущности (B-)\n",
    "             # В spaCy v3+ можно проще: if token.ent_iob == 3: # 3 corresponds to B-\n",
    "             # Но для совместимости проверим тип сущности\n",
    "             # Простой способ - добавить всю сущность, когда встретили первый токен\n",
    "             # (может добавить дубликаты, если сущность повторяется)\n",
    "             # Более надежно - проверять token.ent_iob_ == 'B'\n",
    "             # Но самый простой - просто добавить текст сущности\n",
    "             # Мы сделаем это после цикла по токенам, используя doc.ents\n",
    "\n",
    "             # Пока просто пропустим токены сущностей в этом цикле\n",
    "             pass\n",
    "    # Если токен не сущность, не стоп-слово и не пунктуация - лемматизируем\n",
    "    elif not token.is_stop and not token.is_punct and token.is_alpha:\n",
    "        lemmatized_non_stopwords.append(token.lemma_)\n",
    "\n",
    "# Извлечение уникальных сущностей нужного типа\n",
    "unique_entities = set()\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ in [\"ORG\", \"GPE\"]:\n",
    "        unique_entities.add((ent.text.strip(), ent.label_)) # Добавляем кортеж (текст, тип)\n",
    "\n",
    "print(\"\\n--- Task Results ---\")\n",
    "print(\"\\nExtracted Organizations (ORG) and Geopolitical Entities (GPE):\")\n",
    "if unique_entities:\n",
    "    for entity, label in sorted(list(unique_entities)): # Сортируем для порядка\n",
    "        print(f\"- {entity} ({label})\")\n",
    "else:\n",
    "    print(\"No ORG or GPE entities found.\")\n",
    "\n",
    "print(\"\\nLemmatized non-stopword/non-punctuation/non-entity tokens:\")\n",
    "# Выведем только часть для краткости\n",
    "print(\" \".join(lemmatized_non_stopwords[:50]) + \"...\")\n",
    "\n",
    "# --- Конец Примера ---\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 8: spaCy vs NLTK - Краткое Сравнение\n",
    "\n",
    "# | Фича                  | spaCy                                  | NLTK                                      |\n",
    "# |-----------------------|----------------------------------------|-------------------------------------------|\n",
    "# | **Философия**         | Промышленный, Мнениющаяся, Быстрая    | Академическая, Гибкая, Исследовательская |\n",
    "# | **Производительность**| Высокая (Cython)                       | Ниже                                      |\n",
    "# | **API**               | Объектно-ориентированный, Простой      | Функциональный, Более сложный            |\n",
    "# | **Предобуч. Модели** | Да, для многих языков (легко скачать) | Ограничено (нужно искать/обучать)        |\n",
    "# | **Задачи \"из коробки\"**| Токен., POS, NER, Зависимости и др.   | Токен., Стемминг, Леммы, POS, WordNet...  |\n",
    "# | **Алгоритмы**         | Обычно 1 оптимизированный вариант      | Множество алгоритмов на выбор             |\n",
    "# | **Векторы Слов**      | Встроены в `md`/`lg`/`trf` модели     | Требует Gensim или загрузки извне         |\n",
    "# | **Трансформеры**      | Интеграция через `spacy-transformers`  | Нет прямой поддержки (нужен Transformers) |\n",
    "# | **Кастомизация**      | Настройка Pipeline, Компоненты         | Высокая гибкость, но больше кода          |\n",
    "# | **Сообщество**        | Активное, Хорошая документация         | Большое, Много туториалов (но могут быть устаревшими) |\n",
    "# | **Лучше для...**      | Продакшн, Быстрое прототипирование     | Обучение, Исследования, Доступ к ресурсам |\n",
    "\n",
    "# Вывод: spaCy отлично подходит для создания реальных NLP-приложений благодаря\n",
    "# своей скорости, точности предобученных моделей и простому API. NLTK остается\n",
    "# ценным ресурсом для изучения основ NLP и доступа к разнообразным лингвистическим\n",
    "# данным и алгоритмам. Часто их используют вместе или в связке с другими\n",
    "# библиотеками (например, spaCy + Transformers).\n",
    "\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Введение в Классификацию Текста\n",
    "\n",
    "# Что такое Классификация Текста?\n",
    "# Это задача NLP, заключающаяся в присвоении тексту одной или нескольких\n",
    "# предопределенных меток или категорий.\n",
    "# Вход: Фрагмент текста (предложение, абзац, документ).\n",
    "# Выход: Метка класса (или несколько меток) и/или оценка уверенности.\n",
    "\n",
    "# Примеры Задач:\n",
    "# - Анализ Тональности (Sentiment Analysis): Определение эмоциональной окраски\n",
    "#   (позитивная, негативная, нейтральная).\n",
    "# - Классификация по Темам (Topic Classification): Определение основной темы\n",
    "#   текста (спорт, политика, технологии, искусство).\n",
    "# - Определение Спама (Spam Detection): Является ли email спамом или нет.\n",
    "# - Определение Языка (Language Detection): На каком языке написан текст.\n",
    "# - Определение Намерения (Intent Recognition): Какова цель пользователя в\n",
    "#   запросе к чат-боту (заказ пиццы, проверка погоды).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Подходы к Классификации Текста с spaCy\n",
    "\n",
    "# spaCy предлагает несколько способов решения задач классификации текста:\n",
    "\n",
    "# 1. Использование Встроенного Компонента `textcat`:\n",
    "#    - spaCy предоставляет компонент конвейера (pipeline component) `textcat`\n",
    "#      (и его варианты `textcat_bow`, `textcat_cnn`, `textcat_ensemble`),\n",
    "#      специально разработанный для классификации текстов.\n",
    "#    - Он обучается на ваших данных и может быть добавлен в конвейер spaCy.\n",
    "#    - **Это основной способ, который мы рассмотрим в этом туториале.**\n",
    "\n",
    "# 2. Извлечение Признаков + Внешний Классификатор:\n",
    "#    - Использовать spaCy для предобработки и извлечения признаков из текста:\n",
    "#      - Векторы слов/документов (из моделей `md` или `lg`).\n",
    "#      - Частоты токенов/лемм (похоже на BoW/TF-IDF).\n",
    "#      - POS-теги, информация о зависимостях.\n",
    "#    - Затем использовать эти признаки для обучения классификатора из другой\n",
    "#      библиотеки, например, Scikit-learn (SVM, Logistic Regression, Naive Bayes).\n",
    "#    - Гибкий подход, но требует больше ручной работы по извлечению признаков.\n",
    "\n",
    "# 3. Использование Трансформеров через `spacy-transformers`:\n",
    "#    - Библиотека `spacy-transformers` интегрирует модели из Hugging Face\n",
    "#      (BERT, RoBERTa, GPT и т.д.) в конвейер spaCy.\n",
    "#    - Можно использовать предобученные трансформеры для классификации или\n",
    "#      дообучить их на своих данных внутри экосистемы spaCy.\n",
    "#    - Обеспечивает state-of-the-art результаты, но требует больше ресурсов.\n",
    "\n",
    "# Мы сфокусируемся на встроенном компоненте `textcat`.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Компонент `textcat` в spaCy\n",
    "\n",
    "# Архитектура `textcat`:\n",
    "# - spaCy предлагает несколько архитектур для `textcat`:\n",
    "#   - `textcat_bow` (Bag-of-Words): Использует простой подход \"мешка слов\"\n",
    "#     (учитывает n-граммы слов/символов) для создания вектора признаков,\n",
    "#     который затем подается в линейный классификатор или простую нейросеть.\n",
    "#     Быстрый, хорошо работает на простых задачах или как базовый вариант.\n",
    "#   - `textcat_cnn` (Convolutional Neural Network): Использует сверточную\n",
    "#     нейронную сеть (обычно с эмбеддингами слов) для извлечения признаков\n",
    "#     из текста. Учитывает локальный порядок слов. Часто дает лучшие\n",
    "#     результаты, чем BoW, но медленнее обучается и работает.\n",
    "#   - `textcat_ensemble`: Комбинирует предсказания `textcat_bow` и `textcat_cnn`\n",
    "#     для потенциально лучшей точности.\n",
    "# - Выбор архитектуры происходит при конфигурации конвейера. По умолчанию\n",
    "#   (если просто добавить `textcat`) часто используется ансамбль или CNN.\n",
    "\n",
    "# Типы Классификации:\n",
    "# - Эксклюзивная (Exclusive): Текст может принадлежать только к ОДНОМУ классу.\n",
    "#   (Например, тональность: только позитивная ИЛИ негативная).\n",
    "# - Мульти-лейбл (Multi-label): Текст может принадлежать к НЕСКОЛЬКИМ классам\n",
    "#   одновременно. (Например, темы: текст может быть и про \"спорт\", и про \"политику\").\n",
    "# - `textcat` поддерживает оба типа.\n",
    "\n",
    "# Формат Данных для Обучения:\n",
    "# - Обучающие данные должны быть представлены в виде списка кортежей.\n",
    "# - Каждый кортеж: `(текст, словарь_аннотаций)`\n",
    "# - `текст`: Строка с текстом для классификации.\n",
    "# - `словарь_аннотаций`: Словарь, содержащий ключ `'cats'`.\n",
    "#   - Значение по ключу `'cats'`: Другой словарь, где ключи - это имена ваших\n",
    "#     классов (строки), а значения - это оценки (scores):\n",
    "#     - Для эксклюзивной классификации: 1.0 для истинного класса, 0.0 для остальных.\n",
    "#       Пример: `{\"cats\": {\"POSITIVE\": 1.0, \"NEGATIVE\": 0.0}}`\n",
    "#     - Для мульти-лейбл классификации: 1.0 для классов, к которым текст относится,\n",
    "#       0.0 для тех, к которым не относится.\n",
    "#       Пример: `{\"cats\": {\"SPORTS\": 1.0, \"POLITICS\": 1.0, \"TECHNOLOGY\": 0.0}}`\n",
    "\n",
    "# Пример данных для эксклюзивной классификации (тональность):\n",
    "# TRAIN_DATA = [\n",
    "#     (\"This is great!\", {\"cats\": {\"POSITIVE\": 1.0, \"NEGATIVE\": 0.0}}),\n",
    "#     (\"I hated it.\", {\"cats\": {\"POSITIVE\": 0.0, \"NEGATIVE\": 1.0}}),\n",
    "#     (\"What a wonderful movie.\", {\"cats\": {\"POSITIVE\": 1.0, \"NEGATIVE\": 0.0}}),\n",
    "#     (\"Terrible experience.\", {\"cats\": {\"POSITIVE\": 0.0, \"NEGATIVE\": 1.0}}),\n",
    "#     # ... больше данных\n",
    "# ]\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Обучение Модели `textcat`\n",
    "\n",
    "# Шаги Обучения:\n",
    "\n",
    "# 1. Загрузка Базовой Модели или Создание Пустой:\n",
    "#    - Можно начать с предобученной модели spaCy (например, `en_core_web_sm`)\n",
    "#      и дообучить ее компонент `textcat` (если он там есть) или добавить новый.\n",
    "#    - Часто лучше начать с пустой модели (`spacy.blank(\"en\")`), особенно если\n",
    "#      ваши данные сильно отличаются от тех, на которых обучалась базовая модель,\n",
    "#      или если вам не нужны другие компоненты (POS, NER).\n",
    "import spacy\n",
    "import random\n",
    "from spacy.training.example import Example # Для создания обучающих примеров\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\") # Вариант 1: Загрузить существующую\n",
    "nlp = spacy.blank(\"en\") # Вариант 2: Создать пустую модель для английского\n",
    "\n",
    "# 2. Добавление Компонента `textcat` в Конвейер:\n",
    "#    - Если компонент еще не существует, его нужно добавить.\n",
    "#    - Нужно указать конфигурацию, включая архитектуру (`\"cnn\"`, `\"bow\"`, `\"ensemble\"`)\n",
    "#      и тип классификации (`exclusive_classes=True` или `False`).\n",
    "if \"textcat\" not in nlp.pipe_names:\n",
    "    # Указываем архитектуру (например, CNN) и тип (эксклюзивные классы)\n",
    "    config = {\n",
    "        \"threshold\": 0.5,\n",
    "        # \"model\": { # Можно детальнее настроить архитектуру\n",
    "        #     \"@architectures\": \"spacy.TextCatCNN.v2\",\n",
    "        #     \"exclusive_classes\": True,\n",
    "        #     \"ngram_size\": 1,\n",
    "        #     \"pretrained_vectors\": None, # Использовать ли векторы из базовой модели\n",
    "        #     \"width\": 64\n",
    "        # }\n",
    "    }\n",
    "    # Добавляем компонент с настройками по умолчанию (часто ensemble или cnn)\n",
    "    # и указываем, что классы эксклюзивные\n",
    "    textcat = nlp.add_pipe(\"textcat\", last=True, config={\"exclusive_classes\": True})\n",
    "    print(\"Added 'textcat' component to the pipeline.\")\n",
    "else:\n",
    "    textcat = nlp.get_pipe(\"textcat\")\n",
    "    print(\"Found existing 'textcat' component.\")\n",
    "\n",
    "# 3. Добавление Меток Классов:\n",
    "#    - Компоненту `textcat` нужно сообщить, какие классы он должен предсказывать.\n",
    "textcat.add_label(\"POSITIVE\")\n",
    "textcat.add_label(\"NEGATIVE\")\n",
    "print(\"Added labels 'POSITIVE', 'NEGATIVE' to textcat.\")\n",
    "\n",
    "# 4. Подготовка Обучающих Данных:\n",
    "#    - Данные должны быть в формате `(text, {\"cats\": {\"LABEL1\": score1, ...}})`\n",
    "#    - (См. пример TRAIN_DATA в Блоке 3)\n",
    "\n",
    "# 5. Обучающий Цикл:\n",
    "#    - Обучение происходит итеративно, прогоняя данные через модель несколько раз (эпох).\n",
    "#    - Используется `nlp.update()` для обновления весов модели.\n",
    "#    - Важно перемешивать данные (`random.shuffle`) перед каждой эпохой.\n",
    "#    - Рекомендуется отключать другие компоненты конвейера (`ner`, `parser`),\n",
    "#      если они не нужны для `textcat`, чтобы ускорить обучение.\n",
    "\n",
    "# Пример обучающего цикла (псевдокод):\n",
    "# n_iterations = 10 # Количество эпох\n",
    "# optimizer = nlp.begin_training() # Инициализация оптимизатора\n",
    "#\n",
    "# for itn in range(n_iterations):\n",
    "#     random.shuffle(TRAIN_DATA)\n",
    "#     losses = {}\n",
    "#     for text, annotations in TRAIN_DATA:\n",
    "#         # Создаем объект Example для обучения\n",
    "#         doc = nlp.make_doc(text) # Создаем Doc без аннотаций\n",
    "#         example = Example.from_dict(doc, annotations) # Объединяем текст и аннотации\n",
    "#         # Обновляем модель\n",
    "#         nlp.update([example], sgd=optimizer, losses=losses)\n",
    "#     print(f\"Epoch {itn+1}, Losses: {losses}\")\n",
    "\n",
    "# 6. Сохранение Обученной Модели:\n",
    "#    - После обучения модель можно сохранить на диск.\n",
    "# output_dir = \"./my_textcat_model\"\n",
    "# nlp.to_disk(output_dir)\n",
    "# print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "# 7. Загрузка Обученной Модели:\n",
    "# nlp_loaded = spacy.load(output_dir)\n",
    "# print(f\"Loaded model from {output_dir}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Оценка и Инференс\n",
    "\n",
    "# Оценка Модели:\n",
    "# - После обучения модель нужно оценить на отложенной тестовой выборке (данные, которые модель не видела).\n",
    "# - Используйте стандартные метрики классификации: Accuracy, Precision, Recall, F1-score.\n",
    "# - spaCy предоставляет утилиту `Scorer` для оценки.\n",
    "#   ```python\n",
    "#   from spacy.scorer import Scorer\n",
    "#\n",
    "#   def evaluate(nlp_model, test_data):\n",
    "#       examples = []\n",
    "#       scorer = Scorer()\n",
    "#       for text, annotations in test_data:\n",
    "#           doc = nlp_model.make_doc(text)\n",
    "#           example = Example.from_dict(doc, annotations)\n",
    "#           # Предсказываем на тексте из примера\n",
    "#           pred_doc = nlp_model(example.predicted)\n",
    "#           # Создаем Example с предсказанным и истинным документом\n",
    "#           example.reference = pred_doc # Используем предсказанный как референс для оценки? Нет, наоборот\n",
    "#           # Нужно создать Example с истинными аннотациями и предсказанным текстом\n",
    "#           # Или проще: передать предсказанный doc и example с истинными данными\n",
    "#           examples.append(Example(pred_doc, example.reference)) # Нужно уточнить API Scorer\n",
    "#\n",
    "#       # Оценка по категориям (textcat)\n",
    "#       scores = scorer.score_cats(examples, \"textcat\", labels=nlp_model.get_pipe(\"textcat\").labels)\n",
    "#       return scores\n",
    "#   ```\n",
    "#   *Примечание: API оценки может меняться, сверяйтесь с документацией spaCy.*\n",
    "#   Проще может быть вручную прогнать модель по тестовым данным и посчитать метрики.\n",
    "\n",
    "# Инференс (Использование Модели):\n",
    "# - Загрузите обученную модель (`nlp = spacy.load(path)`).\n",
    "# - Обработайте новый текст: `doc = nlp(\"Текст для классификации\")`.\n",
    "# - Результаты классификации хранятся в атрибуте `doc.cats`.\n",
    "#   Это словарь, где ключи - имена классов, а значения - предсказанные оценки (обычно вероятности после softmax или sigmoid).\n",
    "\n",
    "# Пример инференса:\n",
    "# test_text_positive = \"I really enjoyed this movie, it was fantastic!\"\n",
    "# doc_pos = nlp_loaded(test_text_positive)\n",
    "# print(f\"\\nText: '{test_text_positive}'\")\n",
    "# print(f\"Predicted scores: {doc_pos.cats}\")\n",
    "# # Определение предсказанного класса (для эксклюзивной классификации)\n",
    "# predicted_label_pos = max(doc_pos.cats, key=doc_pos.cats.get)\n",
    "# print(f\"Predicted label: {predicted_label_pos}\")\n",
    "#\n",
    "# test_text_negative = \"Absolutely terrible, I want my money back.\"\n",
    "# doc_neg = nlp_loaded(test_text_negative)\n",
    "# print(f\"\\nText: '{test_text_negative}'\")\n",
    "# print(f\"Predicted scores: {doc_neg.cats}\")\n",
    "# predicted_label_neg = max(doc_neg.cats, key=doc_neg.cats.get)\n",
    "# print(f\"Predicted label: {predicted_label_neg}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Пример Задачи и Решения (Обучение `textcat`)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Обучить модель spaCy (`textcat`) для классификации коротких\n",
    "# текстовых сообщений на два класса: \"SPAM\" и \"HAM\" (не спам).\n",
    "# Используем небольшой синтетический набор данных.\n",
    "\n",
    "# --- Решение (Полный Код) ---\n",
    "\n",
    "import spacy\n",
    "import random\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "import os\n",
    "\n",
    "# --- 0. Настройки ---\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.001 # Не используется напрямую в nlp.update, но для информации\n",
    "MODEL_SAVE_DIR = \"./spam_textcat_model\"\n",
    "LABELS = [\"SPAM\", \"HAM\"]\n",
    "\n",
    "# --- 1. Подготовка Обучающих Данных ---\n",
    "# Формат: (текст, {\"cats\": {\"SPAM\": 0/1, \"HAM\": 0/1}})\n",
    "# Эксклюзивная классификация: только один класс может быть 1.0\n",
    "TRAIN_DATA = [\n",
    "    (\"URGENT! You have won a 1 week FREE membership\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
    "    (\"Free entry in 2 a wkly comp to win FA Cup final tkts\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
    "    (\"Winner!! As a valued network customer you have been selected\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
    "    (\"Had your mobile 11 months or more? U R entitled to Update\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
    "    (\"SIX chances to win CASH! From 100 to 20,000 pounds\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
    "    (\"Ok lar... Joking wif u oni...\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"Sorry, I'll call later\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"I'm going to try for 2 months ha ha only joking\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"Fine if that's the way u feel. That's the way its gota b\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"Is that seriously how you spell his name?\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"I‘m going to try for 2 months ha ha only joking\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"Just forced myself to eat a slice. I'm really not hungry tho.\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"Did you catch the bus ? Are you frying an egg ? \", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"I'm back & we're packing the car now, I'll let you know if there's room\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"Ahhh. Work. I vaguely remember that! What does it feel like?\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"Yeah he got in at 2 and was v apologetic. n had fallen out\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"You have WON a guaranteed £1000 cash or a £2000 prize!\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
    "    (\"PRIVATE! Your 2003 Account Statement for 077xxx shows 800 un-redeemed points.\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
    "]\n",
    "\n",
    "# Разделим на обучающую и тестовую (для примера просто возьмем часть)\n",
    "random.shuffle(TRAIN_DATA)\n",
    "split_point = int(len(TRAIN_DATA) * 0.8)\n",
    "train_data = TRAIN_DATA[:split_point]\n",
    "test_data = TRAIN_DATA[split_point:]\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")\n",
    "\n",
    "# --- 2. Создание Модели и Конвейера ---\n",
    "# Начнем с пустой английской модели\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Добавляем textcat в конвейер\n",
    "# Используем настройки по умолчанию, но указываем exclusive_classes\n",
    "config = {\"exclusive_classes\": True, \"architecture\": \"simple_cnn\"} # Попробуем CNN\n",
    "textcat = nlp.add_pipe(\"textcat\", last=True, config=config)\n",
    "\n",
    "# Добавляем метки\n",
    "for label in LABELS:\n",
    "    textcat.add_label(label)\n",
    "\n",
    "print(f\"Pipeline: {nlp.pipe_names}\")\n",
    "\n",
    "# --- 3. Обучение Модели ---\n",
    "print(\"\\nStarting training...\")\n",
    "training_start_time = time.time()\n",
    "\n",
    "# Отключаем другие пайпы (если бы они были) для эффективности\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"textcat\"]\n",
    "with nlp.disable_pipes(*other_pipes): # Контекстный менеджер для отключения\n",
    "    optimizer = nlp.begin_training() # Инициализация оптимизатора\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        # Используем батчинг\n",
    "        batches = minibatch(train_data, size=BATCH_SIZE)\n",
    "        for batch in batches:\n",
    "            # Преобразуем тексты и аннотации в объекты Example\n",
    "            examples = []\n",
    "            for text, annotations in batch:\n",
    "                doc = nlp.make_doc(text)\n",
    "                examples.append(Example.from_dict(doc, annotations))\n",
    "\n",
    "            # Обновляем модель на батче\n",
    "            nlp.update(examples, sgd=optimizer, losses=losses)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Losses: {losses}\")\n",
    "\n",
    "training_end_time = time.time()\n",
    "print(f\"Training finished in {training_end_time - training_start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 4. Сохранение Модели ---\n",
    "if not os.path.exists(MODEL_SAVE_DIR):\n",
    "    os.makedirs(MODEL_SAVE_DIR)\n",
    "nlp.to_disk(MODEL_SAVE_DIR)\n",
    "print(f\"Model saved to {MODEL_SAVE_DIR}\")\n",
    "\n",
    "# --- 5. Тестирование (Инференс) ---\n",
    "print(\"\\nLoading trained model and testing...\")\n",
    "nlp_trained = spacy.load(MODEL_SAVE_DIR)\n",
    "\n",
    "# Тестируем на данных, которые модель не видела\n",
    "print(\"\\n--- Test Results ---\")\n",
    "correct_predictions = 0\n",
    "for text, true_annotations in test_data:\n",
    "    doc = nlp_trained(text)\n",
    "    predicted_label = max(doc.cats, key=doc.cats.get)\n",
    "    true_label = max(true_annotations[\"cats\"], key=true_annotations[\"cats\"].get)\n",
    "\n",
    "    is_correct = predicted_label == true_label\n",
    "    if is_correct:\n",
    "        correct_predictions += 1\n",
    "\n",
    "    print(f\"Text: {text[:50]}...\")\n",
    "    print(f\"  True: {true_label}, Predicted: {predicted_label} (Scores: {doc.cats}) - Correct: {is_correct}\")\n",
    "\n",
    "accuracy = correct_predictions / len(test_data)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Пример на новом тексте\n",
    "print(\"\\n--- New Example ---\")\n",
    "test_text_spam = \"Claim your free prize now! Click link!\"\n",
    "doc_spam = nlp_trained(test_text_spam)\n",
    "print(f\"Text: '{test_text_spam}'\")\n",
    "print(f\"Scores: {doc_spam.cats}\")\n",
    "print(f\"Predicted: {max(doc_spam.cats, key=doc_spam.cats.get)}\")\n",
    "\n",
    "test_text_ham = \"Hey, are you free for lunch tomorrow?\"\n",
    "doc_ham = nlp_trained(test_text_ham)\n",
    "print(f\"Text: '{test_text_ham}'\")\n",
    "print(f\"Scores: {doc_ham.cats}\")\n",
    "print(f\"Predicted: {max(doc_ham.cats, key=doc_ham.cats.get)}\")\n",
    "\n",
    "# --- Конец Примера ---\n",
    "\n",
    "# --------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Введение в spacy-transformers\n",
    "\n",
    "# Что такое spacy-transformers?\n",
    "# Это пакет расширения для spaCy, который обеспечивает бесшовную интеграцию\n",
    "# с популярной библиотекой Hugging Face Transformers.\n",
    "# Он позволяет использовать state-of-the-art модели Трансформеров (BERT, RoBERTa,\n",
    "# XLNet, GPT-2, DistilBERT и многие другие) внутри конвейера (pipeline) spaCy.\n",
    "\n",
    "# Зачем использовать spacy-transformers?\n",
    "# - Доступ к Мощным Моделям: Позволяет легко применять передовые модели\n",
    "#   Трансформеров, которые отлично улавливают контекст и семантику языка.\n",
    "# - Улучшенная Точность: Использование Трансформеров в качестве источника\n",
    "#   признаков часто приводит к значительному повышению точности для\n",
    "#   последующих компонентов spaCy (NER, POS-tagging, Text Classification и т.д.).\n",
    "# - Единый Интерфейс: Работа с Трансформерами происходит через привычный\n",
    "#   объектно-ориентированный интерфейс spaCy (`nlp`, `Doc`, `Token`, `Span`).\n",
    "# - Fine-tuning: Поддерживает дообучение (fine-tuning) Трансформеров на\n",
    "#   пользовательских данных в рамках экосистемы spaCy (через `spacy train`).\n",
    "\n",
    "# Основная Идея:\n",
    "# `spacy-transformers` добавляет компонент `transformer` в конвейер spaCy.\n",
    "# Этот компонент генерирует выходы Трансформера (обычно контекстуализированные\n",
    "# эмбеддинги слов или подслов). Другие компоненты spaCy (например, `tagger`,\n",
    "# `parser`, `ner`, `textcat`) могут затем использовать эти богатые признаки\n",
    "# вместо или в дополнение к своим стандартным признакам.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Установка и Настройка\n",
    "\n",
    "# 1. Установка spacy-transformers:\n",
    "#    - Убедитесь, что у вас установлен spaCy v3+.\n",
    "#    - Установите пакет:\n",
    "#      `pip install -U spacy-transformers`\n",
    "#    - Вам также понадобится один из бэкендов глубокого обучения, поддерживаемый\n",
    "#      Hugging Face Transformers (PyTorch или TensorFlow):\n",
    "#      `pip install -U torch`  # Рекомендуется PyTorch\n",
    "#      # или\n",
    "#      # `pip install -U tensorflow`\n",
    "\n",
    "# 2. Загрузка Transformer-моделей spaCy:\n",
    "#    - `spacy-transformers` работает с моделями spaCy, которые включают\n",
    "#      конфигурацию для использования Трансформера.\n",
    "#    - Эти модели обычно имеют суффикс `_trf` в названии.\n",
    "#    - Они значительно больше по размеру, чем стандартные модели (`sm`, `md`, `lg`),\n",
    "#      так как содержат веса Трансформера.\n",
    "#    - Загрузка модели (пример для английского языка):\n",
    "#      `python -m spacy download en_core_web_trf`\n",
    "#    - Существуют `_trf` модели и для других языков (проверяйте доступность).\n",
    "\n",
    "# 3. Требования к Ресурсам:\n",
    "#    - **Важно:** Transformer-модели требуют значительно больше вычислительных\n",
    "#      ресурсов (ЦП, ОЗУ и особенно ГП) по сравнению со стандартными моделями spaCy.\n",
    "#    - Наличие GPU сильно рекомендуется для приемлемой скорости обработки и обучения.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Использование Моделей с `spacy-transformers`\n",
    "\n",
    "import spacy\n",
    "\n",
    "# 1. Загрузка Transformer-модели spaCy:\n",
    "#    - Используйте `spacy.load()` с именем загруженной `_trf` модели.\n",
    "model_name = \"en_core_web_trf\"\n",
    "try:\n",
    "    nlp_trf = spacy.load(model_name)\n",
    "    print(f\"Loaded Transformer model: '{model_name}'\")\n",
    "except OSError:\n",
    "    print(f\"Error: Model '{model_name}' not found. Please run:\")\n",
    "    print(f\"python -m spacy download {model_name}\")\n",
    "    # Создадим пустой объект для предотвращения ошибок далее, но он бесполезен\n",
    "    nlp_trf = spacy.blank(\"en\")\n",
    "\n",
    "# 2. Просмотр Конвейера (Pipeline):\n",
    "#    - В конвейере загруженной модели должен присутствовать компонент `transformer`.\n",
    "#    - Другие компоненты (tagger, parser, ner и т.д.) будут сконфигурированы\n",
    "#      для использования выходов `transformer`.\n",
    "# print(f\"\\nPipeline components: {nlp_trf.pipe_names}\")\n",
    "# Ожидаемый вывод (может немного отличаться): ['transformer', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
    "\n",
    "# 3. Обработка Текста:\n",
    "#    - Используйте объект `nlp` как обычно. Обработка может занять больше времени.\n",
    "text_example = \"Apple Inc. is planning to open a new store in London.\"\n",
    "# doc = nlp_trf(text_example)\n",
    "# print(f\"\\nProcessed text: '{text_example}'\")\n",
    "\n",
    "# 4. Доступ к Аннотациям:\n",
    "#    - Доступ к POS-тегам, зависимостям, сущностям и т.д. осуществляется\n",
    "#      так же, как и в стандартном spaCy (`token.pos_`, `token.dep_`, `doc.ents`).\n",
    "#    - Однако эти аннотации теперь генерируются с использованием признаков\n",
    "#      из Трансформера, что обычно повышает их качество.\n",
    "\n",
    "# print(\"\\nTokens and POS tags (Transformer-backed):\")\n",
    "# for token in doc:\n",
    "#     print(f\"  {token.text:<10} {token.pos_:<8} {token.tag_:<8}\")\n",
    "\n",
    "# print(\"\\nNamed Entities (Transformer-backed):\")\n",
    "# if doc.ents:\n",
    "#     for ent in doc.ents:\n",
    "#         print(f\"  {ent.text:<20} {ent.label_:<10}\")\n",
    "# else:\n",
    "#     print(\"  No entities found.\")\n",
    "\n",
    "# 5. Доступ к Выходам Трансформера (Продвинутый Уровень):\n",
    "#    - `spacy-transformers` сохраняет детальные выходы Трансформера\n",
    "#      (эмбеддинги токенов, скрытые состояния) в пользовательских атрибутах `Doc`.\n",
    "#    - Доступ через `doc._.trf_data`. Это может быть полезно для специфических задач.\n",
    "# try:\n",
    "#     trf_data = doc._.trf_data\n",
    "#     # print(f\"\\nTransformer data available: {trf_data is not None}\")\n",
    "#     # print(f\"Type of trf_data: {type(trf_data)}\") # Обычно свой класс TrfData\n",
    "#     # Пример доступа к эмбеддингам последнего слоя (может зависеть от версии)\n",
    "#     # last_hidden_states = trf_data.last_hidden_state\n",
    "#     # print(f\"Shape of last hidden states: {last_hidden_states.shape}\") # (num_wp_tokens, hidden_size)\n",
    "#     # Эмбеддинги уровня spaCy токенов (усредненные для токенов, разбитых на подслова)\n",
    "#     # token_embeddings = trf_data.tensors[-1] # Может быть в другом месте\n",
    "#     # print(f\"Shape of token embeddings: {token_embeddings.shape}\") # (num_spacy_tokens, hidden_size)\n",
    "# except AttributeError:\n",
    "#      print(\"\\nTransformer data (`doc._.trf_data`) not found. Model might be blank.\")\n",
    "\n",
    "\n",
    "# 6. Сходство (Similarity):\n",
    "#    - Если модель `_trf` включает векторы (обычно это так), методы `.similarity()`\n",
    "#      будут использовать контекстуализированные эмбеддинги Трансформера,\n",
    "#      что дает более точную оценку семантического сходства в контексте.\n",
    "# doc1 = nlp_trf(\"The cat sat on the mat.\")\n",
    "# doc2 = nlp_trf(\"The feline rested on the rug.\")\n",
    "# doc3 = nlp_trf(\"Berlin is the capital of Germany.\")\n",
    "#\n",
    "# print(f\"\\nSimilarity (Transformer-based):\")\n",
    "# print(f\"doc1 vs doc2: {doc1.similarity(doc2):.4f}\") # Ожидается высокое сходство\n",
    "# print(f\"doc1 vs doc3: {doc1.similarity(doc3):.4f}\") # Ожидается низкое сходство\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Классификация Текста с `spacy-transformers`\n",
    "\n",
    "# Использование `textcat` с Трансформером:\n",
    "# - Вы можете добавить компонент `textcat` в конвейер, который уже содержит\n",
    "#   компонент `transformer`.\n",
    "# - При обучении `textcat` автоматически будет использовать выходы\n",
    "#   Трансформера в качестве признаков (если настроено соответствующим образом,\n",
    "#   что обычно происходит по умолчанию в spaCy v3 при наличии `transformer`).\n",
    "# - Это позволяет обучать классификатор текста, используя мощные\n",
    "#   контекстуализированные представления.\n",
    "\n",
    "# Процесс Обучения:\n",
    "# 1.  **Загрузить `_trf` модель:** `nlp = spacy.load(\"en_core_web_trf\")`\n",
    "# 2.  **Добавить `textcat`:**\n",
    "#     ```python\n",
    "#     if \"textcat\" not in nlp.pipe_names:\n",
    "#         # Конфигурация может указывать на использование Transformer features\n",
    "#         # Часто это происходит автоматически, если 'transformer' есть в пайплайне\n",
    "#         config = {\"exclusive_classes\": True} # Пример для эксклюзивной\n",
    "#         textcat = nlp.add_pipe(\"textcat\", last=True, config=config)\n",
    "#     else:\n",
    "#         textcat = nlp.get_pipe(\"textcat\")\n",
    "#     ```\n",
    "# 3.  **Добавить метки:** `textcat.add_label(\"LABEL1\")`, ...\n",
    "# 4.  **Подготовить данные:** Формат `(text, {\"cats\": {...}})` остается тем же.\n",
    "# 5.  **Обучающий цикл:** Используйте `nlp.update()` как и раньше. spaCy\n",
    "#     автоматически передаст признаки из `transformer` в `textcat`.\n",
    "#     ```python\n",
    "#     optimizer = nlp.resume_training() # Или begin_training, если textcat новый\n",
    "#     # ... цикл по эпохам и батчам ...\n",
    "#     # Отключаем другие компоненты, КРОМЕ 'transformer' и 'textcat'\n",
    "#     other_pipes = [p for p in nlp.pipe_names if p not in [\"transformer\", \"textcat\"]]\n",
    "#     with nlp.disable_pipes(*other_pipes):\n",
    "#          for batch in batches:\n",
    "#              examples = [...] # Создаем Example\n",
    "#              nlp.update(examples, sgd=optimizer, losses=losses)\n",
    "#     ```\n",
    "# 6.  **Сохранить/Загрузить/Оценить/Использовать:** Так же, как и для обычной модели `textcat`.\n",
    "\n",
    "# Преимущество: `textcat` обучается на гораздо более мощных признаках,\n",
    "# что обычно приводит к лучшей точности по сравнению с `textcat_bow` или `textcat_cnn`\n",
    "# на стандартных эмбеддингах.\n",
    "\n",
    "# Недостаток: Обучение и инференс будут значительно медленнее и требовательнее к ресурсам.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Fine-tuning Трансформеров (Продвинутый Уровень)\n",
    "\n",
    "# `spacy-transformers` также позволяет дообучать (fine-tune) сами веса\n",
    "# Трансформера на вашей конкретной задаче (например, классификации текста)\n",
    "# одновременно с обучением компонента задачи (например, `textcat`).\n",
    "\n",
    "# Как это работает:\n",
    "# - Используется команда `spacy train` с детальным файлом конфигурации (`config.cfg`).\n",
    "# - В конфигурации указывается базовая Transformer-модель, компоненты конвейера\n",
    "#   (включая `transformer` и, например, `textcat`), данные для обучения/валидации,\n",
    "#   параметры обучения (learning rate, batch size, эпохи).\n",
    "# - Во время `spacy train` градиенты проходят через компонент задачи (`textcat`)\n",
    "#   и обратно в компонент `transformer`, обновляя веса базовой модели Hugging Face.\n",
    "\n",
    "# Пример Фрагмента Конфигурации (`config.cfg`):\n",
    "# ```ini\n",
    "# [nlp]\n",
    "# lang = \"en\"\n",
    "# pipeline = [\"transformer\", \"textcat\"]\n",
    "# batch_size = 128\n",
    "#\n",
    "# [components]\n",
    "#\n",
    "# [components.transformer]\n",
    "# factory = \"transformer\"\n",
    "# model = \"roberta-base\" # Указываем модель Hugging Face\n",
    "# # ... другие параметры transformer ...\n",
    "#\n",
    "# [components.textcat]\n",
    "# factory = \"textcat\"\n",
    "# # Указываем, что textcat должен использовать выходы transformer\n",
    "# upstream_name = \"transformer\" # Не всегда нужно явно указывать в spaCy v3+\n",
    "# exclusive_classes = true\n",
    "# # ... другие параметры textcat ...\n",
    "#\n",
    "# [training]\n",
    "# frozen_components = [] # Пусто, значит обучаем и transformer, и textcat\n",
    "# # Или frozen_components = [\"transformer\"] для обучения только textcat поверх замороженного трансформера\n",
    "# # ... параметры оптимизатора, расписания LR ...\n",
    "# ```\n",
    "\n",
    "# Запуск Обучения:\n",
    "# `python -m spacy train config.cfg --output ./my_tuned_trf_model --paths.train train.spacy --paths.dev dev.spacy`\n",
    "# (Требует данные в формате `.spacy`, создаваемом с помощью `spacy convert`)\n",
    "\n",
    "# Это мощный подход для достижения максимальной точности, но он сложнее в настройке\n",
    "# и требует значительных вычислительных ресурсов (GPU обязателен).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Пример Задачи - Классификация Спама с `spacy-transformers`\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Повторно решить задачу классификации SMS на \"SPAM\" и \"HAM\",\n",
    "# но на этот раз используя предобученную Transformer-модель (`_trf`)\n",
    "# в качестве основы для обучения компонента `textcat`.\n",
    "\n",
    "# --- Решение (Полный Код) ---\n",
    "\n",
    "import spacy\n",
    "import random\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch, compounding\n",
    "import os\n",
    "import time\n",
    "import torch # Проверим наличие GPU\n",
    "\n",
    "# --- 0. Настройки ---\n",
    "NUM_EPOCHS = 5 # Меньше эпох, т.к. трансформеры дольше обучаются\n",
    "BATCH_SIZE = 2 # Маленький батч из-за памяти\n",
    "LEARNING_RATE = 2e-5 # Типичный LR для fine-tuning трансформеров (но spaCy управляет им)\n",
    "MODEL_SAVE_DIR_TRF = \"./spam_textcat_trf_model\"\n",
    "LABELS = [\"SPAM\", \"HAM\"]\n",
    "BASE_MODEL = \"en_core_web_trf\" # Используем Transformer-модель\n",
    "\n",
    "# Проверка GPU\n",
    "use_gpu = spacy.prefer_gpu()\n",
    "print(f\"Using GPU: {use_gpu}\")\n",
    "if use_gpu:\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\") # Опционально, для ускорения\n",
    "\n",
    "# --- 1. Подготовка Обучающих Данных ---\n",
    "# Используем те же данные, что и в предыдущем примере textcat\n",
    "TRAIN_DATA = [\n",
    "    (\"URGENT! You have won a 1 week FREE membership\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
    "    (\"Free entry in 2 a wkly comp to win FA Cup final tkts\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
    "    (\"Winner!! As a valued network customer you have been selected\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
    "    (\"Had your mobile 11 months or more? U R entitled to Update\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
    "    (\"SIX chances to win CASH! From 100 to 20,000 pounds\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
    "    (\"Ok lar... Joking wif u oni...\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"Sorry, I'll call later\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"I'm going to try for 2 months ha ha only joking\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"Fine if that's the way u feel. That's the way its gota b\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"Is that seriously how you spell his name?\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"I‘m going to try for 2 months ha ha only joking\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"Just forced myself to eat a slice. I'm really not hungry tho.\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"Did you catch the bus ? Are you frying an egg ? \", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"I'm back & we're packing the car now, I'll let you know if there's room\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"Ahhh. Work. I vaguely remember that! What does it feel like?\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"Yeah he got in at 2 and was v apologetic. n had fallen out\", {\"cats\": {\"SPAM\": 0.0, \"HAM\": 1.0}}),\n",
    "    (\"You have WON a guaranteed £1000 cash or a £2000 prize!\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
    "    (\"PRIVATE! Your 2003 Account Statement for 077xxx shows 800 un-redeemed points.\", {\"cats\": {\"SPAM\": 1.0, \"HAM\": 0.0}}),\n",
    "]\n",
    "random.shuffle(TRAIN_DATA)\n",
    "split_point = int(len(TRAIN_DATA) * 0.8)\n",
    "train_data = TRAIN_DATA[:split_point]\n",
    "test_data = TRAIN_DATA[split_point:]\n",
    "print(f\"Training data size: {len(train_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")\n",
    "\n",
    "# --- 2. Загрузка Модели и Добавление/Настройка textcat ---\n",
    "try:\n",
    "    nlp = spacy.load(BASE_MODEL)\n",
    "    print(f\"Loaded base Transformer model: '{BASE_MODEL}'\")\n",
    "except OSError:\n",
    "    print(f\"Error: Model '{BASE_MODEL}' not found. Please run:\")\n",
    "    print(f\"python -m spacy download {BASE_MODEL}\")\n",
    "    exit()\n",
    "\n",
    "# Проверяем, есть ли textcat, и добавляем/настраиваем его\n",
    "if \"textcat\" not in nlp.pipe_names:\n",
    "    # Конфигурация для textcat, использующего трансформер\n",
    "    # В spaCy v3+ обычно достаточно добавить его после трансформера\n",
    "    config = {\"exclusive_classes\": True}\n",
    "    textcat = nlp.add_pipe(\"textcat\", last=True, config=config)\n",
    "    print(\"Added 'textcat' component.\")\n",
    "else:\n",
    "    textcat = nlp.get_pipe(\"textcat\")\n",
    "    print(\"Found existing 'textcat' component.\")\n",
    "\n",
    "# Добавляем метки (если textcat был добавлен или пуст)\n",
    "existing_labels = textcat.labels\n",
    "needs_labels = False\n",
    "for label in LABELS:\n",
    "    if label not in existing_labels:\n",
    "        textcat.add_label(label)\n",
    "        needs_labels = True\n",
    "if needs_labels:\n",
    "    print(f\"Added labels {LABELS} to textcat.\")\n",
    "\n",
    "print(f\"Pipeline: {nlp.pipe_names}\")\n",
    "\n",
    "# --- 3. Обучение Модели ---\n",
    "print(\"\\nStarting training (will be slower due to Transformer)...\")\n",
    "training_start_time = time.time()\n",
    "\n",
    "# Отключаем все компоненты, кроме transformer и textcat\n",
    "# Это важно, чтобы не тратить время на ненужные вычисления (NER, parser и т.д.)\n",
    "# и чтобы они не мешали обучению textcat\n",
    "pipe_exceptions = [\"transformer\", \"textcat\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    # Можно использовать nlp.resume_training() если textcat уже был в модели,\n",
    "    # или nlp.begin_training() если он только что добавлен или мы хотим начать с нуля.\n",
    "    # begin_training безопаснее, если мы не уверены в состоянии textcat.\n",
    "    optimizer = nlp.begin_training()\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=BATCH_SIZE)\n",
    "        batch_num = 0\n",
    "        for batch in batches:\n",
    "            batch_num += 1\n",
    "            examples = []\n",
    "            for text, annotations in batch:\n",
    "                doc = nlp.make_doc(text)\n",
    "                examples.append(Example.from_dict(doc, annotations))\n",
    "\n",
    "            # Обновляем и transformer, и textcat (если не заморожен)\n",
    "            nlp.update(examples, sgd=optimizer, losses=losses)\n",
    "            # Печатаем лосс реже, т.к. батчи медленные\n",
    "            # if batch_num % 5 == 0:\n",
    "            #     print(f\"  Epoch {epoch+1}, Batch {batch_num}, Losses: {losses}\")\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Losses: {losses}\")\n",
    "\n",
    "training_end_time = time.time()\n",
    "print(f\"Training finished in {training_end_time - training_start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 4. Сохранение Модели ---\n",
    "if not os.path.exists(MODEL_SAVE_DIR_TRF):\n",
    "    os.makedirs(MODEL_SAVE_DIR_TRF)\n",
    "nlp.to_disk(MODEL_SAVE_DIR_TRF)\n",
    "print(f\"Model saved to {MODEL_SAVE_DIR_TRF}\")\n",
    "\n",
    "# --- 5. Тестирование (Инференс) ---\n",
    "print(\"\\nLoading trained Transformer model and testing...\")\n",
    "nlp_trained_trf = spacy.load(MODEL_SAVE_DIR_TRF)\n",
    "\n",
    "print(\"\\n--- Test Results (Transformer) ---\")\n",
    "correct_predictions_trf = 0\n",
    "for text, true_annotations in test_data:\n",
    "    doc = nlp_trained_trf(text)\n",
    "    predicted_label = max(doc.cats, key=doc.cats.get)\n",
    "    true_label = max(true_annotations[\"cats\"], key=true_annotations[\"cats\"].get)\n",
    "\n",
    "    is_correct = predicted_label == true_label\n",
    "    if is_correct:\n",
    "        correct_predictions_trf += 1\n",
    "\n",
    "    print(f\"Text: {text[:50]}...\")\n",
    "    print(f\"  True: {true_label}, Predicted: {predicted_label} (Scores: {doc.cats}) - Correct: {is_correct}\")\n",
    "\n",
    "accuracy_trf = correct_predictions_trf / len(test_data)\n",
    "print(f\"\\nTest Accuracy (Transformer): {accuracy_trf:.4f}\")\n",
    "\n",
    "# Ожидается, что точность будет выше (или сравнимой, на таком маленьком датасете разница может быть невелика),\n",
    "# чем в предыдущем примере с CNN/BOW, но обучение и инференс будут медленнее.\n",
    "\n",
    "# --- Конец Примера ---\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 7: Преимущества и Недостатки spacy-transformers\n",
    "\n",
    "# Преимущества:\n",
    "# + State-of-the-Art Точность: Использование Трансформеров значительно повышает\n",
    "#   качество понимания контекста и точность для многих задач NLP.\n",
    "# + Интеграция: Плавное включение мощных моделей в удобный пайплайн spaCy.\n",
    "# + Единый API: Работа с результатами (Doc, Token, Span) остается прежней.\n",
    "# + Fine-tuning: Возможность дообучения Трансформеров на своих данных внутри spaCy.\n",
    "\n",
    "# Недостатки:\n",
    "# - Требования к Ресурсам: Значительно выше потребление CPU/RAM, GPU крайне желателен.\n",
    "# - Скорость: Обработка текста (инференс) и обучение медленнее, чем у стандартных моделей.\n",
    "# - Размер Моделей: Модели `_trf` занимают гораздо больше места на диске.\n",
    "# - Сложность Конфигурации: Fine-tuning и детальная настройка требуют работы с файлами конфигурации.\n",
    "\n",
    "# Вывод: `spacy-transformers` - это мощный инструмент для тех, кому нужна\n",
    "# максимальная точность в задачах NLP и кто готов выделить необходимые\n",
    "# вычислительные ресурсы. Он успешно объединяет удобство spaCy с мощью\n",
    "# современных Трансформеров.\n",
    "\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Задача Прогнозирования Следующего Слова (Next Word Prediction)\n",
    "\n",
    "# Что такое Прогнозирование Следующего Слова?\n",
    "# Это задача NLP, цель которой - предсказать наиболее вероятное слово (или слова),\n",
    "# которое последует за заданной последовательностью слов (контекстом).\n",
    "# Вход: Последовательность токенов (слов, подслов).\n",
    "# Выход: Вероятностное распределение по словарю для следующего токена,\n",
    "#        или непосредственно наиболее вероятный следующий токен(ы).\n",
    "\n",
    "# Применение:\n",
    "# - Автодополнение текста (Text Autocompletion) в клавиатурах, IDE, поисковых системах.\n",
    "# - Помощь при письме (Writing Assistance).\n",
    "# - Основа для генерации текста (хотя полная генерация обычно сложнее).\n",
    "# - Компонент в системах распознавания речи или машинного перевода.\n",
    "\n",
    "# Это Фундаментальная Задача Языкового Моделирования (Language Modeling - LM):\n",
    "# По сути, прогнозирование следующего слова - это основная задача, на которой\n",
    "# обучаются многие языковые модели. Модель учится присваивать вероятности\n",
    "# последовательностям слов: P(w_n | w_1, w_2, ..., w_{n-1}).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Роль spaCy в Прогнозировании Следующего Слова\n",
    "\n",
    "# Важное Замечание:\n",
    "# spaCy **не предназначен** напрямую для задачи прогнозирования следующего слова\n",
    "# или генерации текста в том виде, в каком это делают специализированные\n",
    "# языковые модели (как GPT или RNN/LSTM).\n",
    "# У spaCy **нет встроенного компонента конвейера**, который бы принимал\n",
    "# последовательность токенов и напрямую выдавал предсказание следующего слова.\n",
    "\n",
    "# Основная Цель spaCy:\n",
    "# spaCy фокусируется на **анализе и понимании** существующего текста:\n",
    "# - Токенизация\n",
    "# - POS-теггинг\n",
    "# - Синтаксический анализ зависимостей\n",
    "# - Распознавание именованных сущностей (NER)\n",
    "# - Классификация текста (`textcat`)\n",
    "# - Извлечение признаков (векторы слов/документов)\n",
    "\n",
    "# Как spaCy Может Быть Полезен (Косвенно):\n",
    "# 1.  **Предобработка Текста:** spaCy отлично подходит для подготовки текста\n",
    "#     (токенизация, лемматизация, удаление стоп-слов), который затем может\n",
    "#     быть подан в отдельную модель для прогнозирования следующего слова.\n",
    "# 2.  **Извлечение Признаков:** Векторные представления слов или документов,\n",
    "#     полученные с помощью моделей spaCy (`md`, `lg`, `trf`), могут служить\n",
    "#     входными признаками для вашей собственной модели прогнозирования.\n",
    "# 3.  **Доступ к Трансформерам (`spacy-transformers`):** Хотя `spacy-transformers`\n",
    "#     интегрирует модели типа BERT/GPT, сам интерфейс spaCy не предоставляет\n",
    "#     прямого и удобного способа использовать эти модели для *генерации* или\n",
    "#     *прогнозирования следующего токена* так, как это делает библиотека\n",
    "#     Hugging Face Transformers. spaCy использует их для улучшения *анализа*.\n",
    "\n",
    "# Вывод: Для непосредственного решения задачи прогнозирования следующего слова\n",
    "# следует использовать другие инструменты, более подходящие для языкового моделирования\n",
    "# и генерации последовательностей.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Подходящие Инструменты и Подходы\n",
    "\n",
    "# 1. N-граммы (Классический Статистический Подход):\n",
    "#    - Принцип: Вероятность следующего слова зависит только от `N-1` предыдущих слов.\n",
    "#      Например, в триграммной модели P(w_n | w_1..w_{n-1}) ≈ P(w_n | w_{n-2}, w_{n-1}).\n",
    "#    - Обучение: Подсчет частот n-грамм в большом текстовом корпусе.\n",
    "#    - Библиотеки: NLTK предоставляет инструменты для работы с n-граммами.\n",
    "#    - Недостатки: Плохо справляется с длинными зависимостями, проблема разреженности данных\n",
    "#      (многие n-граммы не встречаются в обучающих данных), большой размер модели.\n",
    "\n",
    "# 2. Рекуррентные Нейронные Сети (RNN, LSTM, GRU):\n",
    "#    - Принцип: Нейронные сети, специально разработанные для обработки последовательностей.\n",
    "#      Они имеют \"память\" (скрытое состояние), которая передается от одного шага\n",
    "#      последовательности к другому, позволяя учитывать предыдущий контекст.\n",
    "#    - Обучение: Модель обучается предсказывать следующий токен на основе предыдущих\n",
    "#      токенов и своего скрытого состояния.\n",
    "#    - Библиотеки: PyTorch, TensorFlow/Keras позволяют строить и обучать такие модели.\n",
    "#    - Преимущества: Учитывают контекст произвольной длины (теоретически), лучше обобщаются.\n",
    "#    - Недостатки: Могут страдать от затухания/взрыва градиента (особенно простые RNN),\n",
    "#      сложности с очень длинными зависимостями (LSTM/GRU помогают, но не идеально),\n",
    "#      обработка происходит последовательно (сложно параллелизовать).\n",
    "\n",
    "# 3. Трансформеры (Transformers - GPT, BERT и др.):\n",
    "#    - Принцип: Используют механизм внимания (self-attention), который позволяет модели\n",
    "#      напрямую взвешивать важность всех предыдущих слов при предсказании следующего,\n",
    "#      не полагаясь на последовательную передачу скрытого состояния как в RNN.\n",
    "#    - Модели:\n",
    "#      - **GPT-подобные (Авторегрессионные):** Обучаются именно на задаче предсказания\n",
    "#        следующего слова (GPT, GPT-2, GPT-3, GPT-Neo, XLNet в авторегрессионном режиме).\n",
    "#        **Идеально подходят для этой задачи.**\n",
    "#      - **BERT-подобные (Автокодировщики):** Обучаются на задаче предсказания\n",
    "#        маскированных (пропущенных) слов (Masked Language Model - MLM) и предсказания\n",
    "#        следующего предложения. Не предназначены напрямую для предсказания *следующего*\n",
    "#        слова, но могут быть адаптированы или использованы для оценки вероятности предложений.\n",
    "#    - Библиотеки: **Hugging Face Transformers** - стандарт де-факто для работы\n",
    "#      с Трансформерами. Предоставляет тысячи предобученных моделей и удобные API.\n",
    "#    - Преимущества: State-of-the-art результаты, хорошо работают с длинными зависимостями,\n",
    "#      легко параллелизуются.\n",
    "#    - Недостатки: Требуют много данных для обучения с нуля, большие размеры моделей,\n",
    "#      высокие вычислительные требования.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Концептуальный Пример с Hugging Face Transformers\n",
    "\n",
    "# Поскольку spaCy не подходит напрямую, покажем, как это делается с помощью\n",
    "# библиотеки, предназначенной для этой задачи.\n",
    "\n",
    "# Установка:\n",
    "# pip install transformers torch # или tensorflow\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# --- Вариант 1: Использование pipeline (Самый простой) ---\n",
    "# Pipeline 'text-generation' по сути решает задачу предсказания следующих слов много раз.\n",
    "# Мы можем ограничить количество генерируемых токенов.\n",
    "generator_pipeline = pipeline('text-generation', model='gpt2') # Используем GPT-2\n",
    "\n",
    "prompt = \"The quick brown fox jumps over the\"\n",
    "# max_new_tokens=1 предскажет только одно следующее слово (токен)\n",
    "# num_return_sequences позволяет получить несколько вариантов\n",
    "results = generator_pipeline(prompt, max_new_tokens=5, num_return_sequences=3)\n",
    "\n",
    "# print(f\"\\n--- Pipeline Results for: '{prompt}' ---\")\n",
    "# for i, result in enumerate(results):\n",
    "#     # Извлекаем только сгенерированную часть\n",
    "#     generated_text = result['generated_text'][len(prompt):].strip()\n",
    "#     # Первое слово в сгенерированной части - наше предсказание\n",
    "#     next_word_prediction = generated_text.split()[0] if generated_text else \"[No prediction]\"\n",
    "#     print(f\"Option {i+1}: Next word -> '{next_word_prediction}' (Full: '{generated_text}')\")\n",
    "\n",
    "# --- Вариант 2: Ручное использование Модели и Токенизатора ---\n",
    "model_name = \"gpt2\" # Можно использовать другие авторегрессионные модели\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval() # Переводим в режим оценки\n",
    "\n",
    "input_text = \"My name is Clara and I am\"\n",
    "\n",
    "# 1. Токенизация входа\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# 2. Получение выходов модели (логитов)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits # Форма: [batch_size, sequence_length, vocab_size]\n",
    "\n",
    "# 3. Логиты для ПОСЛЕДНЕГО токена во входной последовательности\n",
    "#    Эти логиты представляют предсказание для СЛЕДУЮЩЕГО токена\n",
    "next_token_logits = logits[:, -1, :] # Берем логиты последнего токена\n",
    "\n",
    "# 4. Применение Softmax для получения вероятностей (опционально)\n",
    "# probabilities = torch.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "# 5. Нахождение наиболее вероятных следующих токенов\n",
    "top_k = 5 # Сколько лучших предсказаний мы хотим\n",
    "top_k_logits, top_k_indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
    "top_k_indices = top_k_indices.squeeze(0).tolist() # Убираем batch dim, конвертируем в список\n",
    "\n",
    "# 6. Декодирование индексов обратно в токены (слова/подслова)\n",
    "predicted_next_tokens = tokenizer.convert_ids_to_tokens(top_k_indices)\n",
    "\n",
    "# print(f\"\\n--- Manual Prediction Results for: '{input_text}' ---\")\n",
    "# print(f\"Top {top_k} predicted next tokens:\")\n",
    "# for i, token in enumerate(predicted_next_tokens):\n",
    "#     print(f\"  {i+1}. {token}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Использование spaCy для Предобработки (Пример)\n",
    "\n",
    "# Хотя spaCy не предсказывает следующее слово, его можно использовать для\n",
    "# подготовки текста перед подачей в модель языкового моделирования.\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Загружаем модель spaCy (можно и маленькую, если нужны только токены/леммы)\n",
    "try:\n",
    "    nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"\\nLoaded 'en_core_web_sm' for preprocessing.\")\n",
    "except OSError:\n",
    "    print(\"\\nWarning: 'en_core_web_sm' not found. Preprocessing example might fail.\")\n",
    "    nlp_spacy = spacy.blank(\"en\")\n",
    "\n",
    "\n",
    "raw_text = \"   This is an example sentence, with punctuation! And numbers 123.  \"\n",
    "\n",
    "# Обработка с помощью spaCy\n",
    "doc = nlp_spacy(raw_text)\n",
    "\n",
    "# Извлечение лемм, удаление стоп-слов, пунктуации, пробелов, приведение к нижнему регистру\n",
    "processed_tokens = [\n",
    "    token.lemma_.lower()\n",
    "    for token in doc\n",
    "    if not token.is_stop and not token.is_punct and not token.is_space\n",
    "]\n",
    "\n",
    "# Результат можно использовать как вход для N-граммной модели или RNN/Transformer\n",
    "preprocessed_text = \" \".join(processed_tokens)\n",
    "# print(f\"\\nRaw text: '{raw_text}'\")\n",
    "# print(f\"Preprocessed text for LM input: '{preprocessed_text}'\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Выводы\n",
    "\n",
    "# - **spaCy не предназначен для прогнозирования следующего слова.** Его сила - в анализе и извлечении информации из существующего текста.\n",
    "# - Для задачи прогнозирования следующего слова (языкового моделирования) используйте:\n",
    "#   - **Hugging Face Transformers:** Библиотека выбора для state-of-the-art результатов с моделями типа GPT. Предоставляет как высокоуровневые `pipeline`, так и низкоуровневый доступ к моделям.\n",
    "#   - **RNN/LSTM/GRU:** Можно реализовать и обучить с помощью PyTorch/TensorFlow.\n",
    "#   - **N-граммы:** Простой статистический метод (можно использовать NLTK), но с ограничениями.\n",
    "# - **spaCy может быть полезен на этапе предобработки** данных для этих моделей.\n",
    "\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Введение в Классификацию Текста по Темам (Topic Classification)\n",
    "\n",
    "# Что такое Классификация по Темам?\n",
    "# Это задача NLP, в которой текст (документ, статья, сообщение) автоматически\n",
    "# распределяется по одной или нескольким предопределенным тематическим категориям.\n",
    "# Вход: Текст.\n",
    "# Выход: Одна или несколько меток тем (например, \"Спорт\", \"Технологии\", \"Политика\").\n",
    "\n",
    "# Цель:\n",
    "# Автоматически организовать, фильтровать или маршрутизировать текстовую информацию\n",
    "# на основе ее основного содержания или темы.\n",
    "\n",
    "# Примеры Применения:\n",
    "# - Категоризация новостных статей.\n",
    "# - Маршрутизация запросов в службу поддержки по отделам (технический, биллинг).\n",
    "# - Анализ отзывов клиентов по аспектам продукта (цена, функциональность, дизайн).\n",
    "# - Фильтрация контента в социальных сетях.\n",
    "# - Организация научных публикаций.\n",
    "\n",
    "# Типы Классификации по Темам:\n",
    "# - Однометочная (Single-label): Каждый документ относится ровно к одной теме.\n",
    "# - Многометочная (Multi-label): Документ может относиться к нескольким темам одновременно\n",
    "#   (например, статья о влиянии технологий на политику может быть отнесена и к \"Технологии\", и к \"Политика\").\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Ключевые Концепции и Представление Текста\n",
    "\n",
    "# 1. Темы/Категории:\n",
    "#    - Заранее определенный набор тем, релевантных для конкретной задачи.\n",
    "#    - Требуется наличие размеченных данных, где для каждого текста указана его тема(ы).\n",
    "\n",
    "# 2. Извлечение Признаков (Feature Extraction):\n",
    "#    - Преобразование текста в числовой формат, понятный моделям машинного обучения.\n",
    "#    - Распространенные методы:\n",
    "#      - **Bag-of-Words (BoW):** Представление текста как вектора частот слов.\n",
    "#        Просто, но игнорирует порядок и семантику.\n",
    "#      - **TF-IDF (Term Frequency-Inverse Document Frequency):** Улучшение BoW,\n",
    "#        взвешивающее слова по их важности в документе и редкости в корпусе.\n",
    "#        Часто является сильным базовым вариантом для классических моделей ML.\n",
    "#      - **Векторные Представления Слов (Word Embeddings - Word2Vec, GloVe, FastText):**\n",
    "#        Плотные векторы, захватывающие семантику слов. Для представления документа\n",
    "#        векторы слов часто усредняют или суммируют (что может терять информацию).\n",
    "#      - **Векторные Представления Документов (Document Embeddings - Doc2Vec, Sentence-BERT):**\n",
    "#        Модели, специально обученные генерировать единый вектор для всего текста,\n",
    "#        стараясь сохранить его семантический смысл.\n",
    "#      - **Контекстуализированные Эмбеддинги (BERT, RoBERTa и др.):** Генерируются\n",
    "#        моделями глубокого обучения (Трансформерами) и учитывают контекст.\n",
    "#        Обычно используются как вход для DL моделей, а не как статические признаки.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Подходы и Алгоритмы\n",
    "\n",
    "# --- 3.1 Традиционное Машинное Обучение (Supervised ML) ---\n",
    "# # Требует явного шага извлечения признаков (BoW, TF-IDF).\n",
    "# # Алгоритмы:\n",
    "# # - Наивный Байес (Naive Bayes - MultinomialNB, BernoulliNB):\n",
    "# #   - Вероятностный подход, основанный на теореме Байеса.\n",
    "# #   - Простой, быстрый, часто дает удивительно хорошие результаты для текста (особенно с TF-IDF).\n",
    "# #   - Хороший базовый вариант (baseline).\n",
    "# # - Метод Опорных Векторов (Support Vector Machines - SVM):\n",
    "# #   - Находит оптимальную гиперплоскость для разделения классов.\n",
    "# #   - Очень эффективен для высокоразмерных и разреженных данных (как TF-IDF векторы).\n",
    "# #   - Часто показывает высокую точность. Рекомендуется LinearSVC для текста.\n",
    "# # - Логистическая Регрессия (Logistic Regression):\n",
    "# #   - Линейная модель, предсказывающая вероятность принадлежности к классу.\n",
    "# #   - Простая, интерпретируемая, хороший baseline.\n",
    "# # - Деревья Решений и Ансамбли (Random Forest, Gradient Boosting):\n",
    "# #   - Могут использоваться, но часто уступают SVM или Naive Bayes на текстовых данных с высокой размерностью признаков.\n",
    "# # Библиотеки: Scikit-learn (`sklearn`) - основной инструмент в Python.\n",
    "\n",
    "# --- 3.2 Глубокое Обучение (Deep Learning) ---\n",
    "# # Автоматически изучает признаки из текста. Не требует ручного feature engineering.\n",
    "# # Архитектуры:\n",
    "# # - Сверточные Нейронные Сети (CNN):\n",
    "# #   - Используют 1D свертки для захвата локальных паттернов (n-грамм) в тексте.\n",
    "# #   - Эффективны для классификации, могут быть быстрыми.\n",
    "# # - Рекуррентные Нейронные Сети (RNN - LSTM, GRU):\n",
    "# #   - Обрабатывают текст последовательно, учитывая порядок слов и зависимости.\n",
    "# #   - Хороши для понимания структуры предложения, но могут быть медленными.\n",
    "# # - Трансформеры (Transformers - BERT, RoBERTa, XLNet, DistilBERT и др.):\n",
    "# #   - Используют механизм self-attention для улавливания зависимостей между словами независимо от их расстояния.\n",
    "# #   - **State-of-the-art подход.** Обычно используется fine-tuning предобученной модели\n",
    "# #     на конкретную задачу классификации тем.\n",
    "# # Библиотеки: PyTorch, TensorFlow/Keras, Hugging Face Transformers.\n",
    "\n",
    "# --- 3.3 Тематическое Моделирование (Topic Modeling - Unsupervised) ---\n",
    "# # Важно отличать от классификации тем!\n",
    "# # Цель: Автоматически обнаружить скрытые (\"латентные\") темы в коллекции *неразмеченных* документов.\n",
    "# # Алгоритмы: LDA (Latent Dirichlet Allocation), NMF (Non-negative Matrix Factorization).\n",
    "# # Выход: Распределение тем для каждого документа и распределение слов для каждой темы.\n",
    "# # Не использует предопределенные категории.\n",
    "# # Может использоваться для анализа данных или как шаг для генерации признаков для последующей *супервизорной* классификации.\n",
    "# # Библиотеки: Gensim, Scikit-learn.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Подготовка Данных и Предобработка\n",
    "\n",
    "# 1. Сбор Данных: Нужен корпус текстов с присвоенными метками тем (для supervised learning).\n",
    "# 2. Очистка Текста:\n",
    "#    - Приведение к нижнему регистру.\n",
    "#    - Удаление HTML-тегов, URL, email-адресов.\n",
    "#    - Удаление специальных символов, лишних пробелов.\n",
    "#    - Обработка чисел (удалить или заменить на специальный токен).\n",
    "# 3. Токенизация: Разделение на слова или подслова (для некоторых моделей).\n",
    "# 4. Удаление Стоп-слов: Удаление неинформативных слов.\n",
    "# 5. Стемминг / Лемматизация:\n",
    "#    - Приведение слов к базовой форме. Лемматизация предпочтительнее, так как сохраняет смысл.\n",
    "#    - Может быть полезно для BoW/TF-IDF, но не всегда нужно (или даже вредно) для моделей на основе эмбеддингов или трансформеров, которые могут извлекать пользу из морфологии.\n",
    "# 6. Векторизация: Преобразование очищенных токенов в числовые векторы (BoW, TF-IDF, Embeddings).\n",
    "# 7. Разделение Данных: Разбить на обучающую (train), валидационную (validation) и тестовую (test) выборки. Валидационная используется для настройки гиперпараметров, тестовая - для финальной оценки.\n",
    "# 8. Обработка Несбалансированных Классов: Если какие-то темы встречаются гораздо реже других, это может сместить модель. Техники:\n",
    "#    - Взвешивание классов (class weighting) в функции потерь.\n",
    "#    - Undersampling (уменьшение выборки мажоритарного класса).\n",
    "#    - Oversampling (увеличение выборки миноритарного класса, например, с помощью SMOTE).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Метрики Оценки\n",
    "\n",
    "# Выбор метрики зависит от задачи (одно- или многометочная) и баланса классов.\n",
    "\n",
    "# - Accuracy (Точность): Доля правильно классифицированных документов.\n",
    "#   `accuracy = (TP + TN) / (TP + TN + FP + FN)`\n",
    "#   Проста, но неинформативна при дисбалансе классов.\n",
    "# - Precision (Точность для класса): Доля документов, верно отнесенных к классу X, среди всех документов, которые модель отнесла к классу X.\n",
    "#   `precision = TP / (TP + FP)`\n",
    "#   Важна, когда цена ложноположительного срабатывания высока.\n",
    "# - Recall (Полнота для класса): Доля документов класса X, которые модель правильно идентифицировала.\n",
    "#   `recall = TP / (TP + FN)`\n",
    "#   Важна, когда цена пропуска (ложноотрицательного срабатывания) высока.\n",
    "# - F1-Score (F1-мера): Гармоническое среднее Precision и Recall.\n",
    "#   `F1 = 2 * (precision * recall) / (precision + recall)`\n",
    "#   Хороший баланс между Precision и Recall, полезна при дисбалансе классов.\n",
    "# - Усреднение Метрик (для многоклассовой классификации):\n",
    "#   - Macro Average: Рассчитать метрику для каждого класса и усреднить (все классы равноправны).\n",
    "#   - Micro Average: Рассчитать метрику по всем документам глобально (учитывает общий вклад каждого документа). Эквивалентна Accuracy для однометочной классификации.\n",
    "#   - Weighted Average: Как Macro, но усреднение взвешено по количеству примеров в каждом классе.\n",
    "# - Confusion Matrix (Матрица Ошибок): Таблица, показывающая, какие классы модель путает друг с другом. Полезна для анализа ошибок.\n",
    "# - Hamming Loss (для Multi-label): Доля неправильно предсказанных меток (как отсутствующих, так и лишних).\n",
    "# - Jaccard Score (для Multi-label): Средний IoU между предсказанными и истинными наборами меток для каждого документа.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Пример Задачи и Решения (Традиционный ML с Scikit-learn)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Классифицировать новостные заголовки по 4 темам:\n",
    "# 'Business', 'Sci/Tech', 'Sports', 'World'.\n",
    "# Используем встроенный датасет 20 Newsgroups (упрощенный).\n",
    "\n",
    "# --- Решение (Концептуальный Код) ---\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline # Для объединения шагов\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 1. Загрузка Данных (только нужные категории)\n",
    "categories = ['rec.sport.hockey', 'sci.electronics', 'comp.graphics', 'soc.religion.christian'] # Пример категорий\n",
    "# Загрузим подмножество для примера\n",
    "# newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "# newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42, remove=('headers', 'footers', 'quotes'))\n",
    "#\n",
    "# X_train = newsgroups_train.data\n",
    "# y_train = newsgroups_train.target # Метки - это индексы категорий\n",
    "# X_test = newsgroups_test.data\n",
    "# y_test = newsgroups_test.target\n",
    "# target_names = newsgroups_train.target_names # Названия категорий\n",
    "# print(f\"Categories: {target_names}\")\n",
    "# print(f\"Number of training samples: {len(X_train)}\")\n",
    "# print(f\"Number of test samples: {len(X_test)}\")\n",
    "\n",
    "# 2. Создание Конвейера (Pipeline)\n",
    "#    Объединяет векторизацию и классификацию в один шаг.\n",
    "#    Pipeline для Naive Bayes\n",
    "# nb_pipeline = Pipeline([\n",
    "#     ('tfidf', TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)), # TF-IDF векторизация\n",
    "#     ('clf', MultinomialNB(alpha=0.1)), # Классификатор Naive Bayes\n",
    "# ])\n",
    "#\n",
    "# # Pipeline для SVM\n",
    "# svm_pipeline = Pipeline([\n",
    "#     ('tfidf', TfidfVectorizer(stop_words='english', max_df=0.95, min_df=2)),\n",
    "#     ('clf', LinearSVC(C=1.0, random_state=42)), # Классификатор SVM\n",
    "# ])\n",
    "\n",
    "# 3. Обучение Моделей\n",
    "# print(\"\\nTraining Naive Bayes model...\")\n",
    "# nb_pipeline.fit(X_train, y_train)\n",
    "# print(\"Naive Bayes training complete.\")\n",
    "#\n",
    "# print(\"\\nTraining SVM model...\")\n",
    "# svm_pipeline.fit(X_train, y_train)\n",
    "# print(\"SVM training complete.\")\n",
    "\n",
    "# 4. Оценка Моделей\n",
    "# print(\"\\n--- Naive Bayes Evaluation ---\")\n",
    "# y_pred_nb = nb_pipeline.predict(X_test)\n",
    "# print(f\"Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}\")\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_test, y_pred_nb, target_names=target_names))\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred_nb))\n",
    "#\n",
    "# print(\"\\n--- SVM Evaluation ---\")\n",
    "# y_pred_svm = svm_pipeline.predict(X_test)\n",
    "# print(f\"Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_test, y_pred_svm, target_names=target_names))\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred_svm))\n",
    "\n",
    "# 5. Предсказание на Новых Данных\n",
    "# new_docs = [\n",
    "#     \"The graphics card market is booming with new releases.\", # Sci/Tech or Comp.Graphics\n",
    "#     \"The hockey team won the championship last night.\", # Sports\n",
    "#     \"Discussion about the role of faith in modern society.\" # Religion\n",
    "# ]\n",
    "# pred_nb = nb_pipeline.predict(new_docs)\n",
    "# pred_svm = svm_pipeline.predict(new_docs)\n",
    "#\n",
    "# print(\"\\n--- Predictions for New Docs ---\")\n",
    "# for doc, nb_idx, svm_idx in zip(new_docs, pred_nb, pred_svm):\n",
    "#     print(f\"Doc: {doc[:50]}...\")\n",
    "#     print(f\"  Naive Bayes Prediction: {target_names[nb_idx]}\")\n",
    "#     print(f\"  SVM Prediction:         {target_names[svm_idx]}\")\n",
    "\n",
    "# --- Конец Примера (Scikit-learn) ---\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 7: Пример Задачи и Решения (Deep Learning с Hugging Face)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Та же - классифицировать новостные заголовки по темам.\n",
    "# Используем предобученную модель BERT и fine-tuning.\n",
    "\n",
    "# --- Решение (Концептуальный Код) ---\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "# from datasets import load_dataset # Для загрузки датасетов из Hugging Face Hub\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# 1. Загрузка Данных (Пример с Hugging Face Datasets)\n",
    "# # Многие стандартные датасеты доступны в хабе HF\n",
    "# # dataset = load_dataset(\"ag_news\") # AG News - 4 класса: World, Sports, Business, Sci/Tech\n",
    "# # dataset = dataset.map(lambda x: {\"labels\": x[\"label\"]}) # Переименовать колонку для Trainer\n",
    "# # num_labels = dataset['train'].features['labels'].num_classes\n",
    "# # id2label = {i: label for i, label in enumerate(dataset['train'].features['labels'].names)}\n",
    "# # label2id = {label: i for i, label in id2label.items()}\n",
    "# # print(f\"Labels: {id2label}\")\n",
    "\n",
    "# 2. Загрузка Токенизатора и Модели\n",
    "# model_name = \"distilbert-base-uncased\" # Более легкий вариант BERT\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, id2label=id2label, label2id=label2id)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# 3. Токенизация Данных\n",
    "# def tokenize_function(examples):\n",
    "#     return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "#\n",
    "# tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "# # Удаляем ненужные колонки, переименовываем 'label' в 'labels'\n",
    "# tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "# # tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\") # Уже сделали в map выше\n",
    "# tokenized_datasets.set_format(\"torch\") # Устанавливаем формат PyTorch\n",
    "#\n",
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000)) # Уменьшим для примера\n",
    "# small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "# 4. Определение Метрик для Оценки\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "#     acc = accuracy_score(labels, predictions)\n",
    "#     return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# 5. Настройка Аргументов Обучения и Trainer\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results_topic_clf\",\n",
    "#     num_train_epochs=1, # Для примера достаточно 1 эпохи\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "#     warmup_steps=100,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_dir='./logs_topic_clf',\n",
    "#     logging_steps=10,\n",
    "#     evaluation_strategy=\"epoch\", # Оценивать после каждой эпохи\n",
    "#     save_strategy=\"epoch\",       # Сохранять после каждой эпохи\n",
    "#     load_best_model_at_end=True, # Загрузить лучшую модель в конце\n",
    "#     report_to=\"none\", # Отключить логирование в wandb/tensorboard для примера\n",
    "# )\n",
    "#\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=small_train_dataset,\n",
    "#     eval_dataset=small_eval_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# 6. Запуск Fine-tuning\n",
    "# print(\"\\nStarting Transformer fine-tuning...\")\n",
    "# trainer.train()\n",
    "# print(\"Fine-tuning complete.\")\n",
    "\n",
    "# 7. Оценка\n",
    "# print(\"\\nEvaluating fine-tuned model...\")\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(f\"Evaluation results: {eval_results}\")\n",
    "\n",
    "# 8. Предсказание\n",
    "# print(\"\\n--- Predictions for New Docs (Transformer) ---\")\n",
    "# new_docs = [\n",
    "#     \"The graphics card market is booming with new releases.\",\n",
    "#     \"The hockey team won the championship last night.\",\n",
    "#     \"Global leaders met to discuss climate change.\",\n",
    "#     \"New smartphone announced with amazing camera features.\"\n",
    "# ]\n",
    "#\n",
    "# for doc in new_docs:\n",
    "#     inputs = tokenizer(doc, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         logits = model(**inputs).logits\n",
    "#     predicted_class_id = logits.argmax().item()\n",
    "#     predicted_label = model.config.id2label[predicted_class_id]\n",
    "#     print(f\"Doc: {doc[:50]}...\")\n",
    "#     print(f\"  Predicted Topic: {predicted_label}\")\n",
    "\n",
    "# --- Конец Примера (Hugging Face) ---\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 8: Выбор Подхода\n",
    "\n",
    "# - **Мало данных / Нужен быстрый baseline:** TF-IDF + Naive Bayes / SVM (Scikit-learn).\n",
    "# - **Среднее количество данных / Нужна хорошая точность:** TF-IDF + SVM или fine-tuning простой DL модели (CNN/RNN).\n",
    "# - **Много данных / Нужна максимальная точность / Есть ресурсы (GPU):** Fine-tuning предобученных Трансформеров (Hugging Face).\n",
    "# - **Нужно автоматически *обнаружить* темы (нет разметки):** Тематическое моделирование (LDA, NMF - Gensim, Scikit-learn).\n",
    "\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Подход \"Embedding + CNN/RNN\" для Задач NLP\n",
    "\n",
    "# Основная Идея:\n",
    "# Вместо использования сложных признаков (как TF-IDF) или больших предобученных\n",
    "# моделей (как BERT), мы можем построить свои модели глубокого обучения,\n",
    "# которые сначала преобразуют слова в плотные векторы (эмбеддинги), а затем\n",
    "# обрабатывают последовательность этих векторов с помощью Сверточных (CNN)\n",
    "# или Рекуррентных (RNN/LSTM/GRU) слоев для решения конкретной задачи NLP.\n",
    "\n",
    "# Компоненты:\n",
    "# 1. Слой Эмбеддингов (Embedding Layer):\n",
    "#    - Преобразует целочисленные индексы слов (из словаря) в плотные векторы\n",
    "#      фиксированной размерности (`embedding_dim`).\n",
    "#    - Эти векторы могут быть:\n",
    "#      - Обучаемыми с нуля вместе с остальной моделью.\n",
    "#      - Инициализированы предобученными векторами (Word2Vec, GloVe, FastText)\n",
    "#        и затем дообучены (fine-tuned) или оставлены замороженными.\n",
    "#    - В PyTorch: `torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)`\n",
    "# 2. Слой Обработки Последовательности:\n",
    "#    - **CNN (Сверточная Нейронная Сеть):** Использует 1D свертки для извлечения\n",
    "#      локальных признаков (похожих на n-граммы) из последовательности эмбеддингов.\n",
    "#      Часто используется с max-pooling для получения вектора фиксированной длины.\n",
    "#    - **RNN (Рекуррентная Нейронная Сеть - LSTM/GRU):** Обрабатывает\n",
    "#      последовательность эмбеддингов шаг за шагом, сохраняя информацию\n",
    "#      о предыдущих шагах в скрытом состоянии. Учитывает порядок слов.\n",
    "# 3. Выходной Слой (Output Layer):\n",
    "#    - Обычно один или несколько полносвязных слоев (`torch.nn.Linear`),\n",
    "#      которые принимают выход CNN/RNN и преобразуют его в предсказание\n",
    "#      для конкретной задачи (например, вероятности классов для классификации).\n",
    "\n",
    "# Преимущества:\n",
    "# - Автоматическое извлечение признаков (в отличие от TF-IDF).\n",
    "# - Учет семантики слов (через эмбеддинги).\n",
    "# - Учет порядка слов (в RNN) или локального контекста (в CNN).\n",
    "# - Меньше по размеру и требованиям, чем большие Трансформеры (если не использовать предобученные эмбеддинги большого размера).\n",
    "\n",
    "# Недостатки:\n",
    "# - Требуют больше данных для обучения с нуля, чем классические ML модели на TF-IDF.\n",
    "# - Простые эмбеддинги (обучаемые с нуля или статические) не учитывают контекст слова так, как Трансформеры.\n",
    "# - Обучение может быть дольше, чем у классических моделей.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Архитектура с CNN для NLP\n",
    "\n",
    "# Принцип Работы CNN для Текста:\n",
    "# - Текст представляется как матрица `(sequence_length, embedding_dim)`.\n",
    "# - 1D Свертки (`nn.Conv1d`) применяются вдоль оси последовательности.\n",
    "# - Фильтры (ядра) имеют размер `(kernel_size, embedding_dim)`, где `kernel_size`\n",
    "#   определяет, сколько слов (n-грамма) фильтр \"видит\" за раз.\n",
    "# - Разные фильтры учатся распознавать разные паттерны n-грамм.\n",
    "# - Часто используют несколько сверточных слоев с разными размерами ядер\n",
    "#   (например, 3, 4, 5) для захвата n-грамм разной длины.\n",
    "# - После сверток обычно идет функция активации (ReLU).\n",
    "# - Затем применяется Max-Pooling (часто Max-over-time pooling), который берет\n",
    "#   максимальное значение по всей длине последовательности для каждого фильтра.\n",
    "#   Это создает вектор фиксированной длины независимо от длины входного текста.\n",
    "# - Этот вектор подается на полносвязные слои для финального предсказания.\n",
    "\n",
    "# Концептуальная Модель PyTorch (Классификация Текста):\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_filters, filter_sizes, num_classes, dropout_prob=0.5):\n",
    "        super(CNNTextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0) # padding_idx=0 если 0 используется для паддинга\n",
    "\n",
    "        # Список сверточных слоев для разных размеров n-грамм\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=embed_dim,\n",
    "                      out_channels=num_filters,\n",
    "                      kernel_size=fs)\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "\n",
    "        # Полносвязный слой\n",
    "        # Входной размер = количество фильтров * количество разных размеров ядер\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, text_indices):\n",
    "        # text_indices: [batch_size, seq_len]\n",
    "\n",
    "        embedded = self.embedding(text_indices)\n",
    "        # embedded: [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        # CNN ожидает вход [batch_size, channels, seq_len]\n",
    "        # В нашем случае channels = embed_dim\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        # embedded: [batch_size, embed_dim, seq_len]\n",
    "\n",
    "        # Применяем свертки и max-pooling\n",
    "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
    "        # conved[i]: [batch_size, num_filters, seq_len - filter_sizes[i] + 1]\n",
    "\n",
    "        # Max-over-time pooling\n",
    "        # Применяем max pool по всей длине последовательности\n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        # pooled[i]: [batch_size, num_filters]\n",
    "\n",
    "        # Конкатенируем выходы пулинга от фильтров разных размеров\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        # cat: [batch_size, num_filters * len(filter_sizes)]\n",
    "\n",
    "        # Полносвязный слой для классификации\n",
    "        logits = self.fc(cat)\n",
    "        # logits: [batch_size, num_classes]\n",
    "        return logits\n",
    "\n",
    "# Параметры:\n",
    "# vocab_size: Размер словаря.\n",
    "# embed_dim: Размерность эмбеддингов (e.g., 100, 300).\n",
    "# num_filters: Количество фильтров для каждого размера ядра (e.g., 100, 128).\n",
    "# filter_sizes: Список размеров ядер (n-грамм) (e.g., [3, 4, 5]).\n",
    "# num_classes: Количество выходных классов.\n",
    "# dropout_prob: Вероятность Dropout для регуляризации.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Архитектура с RNN (LSTM/GRU) для NLP\n",
    "\n",
    "# Принцип Работы RNN для Текста:\n",
    "# - Текст представляется как последовательность эмбеддингов слов `(seq_len, batch_size, embedding_dim)`\n",
    "#   (или `(batch_size, seq_len, embedding_dim)` если `batch_first=True`).\n",
    "# - RNN (обычно LSTM или GRU для лучшей работы с длинными зависимостями)\n",
    "#   обрабатывает эту последовательность шаг за шагом.\n",
    "# - На каждом шаге `t` RNN принимает эмбеддинг слова `x_t` и предыдущее\n",
    "#   скрытое состояние `h_{t-1}` (и ячейку памяти `c_{t-1}` для LSTM)\n",
    "#   и вычисляет новый выход `output_t` и новое скрытое состояние `h_t` (и `c_t`).\n",
    "# - Скрытое состояние `h_t` несет в себе информацию о всей предыдущей\n",
    "#   последовательности до шага `t`.\n",
    "# - Для задач классификации всего текста часто используется:\n",
    "#   - Выход RNN на последнем временном шаге (`output[-1]`).\n",
    "#   - Последнее скрытое состояние (`h_n`).\n",
    "# - Этот итоговый вектор подается на полносвязные слои для предсказания.\n",
    "# - Можно использовать двунаправленные RNN (BiLSTM/BiGRU), которые обрабатывают\n",
    "#   последовательность в двух направлениях (вперед и назад), что позволяет\n",
    "#   учитывать и правый, и левый контекст. Выходы конкатенируются.\n",
    "\n",
    "# Концептуальная Модель PyTorch (Классификация Текста):\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, num_classes,\n",
    "                 rnn_type='LSTM', bidirectional=True, dropout_prob=0.5):\n",
    "        super(RNNTextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "\n",
    "        rnn_module = None\n",
    "        if rnn_type == 'LSTM':\n",
    "            rnn_module = nn.LSTM\n",
    "        elif rnn_type == 'GRU':\n",
    "            rnn_module = nn.GRU\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported RNN type. Choose 'LSTM' or 'GRU'.\")\n",
    "\n",
    "        self.rnn = rnn_module(embed_dim,\n",
    "                              hidden_dim,\n",
    "                              num_layers=num_layers,\n",
    "                              bidirectional=bidirectional,\n",
    "                              batch_first=True, # Вход: [batch, seq, feature]\n",
    "                              dropout=dropout_prob if num_layers > 1 else 0) # Dropout между слоями RNN\n",
    "\n",
    "        # Рассчитываем размер входа для FC слоя\n",
    "        fc_in_features = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(fc_in_features, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, text_indices):\n",
    "        # text_indices: [batch_size, seq_len]\n",
    "\n",
    "        embedded = self.embedding(text_indices)\n",
    "        # embedded: [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        # Пропускаем через RNN\n",
    "        # outputs: [batch_size, seq_len, hidden_dim * num_directions] - выходы каждого шага\n",
    "        # hidden: [num_layers * num_directions, batch_size, hidden_dim] - финальное скрытое состояние\n",
    "        # cell (только для LSTM): [num_layers * num_directions, batch_size, hidden_dim] - финальное состояние ячейки\n",
    "        outputs, (hidden, cell) = self.rnn(embedded) # Для GRU будет только outputs, hidden\n",
    "\n",
    "        # Используем финальное скрытое состояние последнего слоя\n",
    "        # Если bidirectional=True, hidden будет содержать состояния прямого и обратного прохода.\n",
    "        # hidden[-1] - скрытое состояние последнего слоя прямого прохода\n",
    "        # hidden[-2] - скрытое состояние последнего слоя обратного прохода (если bidirectional)\n",
    "        if self.rnn.bidirectional:\n",
    "            # Конкатенируем финальные скрытые состояния прямого и обратного проходов\n",
    "            hidden_fwd = hidden[-2,:,:] # Последний слой, прямое направление\n",
    "            hidden_bwd = hidden[-1,:,:] # Последний слой, обратное направление\n",
    "            hidden_cat = torch.cat((hidden_fwd, hidden_bwd), dim=1)\n",
    "        else:\n",
    "            hidden_cat = hidden[-1,:,:] # Последний слой, одно направление\n",
    "\n",
    "        # hidden_cat: [batch_size, hidden_dim * num_directions]\n",
    "\n",
    "        # Применяем Dropout и полносвязный слой\n",
    "        dropped_hidden = self.dropout(hidden_cat)\n",
    "        logits = self.fc(dropped_hidden)\n",
    "        # logits: [batch_size, num_classes]\n",
    "        return logits\n",
    "\n",
    "# Параметры:\n",
    "# vocab_size: Размер словаря.\n",
    "# embed_dim: Размерность эмбеддингов (e.g., 100, 300).\n",
    "# hidden_dim: Размерность скрытого состояния RNN (e.g., 128, 256).\n",
    "# num_layers: Количество слоев RNN (e.g., 1, 2).\n",
    "# num_classes: Количество выходных классов.\n",
    "# rnn_type: 'LSTM' или 'GRU'.\n",
    "# bidirectional: Использовать ли двунаправленный RNN (True/False).\n",
    "# dropout_prob: Вероятность Dropout.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Подготовка Данных для Моделей\n",
    "\n",
    "# Общие Шаги:\n",
    "# 1. Загрузка Данных: Тексты и соответствующие метки.\n",
    "# 2. Предобработка Текста:\n",
    "#    - Токенизация (например, `spaCy` или `nltk.word_tokenize`).\n",
    "#    - Очистка (нижний регистр, удаление спецсимволов - опционально).\n",
    "#    - *Лемматизация/стемминг - часто НЕ используются с эмбеддингами, т.к. модель может выучить разные формы слов.*\n",
    "#    - *Удаление стоп-слов - также часто НЕ делается, т.к. они могут нести контекстную информацию для RNN/CNN.*\n",
    "# 3. Построение Словаря (Vocabulary):\n",
    "#    - Создание отображения каждого уникального токена в корпусе на целочисленный индекс.\n",
    "#    - Добавление специальных токенов:\n",
    "#      - `<PAD>` (Padding): Для выравнивания длины последовательностей в батче (обычно индекс 0).\n",
    "#      - `<UNK>` (Unknown): Для слов, не встречавшихся в обучающем словаре (обычно индекс 1).\n",
    "# 4. Численное Представление: Конвертация токенизированных текстов в последовательности индексов из словаря.\n",
    "# 5. Паддинг (Padding): Дополнение коротких последовательностей индексом `<PAD>` до фиксированной максимальной длины (`max_seq_len`). Это необходимо для батчинга.\n",
    "# 6. Создание `Dataset` и `DataLoader` (PyTorch):\n",
    "#    - `Dataset` для загрузки одного примера (последовательность индексов, метка).\n",
    "#    - `DataLoader` для формирования батчей, перемешивания данных.\n",
    "\n",
    "# Пример использования `torchtext` (может потребовать адаптации под последние версии):\n",
    "# import torchtext\n",
    "# from torchtext.data import Field, LabelField, BucketIterator\n",
    "#\n",
    "# # 1. Определение полей (Fields)\n",
    "# TEXT = Field(tokenize='spacy', # Использовать токенизатор spaCy\n",
    "#              tokenizer_language='en_core_web_sm',\n",
    "#              batch_first=True, # Важно для CNN/RNN с batch_first=True\n",
    "#              # include_lengths=True # Полезно для RNN с PackedSequence\n",
    "#              lower=True)\n",
    "# LABEL = LabelField(dtype=torch.float) # Или long для CrossEntropyLoss\n",
    "#\n",
    "# # 2. Загрузка данных (пример с CSV или JSON)\n",
    "# # train_data, test_data = TabularDataset.splits(...)\n",
    "#\n",
    "# # 3. Построение словаря\n",
    "# TEXT.build_vocab(train_data, max_size=10000, vectors=\"glove.6B.100d\", unk_init=torch.Tensor.normal_) # Можно загрузить предобученные векторы\n",
    "# LABEL.build_vocab(train_data)\n",
    "# vocab_size = len(TEXT.vocab)\n",
    "# num_classes = len(LABEL.vocab)\n",
    "#\n",
    "# # 4. Создание итераторов (DataLoader)\n",
    "# BATCH_SIZE = 64\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# train_iterator, test_iterator = BucketIterator.splits(\n",
    "#     (train_data, test_data),\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     sort_key=lambda x: len(x.text), # Группировка по длине для эффективности RNN\n",
    "#     sort_within_batch=True,\n",
    "#     device=device)\n",
    "#\n",
    "# # Использование итератора в цикле обучения:\n",
    "# # for batch in train_iterator:\n",
    "# #     text = batch.text # Тензор индексов [batch_size, seq_len]\n",
    "# #     labels = batch.label # Тензор меток [batch_size]\n",
    "# #     # ... обучение ...\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Обучение и Инференс\n",
    "\n",
    "# Обучающий Цикл (Стандартный PyTorch):\n",
    "# 1. Инициализация модели, функции потерь (`nn.CrossEntropyLoss` для мультиклассовой, `nn.BCEWithLogitsLoss` для бинарной/мульти-лейбл), оптимизатора (`optim.Adam`).\n",
    "# 2. Цикл по эпохам.\n",
    "# 3. Внутри эпохи:\n",
    "#    - `model.train()`\n",
    "#    - Цикл по батчам из `DataLoader`.\n",
    "#    - Перемещение данных на `device`.\n",
    "#    - `optimizer.zero_grad()`\n",
    "#    - Прямой проход: `outputs = model(batch_text)`\n",
    "#    - Вычисление потерь: `loss = criterion(outputs, batch_labels)`\n",
    "#    - Обратный проход: `loss.backward()`\n",
    "#    - Обрезка градиента (опционально, полезно для RNN): `torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)`\n",
    "#    - Шаг оптимизатора: `optimizer.step()`\n",
    "#    - Подсчет метрик (accuracy, loss) на батче/эпохе.\n",
    "# 4. Валидация после каждой эпохи:\n",
    "#    - `model.eval()`\n",
    "#    - `with torch.no_grad():`\n",
    "#    - Цикл по валидационным батчам.\n",
    "#    - Вычисление метрик на валидационном наборе.\n",
    "#    - Сохранение лучшей модели.\n",
    "\n",
    "# Инференс:\n",
    "# 1. Загрузка обученной модели и словаря (`vocab`).\n",
    "# 2. `model.eval()`\n",
    "# 3. Предобработка входного текста: токенизация, конвертация в индексы словаря, паддинг, создание тензора.\n",
    "# 4. `with torch.no_grad(): output = model(input_tensor)`\n",
    "# 5. Постобработка выхода: применение `softmax` или `sigmoid` для получения вероятностей, `argmax` для получения предсказанного класса.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Выбор между CNN и RNN для Задачи\n",
    "\n",
    "# - **CNN:**\n",
    "#   - **Плюсы:** Быстрее обучаются и работают (параллельные вычисления), хорошо улавливают локальные признаки (важные ключевые слова/фразы), менее чувствительны к проблемам исчезающего градиента.\n",
    "#   - **Минусы:** Менее эффективно улавливают длинные зависимости и строгий порядок слов по сравнению с RNN.\n",
    "#   - **Подходит для:** Классификации текста (особенно анализ тональности, спам), где наличие определенных фраз важнее сложной грамматической структуры.\n",
    "\n",
    "# - **RNN (LSTM/GRU):**\n",
    "#   - **Плюсы:** Естественно моделируют последовательности, хорошо улавливают порядок слов и длинные зависимости, могут использоваться для генерации текста.\n",
    "#   - **Минусы:** Медленнее из-за последовательной обработки, могут быть сложнее в обучении (градиенты).\n",
    "#   - **Подходит для:** Классификации текста, где важен порядок и контекст (например, определение сарказма), машинного перевода, языкового моделирования, NER, POS-tagging.\n",
    "\n",
    "# - **Комбинации:** Иногда используют гибридные модели, сочетающие CNN и RNN слои.\n",
    "\n",
    "# Выбор часто зависит от конкретной задачи, размера данных и доступных ресурсов.\n",
    "# Для многих задач классификации текста и CNN, и LSTM/GRU могут дать хорошие результаты.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 7: Пример Задачи - Классификация Тональности (Концептуально)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Классифицировать отзывы на фильмы как \"позитивные\" или \"негативные\".\n",
    "\n",
    "# --- Решение (Концептуальный Код на PyTorch) ---\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# # Предполагается, что есть функции/классы для:\n",
    "# # - load_data(): Загружает тексты и метки (0/1)\n",
    "# # - build_vocab(texts): Строит словарь {word: index}\n",
    "# # - text_to_indices(text, vocab): Конвертирует текст в индексы\n",
    "# # - pad_sequence(indices, max_len): Дополняет последовательность\n",
    "#\n",
    "# # --- 1. Параметры ---\n",
    "# VOCAB_SIZE = 10000 # Примерный размер словаря\n",
    "# EMBED_DIM = 100\n",
    "# HIDDEN_DIM_RNN = 128 # Для RNN\n",
    "# NUM_FILTERS_CNN = 100 # Для CNN\n",
    "# FILTER_SIZES_CNN = [3, 4, 5] # Для CNN\n",
    "# NUM_CLASSES = 2 # Positive / Negative\n",
    "# NUM_EPOCHS = 5\n",
    "# BATCH_SIZE = 64\n",
    "# MAX_SEQ_LEN = 150 # Максимальная длина последовательности после паддинга\n",
    "#\n",
    "# # --- 2. Данные ---\n",
    "# # train_texts, train_labels = load_data('train')\n",
    "# # test_texts, test_labels = load_data('test')\n",
    "# # vocab = build_vocab(train_texts, max_size=VOCAB_SIZE)\n",
    "# # VOCAB_SIZE = len(vocab) # Обновить размер словаря\n",
    "#\n",
    "# # class SentimentDataset(Dataset):\n",
    "# #     def __init__(self, texts, labels, vocab, max_len):\n",
    "# #         self.texts = texts\n",
    "# #         self.labels = labels\n",
    "# #         self.vocab = vocab\n",
    "# #         self.max_len = max_len\n",
    "# #\n",
    "# #     def __len__(self):\n",
    "# #         return len(self.texts)\n",
    "# #\n",
    "# #     def __getitem__(self, idx):\n",
    "# #         text = self.texts[idx]\n",
    "# #         label = self.labels[idx]\n",
    "# #         indices = text_to_indices(text, self.vocab)\n",
    "# #         padded_indices = pad_sequence(indices, self.max_len)\n",
    "# #         return torch.tensor(padded_indices, dtype=torch.long), torch.tensor(label, dtype=torch.long) # long для CrossEntropy\n",
    "#\n",
    "# # train_dataset = SentimentDataset(train_texts, train_labels, vocab, MAX_SEQ_LEN)\n",
    "# # test_dataset = SentimentDataset(test_texts, test_labels, vocab, MAX_SEQ_LEN)\n",
    "# # train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# # test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "#\n",
    "# # --- 3. Модель (Выбираем одну: CNN или RNN) ---\n",
    "# # model = CNNTextClassifier(VOCAB_SIZE, EMBED_DIM, NUM_FILTERS_CNN, FILTER_SIZES_CNN, NUM_CLASSES)\n",
    "# model = RNNTextClassifier(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM_RNN, num_layers=2, num_classes=NUM_CLASSES, rnn_type='LSTM', bidirectional=True)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model.to(device)\n",
    "#\n",
    "# # --- 4. Обучение ---\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "#\n",
    "# # (Здесь стандартный цикл обучения PyTorch, как описан в Блоке 5)\n",
    "# # for epoch in range(NUM_EPOCHS):\n",
    "# #     model.train()\n",
    "# #     for sequences, labels in train_loader:\n",
    "# #         sequences, labels = sequences.to(device), labels.to(device)\n",
    "# #         optimizer.zero_grad()\n",
    "# #         outputs = model(sequences)\n",
    "# #         loss = criterion(outputs, labels)\n",
    "# #         loss.backward()\n",
    "# #         optimizer.step()\n",
    "# #     # Валидация на test_loader...\n",
    "#\n",
    "# # --- 5. Инференс ---\n",
    "# # def predict_sentiment(text, model, vocab, max_len, device):\n",
    "# #     model.eval()\n",
    "# #     indices = text_to_indices(text, vocab)\n",
    "# #     padded = pad_sequence(indices, max_len)\n",
    "# #     tensor = torch.tensor(padded, dtype=torch.long).unsqueeze(0).to(device) # Добавить batch dim\n",
    "# #     with torch.no_grad():\n",
    "# #         output = model(tensor)\n",
    "# #     probability = torch.softmax(output, dim=1)\n",
    "# #     prediction = torch.argmax(probability, dim=1).item()\n",
    "# #     return prediction, probability[0][prediction].item()\n",
    "#\n",
    "# # example_text = \"This movie was great!\"\n",
    "# # pred_class, confidence = predict_sentiment(example_text, model, vocab, MAX_SEQ_LEN, device)\n",
    "# # print(f\"Text: '{example_text}' -> Predicted Class: {pred_class}, Confidence: {confidence:.4f}\")\n",
    "\n",
    "# --- Конец Примера ---\n",
    "\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Задача - Классификация Тональности (Sentiment Analysis)\n",
    "\n",
    "# Цель: Классифицировать текст (например, отзыв) как \"позитивный\" (1) или \"негативный\" (0).\n",
    "# Подход: Используем модель глубокого обучения на PyTorch, состоящую из:\n",
    "#   1. Слой Эмбеддингов (Embedding) для преобразования слов в векторы.\n",
    "#   2. Слой Рекуррентной Нейронной Сети (BiLSTM) для обработки последовательности векторов.\n",
    "#   3. Полносвязный слой (Linear) для финальной классификации.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Импорты и Настройки\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter # Для построения словаря\n",
    "from tqdm import tqdm # Для индикатора прогресса\n",
    "import numpy as np\n",
    "import re # Для простой очистки текста\n",
    "\n",
    "# Настройки\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Параметры модели и обучения\n",
    "VOCAB_SIZE = 0 # Определится после построения словаря\n",
    "EMBED_DIM = 100 # Размерность эмбеддингов\n",
    "HIDDEN_DIM = 128 # Размерность скрытого состояния LSTM\n",
    "NUM_CLASSES = 2 # Positive (1), Negative (0)\n",
    "NUM_LAYERS = 2 # Количество слоев LSTM\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT_PROB = 0.5\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 4 # Маленький батч для примера\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_SEQ_LEN = 50 # Максимальная длина последовательности (для паддинга)\n",
    "\n",
    "# Специальные токены\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Подготовка Данных\n",
    "\n",
    "# --- 3.1 Пример Данных ---\n",
    "# В реальной задаче данные загружаются из файлов (CSV, JSON, etc.)\n",
    "raw_train_data = [\n",
    "    (\"This movie is fantastic and amazing!\", 1), # Positive\n",
    "    (\"I absolutely loved the plot.\", 1),\n",
    "    (\"What a brilliant performance by the actors.\", 1),\n",
    "    (\"Highly recommended, a must-see!\", 1),\n",
    "    (\"The best film I have seen this year.\", 1),\n",
    "    (\"A truly wonderful experience.\", 1),\n",
    "    (\"It was a complete waste of time.\", 0), # Negative\n",
    "    (\"The acting was terrible and the story boring.\", 0),\n",
    "    (\"I hated every minute of it.\", 0),\n",
    "    (\"Worst movie ever, do not watch.\", 0),\n",
    "    (\"So disappointing, I expected much more.\", 0),\n",
    "    (\"The plot made no sense at all.\", 0),\n",
    "]\n",
    "\n",
    "raw_test_data = [\n",
    "    (\"Incredible film, very engaging.\", 1),\n",
    "    (\"A masterpiece of cinema.\", 1),\n",
    "    (\"Just awful, avoid at all costs.\", 0),\n",
    "    (\"The direction was poor and the script weak.\", 0),\n",
    "]\n",
    "\n",
    "# --- 3.2 Предобработка и Токенизация ---\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # Удаляем простую пунктуацию (можно использовать более сложные методы)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.split() # Простая токенизация по пробелам\n",
    "    return tokens\n",
    "\n",
    "# Применяем предобработку\n",
    "train_texts = [preprocess_text(text) for text, label in raw_train_data]\n",
    "test_texts = [preprocess_text(text) for text, label in raw_test_data]\n",
    "train_labels = [label for text, label in raw_train_data]\n",
    "test_labels = [label for text, label in raw_test_data]\n",
    "\n",
    "# --- 3.3 Построение Словаря ---\n",
    "word_counts = Counter(token for text in train_texts for token in text)\n",
    "# Создаем словарь: слово -> индекс\n",
    "# Добавляем специальные токены PAD и UNK\n",
    "vocab = {word: i+2 for i, (word, count) in enumerate(word_counts.items())} # Начинаем с индекса 2\n",
    "vocab[PAD_TOKEN] = 0\n",
    "vocab[UNK_TOKEN] = 1\n",
    "VOCAB_SIZE = len(vocab) # Обновляем размер словаря\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "# print(f\"Vocabulary sample: {list(vocab.items())[:10]}\")\n",
    "\n",
    "# --- 3.4 Конвертация в Индексы и Паддинг ---\n",
    "def text_to_indices(text_tokens, vocab):\n",
    "    return [vocab.get(token, vocab[UNK_TOKEN]) for token in text_tokens]\n",
    "\n",
    "def pad_sequence(indices, max_len, pad_idx):\n",
    "    current_len = len(indices)\n",
    "    if current_len >= max_len:\n",
    "        return indices[:max_len] # Обрезаем, если длиннее\n",
    "    else:\n",
    "        # Дополняем нулями (индекс PAD_TOKEN) в конец\n",
    "        return indices + [pad_idx] * (max_len - current_len)\n",
    "\n",
    "train_indices = [pad_sequence(text_to_indices(text, vocab), MAX_SEQ_LEN, vocab[PAD_TOKEN]) for text in train_texts]\n",
    "test_indices = [pad_sequence(text_to_indices(text, vocab), MAX_SEQ_LEN, vocab[PAD_TOKEN]) for text in test_texts]\n",
    "\n",
    "# --- 3.5 Создание PyTorch Dataset ---\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, indices, labels):\n",
    "        self.indices = indices\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Возвращаем тензоры\n",
    "        sequence = torch.tensor(self.indices[idx], dtype=torch.long)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long) # long для CrossEntropyLoss\n",
    "        return sequence, label\n",
    "\n",
    "train_dataset = SentimentDataset(train_indices, train_labels)\n",
    "test_dataset = SentimentDataset(test_indices, test_labels)\n",
    "\n",
    "# --- 3.6 Создание DataLoader ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE) # shuffle=False для теста\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Определение Модели (BiLSTM)\n",
    "\n",
    "class RNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes,\n",
    "                 num_layers, bidirectional, dropout_prob, pad_idx):\n",
    "        super(RNNTextClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.rnn = nn.LSTM(embed_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=num_layers,\n",
    "                           bidirectional=bidirectional,\n",
    "                           batch_first=True, # Ожидаем вход [batch, seq, feature]\n",
    "                           dropout=dropout_prob if num_layers > 1 else 0)\n",
    "\n",
    "        fc_in_features = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(fc_in_features, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, text_indices):\n",
    "        # text_indices: [batch_size, seq_len]\n",
    "        embedded = self.dropout(self.embedding(text_indices))\n",
    "        # embedded: [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        # PackedSequence не используется для простоты, но может ускорить RNN\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # hidden: [num_layers * num_directions, batch_size, hidden_dim]\n",
    "\n",
    "        if self.rnn.bidirectional:\n",
    "            # Конкатенируем финальные скрытые состояния последнего слоя (прямое и обратное)\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            # Берем финальное скрытое состояние последнего слоя\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "\n",
    "        # hidden: [batch_size, hidden_dim * num_directions]\n",
    "        logits = self.fc(hidden)\n",
    "        # logits: [batch_size, num_classes]\n",
    "        return logits\n",
    "\n",
    "# Инициализация модели\n",
    "model = RNNTextClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    dropout_prob=DROPOUT_PROB,\n",
    "    pad_idx=vocab[PAD_TOKEN]\n",
    ")\n",
    "model.to(DEVICE)\n",
    "print(\"\\nModel Initialized:\")\n",
    "print(model)\n",
    "\n",
    "# Подсчет параметров (для информации)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Обучение Модели\n",
    "\n",
    "# Функция потерь и оптимизатор\n",
    "criterion = nn.CrossEntropyLoss() # Подходит для мультиклассовой классификации (здесь 2 класса)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train() # Установить режим обучения\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "\n",
    "    # Используем tqdm для прогресс-бара\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Training]\")\n",
    "    for sequences, labels in pbar:\n",
    "        sequences = sequences.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(sequences) # Прямой проход\n",
    "        loss = criterion(outputs, labels) # Вычисление потерь\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1) # Получаем предсказания\n",
    "        correct = (preds == labels).sum().item()\n",
    "        total = labels.size(0)\n",
    "\n",
    "        loss.backward() # Обратный проход\n",
    "        optimizer.step() # Обновление весов\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += correct\n",
    "        epoch_total += total\n",
    "\n",
    "        # Обновляем описание прогресс-бара\n",
    "        pbar.set_postfix({'Loss': loss.item(), 'Acc': correct/total})\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    avg_epoch_acc = epoch_correct / epoch_total\n",
    "    print(f\"Epoch {epoch+1} Summary: Train Loss: {avg_epoch_loss:.4f}, Train Acc: {avg_epoch_acc:.4f}\")\n",
    "\n",
    "training_end_time = time.time()\n",
    "print(f\"Training finished in {training_end_time - training_start_time:.2f} seconds.\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Оценка Модели\n",
    "\n",
    "print(\"\\nStarting Evaluation...\")\n",
    "model.eval() # Установить режим оценки\n",
    "test_loss = 0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad(): # Отключаем вычисление градиентов\n",
    "    pbar_test = tqdm(test_loader, desc=\"[Evaluating]\")\n",
    "    for sequences, labels in pbar_test:\n",
    "        sequences = sequences.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct = (preds == labels).sum().item()\n",
    "        total = labels.size(0)\n",
    "\n",
    "        test_loss += loss.item()\n",
    "        test_correct += correct\n",
    "        test_total += total\n",
    "        pbar_test.set_postfix({'Acc': correct/total})\n",
    "\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "avg_test_acc = test_correct / test_total\n",
    "print(f\"\\nEvaluation Results: Test Loss: {avg_test_loss:.4f}, Test Acc: {avg_test_acc:.4f}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 7: Инференс (Предсказание на Новом Тексте)\n",
    "\n",
    "def predict_sentiment(text, model, vocab, max_len, pad_idx, device):\n",
    "    model.eval() # Убедиться, что модель в режиме оценки\n",
    "    # Предобработка\n",
    "    tokens = preprocess_text(text)\n",
    "    indices = text_to_indices(tokens, vocab)\n",
    "    padded_indices = pad_sequence(indices, max_len, pad_idx)\n",
    "    # Конвертация в тензор и добавление batch dimension\n",
    "    tensor = torch.tensor(padded_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor) # Получаем логиты [1, num_classes]\n",
    "\n",
    "    # Получаем вероятности\n",
    "    probabilities = torch.softmax(output, dim=1).squeeze(0) # Убираем batch dim\n",
    "    # Получаем предсказанный класс\n",
    "    prediction = torch.argmax(probabilities).item() # 0 или 1\n",
    "    confidence = probabilities[prediction].item() # Уверенность в предсказанном классе\n",
    "\n",
    "    label = \"Positive\" if prediction == 1 else \"Negative\"\n",
    "    return label, confidence\n",
    "\n",
    "# Примеры предсказаний\n",
    "print(\"\\n--- Inference Examples ---\")\n",
    "test_sentence_1 = \"This was an absolutely brilliant and engaging movie!\"\n",
    "pred1, conf1 = predict_sentiment(test_sentence_1, model, vocab, MAX_SEQ_LEN, vocab[PAD_TOKEN], DEVICE)\n",
    "print(f\"Text: '{test_sentence_1}'\")\n",
    "print(f\"Predicted: {pred1} (Confidence: {conf1:.4f})\")\n",
    "\n",
    "test_sentence_2 = \"The plot was predictable and the acting felt very wooden.\"\n",
    "pred2, conf2 = predict_sentiment(test_sentence_2, model, vocab, MAX_SEQ_LEN, vocab[PAD_TOKEN], DEVICE)\n",
    "print(f\"Text: '{test_sentence_2}'\")\n",
    "print(f\"Predicted: {pred2} (Confidence: {conf2:.4f})\")\n",
    "\n",
    "test_sentence_3 = \"It was okay, not great but not terrible either.\" # Нейтральный - посмотрим, куда отнесет\n",
    "pred3, conf3 = predict_sentiment(test_sentence_3, model, vocab, MAX_SEQ_LEN, vocab[PAD_TOKEN], DEVICE)\n",
    "print(f\"Text: '{test_sentence_3}'\")\n",
    "print(f\"Predicted: {pred3} (Confidence: {conf3:.4f})\")\n",
    "\n",
    "# --- Конец Примера ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Эмбеддинги в spaCy - Обзор\n",
    "\n",
    "# Как spaCy Работает с Эмбеддингами:\n",
    "# spaCy интегрирует векторные представления (эмбеддинги) непосредственно в свои\n",
    "# языковые модели и конвейеры обработки (pipelines). Подход к эмбеддингам\n",
    "# зависит от типа загруженной модели spaCy (`sm`, `md`, `lg`, `trf`).\n",
    "\n",
    "# Основные Способы Получения и Использования Эмбеддингов в spaCy:\n",
    "# 1. Статические Векторы Слов (Word Vectors):\n",
    "#    - Присутствуют в моделях среднего (`md`) и большого (`lg`) размера (например, `en_core_web_md`, `en_core_web_lg`).\n",
    "#    - Каждому слову из словаря модели сопоставлен один фиксированный вектор (похоже на Word2Vec/GloVe, но часто обучены иначе).\n",
    "#    - Доступны через атрибут `.vector` у объектов `Token`, `Span`, `Doc`.\n",
    "#    - Векторы для `Span` и `Doc` обычно вычисляются как усреднение векторов входящих в них токенов.\n",
    "# 2. Контекстно-чувствительные Тензоры (Context-sensitive Tensors - Tok2Vec):\n",
    "#    - Генерируются компонентом конвейера `tok2vec` (Token-to-Vector), который присутствует\n",
    "#      во всех современных предобученных моделях spaCy (`sm`, `md`, `lg`, `trf` - в `trf` он заменяется трансформером).\n",
    "#    - `tok2vec` использует неглубокую нейронную сеть (часто CNN) для создания векторов токенов,\n",
    "#      учитывающих локальный контекст.\n",
    "#    - Эти тензоры используются *внутренне* как признаки для последующих компонентов\n",
    "#      конвейера (tagger, parser, ner и т.д.).\n",
    "#    - Доступны через атрибут `doc.tensor`.\n",
    "# 3. Контекстуализированные Эмбеддинги Трансформеров (Transformer Embeddings):\n",
    "#    - Используются в моделях `_trf` (например, `en_core_web_trf`) при наличии\n",
    "#      установленного пакета `spacy-transformers`.\n",
    "#    - Генерируются компонентом `transformer` на основе моделей типа BERT, RoBERTa и т.д.\n",
    "#    - Полностью контекстуальные: вектор токена зависит от всего предложения.\n",
    "#    - Атрибуты `.vector` у `Token`, `Span`, `Doc` в `_trf` моделях будут основаны\n",
    "#      на этих контекстуальных представлениях (например, усреднение выходов трансформера\n",
    "#      для токенов, или вектор [CLS] для документа).\n",
    "#    - Детальные выходы трансформера доступны через `doc._.trf_data` (требует `spacy-transformers`).\n",
    "\n",
    "# Ключевое Различие:\n",
    "# - Статические векторы (`md`/`lg`): Один вектор на слово -> Быстро, но не учитывает контекст.\n",
    "# - Tok2Vec (`sm`/`md`/`lg`): Внутренние векторы, учитывают локальный контекст -> Используются для компонентов spaCy.\n",
    "# - Трансформеры (`trf`): Контекстуальные векторы, учитывают глобальный контекст -> Наиболее мощные, но медленные.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Статические Векторы Слов (Модели `md`/`lg`)\n",
    "\n",
    "# Требуется модель `md` или `lg`. Установите:\n",
    "# python -m spacy download en_core_web_md\n",
    "# или\n",
    "# python -m spacy download en_core_web_lg\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "model_name_md = \"en_core_web_md\"\n",
    "try:\n",
    "    nlp_md = spacy.load(model_name_md)\n",
    "    print(f\"\\nLoaded model with static vectors: '{model_name_md}'\")\n",
    "\n",
    "    # Проверка наличия векторов в словаре модели\n",
    "    # print(f\"Vocab vectors shape: {nlp_md.vocab.vectors.shape}\") # (num_vectors, vector_dim)\n",
    "\n",
    "    text = \"Apple and orange are fruits.\"\n",
    "    doc_md = nlp_md(text)\n",
    "\n",
    "    # Доступ к векторам токенов\n",
    "    apple_token = doc_md[0]\n",
    "    orange_token = doc_md[2]\n",
    "    fruits_token = doc_md[5]\n",
    "\n",
    "    # print(f\"\\nToken: '{apple_token.text}'\")\n",
    "    # print(f\"  Has Vector: {apple_token.has_vector}\")\n",
    "    # print(f\"  Vector Norm (L2): {apple_token.vector_norm}\")\n",
    "    # print(f\"  Is OOV (Out Of Vocabulary): {apple_token.is_oov}\")\n",
    "    # print(f\"  Vector (first 5 dims): {apple_token.vector[:5]}\")\n",
    "\n",
    "    # Доступ к вектору документа (усреднение векторов токенов)\n",
    "    # print(f\"\\nDocument: '{doc_md.text}'\")\n",
    "    # print(f\"  Has Vector: {doc_md.has_vector}\")\n",
    "    # print(f\"  Vector Norm: {doc_md.vector_norm}\")\n",
    "    # print(f\"  Vector (first 5 dims): {doc_md.vector[:5]}\")\n",
    "\n",
    "    # Вычисление сходства (на основе статических векторов)\n",
    "    # similarity_apple_orange = apple_token.similarity(orange_token)\n",
    "    # similarity_apple_fruits = apple_token.similarity(fruits_token)\n",
    "    # print(f\"\\nSimilarity ('apple' vs 'orange'): {similarity_apple_orange:.4f}\") # Ожидается высокое\n",
    "    # print(f\"Similarity ('apple' vs 'fruits'): {similarity_apple_fruits:.4f}\") # Ожидается среднее/высокое\n",
    "\n",
    "    # Прямой доступ к векторам через словарь (если нужно)\n",
    "    # apple_vector_vocab = nlp_md.vocab.vectors.get(key=nlp_md.vocab.strings[\"apple\"])\n",
    "    # print(f\"\\nVector for 'apple' from vocab (first 5 dims): {apple_vector_vocab[:5]}\")\n",
    "\n",
    "except OSError:\n",
    "    print(f\"\\nError: Model '{model_name_md}' not found. Please run:\")\n",
    "    print(f\"python -m spacy download {model_name_md}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred loading/using {model_name_md}: {e}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Контекстно-чувствительные Тензоры (Tok2Vec)\n",
    "\n",
    "# Присутствуют во всех современных моделях (`sm`, `md`, `lg`).\n",
    "# Используются внутренне, но можно получить доступ к финальному тензору.\n",
    "\n",
    "model_name_sm = \"en_core_web_sm\" # Подойдет любая модель с tok2vec\n",
    "try:\n",
    "    nlp_sm = spacy.load(model_name_sm)\n",
    "    print(f\"\\nLoaded model with Tok2Vec: '{model_name_sm}'\")\n",
    "\n",
    "    text = \"This is a sample sentence.\"\n",
    "    doc_sm = nlp_sm(text)\n",
    "\n",
    "    # Доступ к тензору документа (выход компонента tok2vec)\n",
    "    # Это НЕ то же самое, что doc.vector в md/lg моделях!\n",
    "    doc_tensor = doc_sm.tensor\n",
    "    # print(f\"\\nTok2Vec tensor shape for '{doc_sm.text}': {doc_tensor.shape}\")\n",
    "    # Форма: (num_tokens, hidden_width) - где hidden_width зависит от конфигурации tok2vec\n",
    "\n",
    "    # Доступ к тензору для отдельного токена\n",
    "    # token_sample = doc_sm[3] # 'sample'\n",
    "    # token_tensor = token_sample.tensor # Не существует такого атрибута напрямую\n",
    "    # Вместо этого берем срез из doc.tensor\n",
    "    # token_tensor_slice = doc_tensor[token_sample.i] # Индекс токена в документе\n",
    "    # print(f\"Tok2Vec tensor for '{token_sample.text}' (first 5 dims): {token_tensor_slice[:5]}\")\n",
    "    # print(f\"Shape: {token_tensor_slice.shape}\")\n",
    "\n",
    "    # Сходство `.similarity()` в моделях `sm` (без статических векторов)\n",
    "    # может быть не определено или давать плохие результаты, так как оно\n",
    "    # обычно ожидает статические векторы.\n",
    "    # token1 = doc_sm[1] # 'is'\n",
    "    # token2 = doc_sm[2] # 'a'\n",
    "    # try:\n",
    "    #     sim_sm = token1.similarity(token2)\n",
    "    #     print(f\"\\nSimilarity in '{model_name_sm}' ('is' vs 'a'): {sim_sm}\")\n",
    "    #     # Может выдать UserWarning, что векторы не загружены.\n",
    "    # except UserWarning as w:\n",
    "    #     print(f\"\\nWarning when calculating similarity in '{model_name_sm}': {w}\")\n",
    "\n",
    "\n",
    "except OSError:\n",
    "    print(f\"\\nError: Model '{model_name_sm}' not found. Please run:\")\n",
    "    print(f\"python -m spacy download {model_name_sm}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred loading/using {model_name_sm}: {e}\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Контекстуализированные Эмбеддинги (Модели `_trf`)\n",
    "\n",
    "# Требуется модель `_trf` и `spacy-transformers`. Установите:\n",
    "# pip install -U spacy-transformers torch # или tensorflow\n",
    "# python -m spacy download en_core_web_trf\n",
    "\n",
    "model_name_trf = \"en_core_web_trf\"\n",
    "try:\n",
    "    nlp_trf = spacy.load(model_name_trf)\n",
    "    print(f\"\\nLoaded Transformer model: '{model_name_trf}'\")\n",
    "\n",
    "    text1 = \"The bank on the river side.\" # 'bank' - берег\n",
    "    text2 = \"I need to go to the bank to withdraw money.\" # 'bank' - финансовое учреждение\n",
    "\n",
    "    doc_trf1 = nlp_trf(text1)\n",
    "    doc_trf2 = nlp_trf(text2)\n",
    "\n",
    "    # Находим токены 'bank' в обоих документах\n",
    "    token_bank1 = doc_trf1[1]\n",
    "    token_bank2 = doc_trf2[6]\n",
    "\n",
    "    # print(f\"\\nToken 1: '{token_bank1.text}' in '{doc_trf1.text}'\")\n",
    "    # print(f\"  Has Vector: {token_bank1.has_vector}\")\n",
    "    # print(f\"  Vector Norm: {token_bank1.vector_norm}\")\n",
    "    # print(f\"  Vector (first 5 dims): {token_bank1.vector[:5]}\")\n",
    "\n",
    "    # print(f\"\\nToken 2: '{token_bank2.text}' in '{doc_trf2.text}'\")\n",
    "    # print(f\"  Has Vector: {token_bank2.has_vector}\")\n",
    "    # print(f\"  Vector Norm: {token_bank2.vector_norm}\")\n",
    "    # print(f\"  Vector (first 5 dims): {token_bank2.vector[:5]}\")\n",
    "\n",
    "    # Сходство между двумя токенами 'bank' (ожидается не очень высоким из-за разного контекста)\n",
    "    # similarity_banks = token_bank1.similarity(token_bank2)\n",
    "    # print(f\"\\nSimilarity between 'bank' (river) and 'bank' (money): {similarity_banks:.4f}\")\n",
    "\n",
    "    # Сходство между документами (использует контекстуальные представления)\n",
    "    # similarity_docs = doc_trf1.similarity(doc_trf2)\n",
    "    # print(f\"Similarity between the two documents: {similarity_docs:.4f}\")\n",
    "\n",
    "    # Доступ к необработанным данным трансформера (продвинутый)\n",
    "    # try:\n",
    "    #     trf_data1 = doc_trf1._.trf_data\n",
    "    #     # print(f\"\\nAccessing raw Transformer data (doc1): {trf_data1 is not None}\")\n",
    "    #     # last_hidden_state1 = trf_data1.last_hidden_state\n",
    "    #     # print(f\"Shape of last hidden state (doc1): {last_hidden_state1.shape}\")\n",
    "    # except AttributeError:\n",
    "    #     print(\"\\nAttribute 'doc._.trf_data' not found. Is spacy-transformers installed and working?\")\n",
    "\n",
    "\n",
    "except OSError:\n",
    "    print(f\"\\nError: Model '{model_name_trf}' not found. Please run:\")\n",
    "    print(f\"python -m spacy download {model_name_trf}\")\n",
    "    print(\"And ensure 'spacy-transformers' and 'torch'/'tensorflow' are installed.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred loading/using {model_name_trf}: {e}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Сводка и Рекомендации\n",
    "\n",
    "# | Тип Модели spaCy | Компонент Эмбеддингов | Тип Эмбеддинга        | Доступ через `.vector` | Учитывает Контекст? | Производительность | Размер Модели |\n",
    "# |------------------|-----------------------|-----------------------|------------------------|---------------------|--------------------|---------------|\n",
    "# | `sm`             | `tok2vec`             | Внутренний тензор     | Нет (или неинформативно)| Локальный           | Высокая            | Маленький     |\n",
    "# | `md`             | `tok2vec`, `vectors`  | Статический + Внутр.  | Да (Статический)       | Локальный (внутр.)  | Средняя            | Средний       |\n",
    "# | `lg`             | `tok2vec`, `vectors`  | Статический + Внутр.  | Да (Статический)       | Локальный (внутр.)  | Средняя/Низкая     | Большой       |\n",
    "# | `trf`            | `transformer`         | Контекстуальный       | Да (Контекстуальный)   | Глобальный          | Низкая (GPU рек.)  | Очень большой |\n",
    "\n",
    "# Рекомендации:\n",
    "# - **Нужна скорость и базовый анализ (POS, Dep, NER)?** Используйте `sm` модели. Эмбеддинги здесь в основном внутренние.\n",
    "# - **Нужно хорошее семантическое сходство на уровне слов без учета контекста или базовые векторы для внешних моделей?** Используйте `md` или `lg` модели и атрибут `.vector`.\n",
    "# - **Нужна максимальная точность, учет контекста для сходства или state-of-the-art признаки для компонентов?** Используйте `trf` модели (с `spacy-transformers`). Будьте готовы к высоким требованиям к ресурсам.\n",
    "# - **Нужен доступ к внутренним контекстно-чувствительным представлениям для кастомных моделей?** Используйте `doc.tensor` (из `tok2vec`) или `doc._.trf_data` (из `transformer`).\n",
    "\n",
    "# Помните, что атрибут `.vector` ведет себя по-разному в зависимости от загруженной модели!\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Введение в Multi-label Text Classification\n",
    "\n",
    "# Что такое Multi-label Classification?\n",
    "# Это задача классификации, в которой каждому экземпляру (в нашем случае, тексту)\n",
    "# может быть присвоено **ноль или несколько** меток (классов, категорий)\n",
    "# из предопределенного набора.\n",
    "\n",
    "# Отличие от Multi-class Classification:\n",
    "# - Multi-class (Многоклассовая): Каждый экземпляр принадлежит **ровно одному** классу\n",
    "#   из нескольких возможных (например, тональность: позитивная ИЛИ негативная ИЛИ нейтральная).\n",
    "#   Используется Softmax на выходе, CrossEntropyLoss.\n",
    "# - Multi-label (Многометочная): Каждый экземпляр может принадлежать **любому количеству**\n",
    "#   классов одновременно, включая ни одного. (например, жанры фильма: может быть\n",
    "#   и \"Боевик\", и \"Комедия\", и \"Фантастика\" одновременно).\n",
    "#   Используется Sigmoid на выходе (для каждого класса независимо), BinaryCrossEntropyLoss.\n",
    "\n",
    "# Примеры Задач:\n",
    "# - Определение жанров фильма по описанию/рецензии.\n",
    "# - Присвоение тем новостной статье (может быть и \"Политика\", и \"Экономика\").\n",
    "# - Тегирование поста в блоге (может быть \"Python\", \"Machine Learning\", \"Tutorial\").\n",
    "# - Определение характеристик продукта, упомянутых в отзыве (цена, дизайн, производительность).\n",
    "# - Категоризация медицинских текстов по нескольким симптомам или диагнозам.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Основные Подходы к Решению\n",
    "\n",
    "# Существует несколько стратегий для решения задач multi-label классификации:\n",
    "\n",
    "# 1. Трансформация Проблемы (Problem Transformation):\n",
    "#    - Идея: Преобразовать multi-label проблему в одну или несколько single-label\n",
    "#      (binary или multi-class) проблем, которые можно решить стандартными алгоритмами.\n",
    "#    - Методы:\n",
    "#      - **Binary Relevance (BR):** Самый простой подход. Обучается отдельный\n",
    "#        бинарный классификатор для каждой метки (принадлежит ли текст метке X?).\n",
    "#        Предсказания делаются независимо для каждой метки.\n",
    "#        *Плюсы:* Простота, можно использовать любой бинарный классификатор.\n",
    "#        *Минусы:* Игнорирует возможные корреляции между метками (например, \"Комедия\" и \"Романтика\" часто идут вместе).\n",
    "#      - **Classifier Chains (CC):** Обучается цепочка бинарных классификаторов.\n",
    "#        Первый классификатор обучается на исходных признаках. Второй обучается\n",
    "#        на исходных признаках + предсказании первого классификатора, и так далее.\n",
    "#        *Плюсы:* Учитывает корреляции между метками (в порядке цепочки).\n",
    "#        *Минусы:* Порядок меток в цепочке важен, ошибки могут накапливаться.\n",
    "#      - **Label Powerset (LP):** Рассматривает каждую уникальную *комбинацию* меток,\n",
    "#        встречающуюся в обучающих данных, как отдельный класс в multi-class задаче.\n",
    "#        *Плюсы:* Идеально учитывает корреляции меток.\n",
    "#        *Минусы:* Количество классов может стать очень большим (до 2^L, где L - число меток),\n",
    "#        проблема с редкими или невиданными комбинациями меток.\n",
    "#    - Библиотеки: Scikit-learn (`sklearn.multiclass` содержит `OneVsRestClassifier` для BR,\n",
    "#      `ClassifierChain`; `skmultilearn` - специализированная библиотека).\n",
    "\n",
    "# 2. Адаптация Алгоритмов (Algorithm Adaptation):\n",
    "#    - Идея: Модифицировать существующие алгоритмы машинного обучения так, чтобы\n",
    "#      они напрямую работали с multi-label данными.\n",
    "#    - Примеры: ML-kNN (Multi-label k-Nearest Neighbors), Multi-output Decision Trees/Random Forests.\n",
    "#    - Менее распространены в стандартных библиотеках по сравнению с методами трансформации\n",
    "#      или глубоким обучением.\n",
    "\n",
    "# 3. Глубокое Обучение (Deep Learning):\n",
    "#    - Идея: Использовать нейронные сети (CNN, RNN, Transformers) для извлечения\n",
    "#      признаков из текста и модифицировать выходной слой для multi-label предсказаний.\n",
    "#    - Реализация:\n",
    "#      - **Выходной Слой:** Полносвязный слой с количеством нейронов, равным\n",
    "#        количеству меток (`num_labels`).\n",
    "#      - **Функция Активации:** Применяется **Sigmoid** к выходу каждого нейрона.\n",
    "#        Sigmoid выдает вероятность от 0 до 1 для *каждой* метки независимо.\n",
    "#      - **Функция Потерь:** Используется **Binary Cross-Entropy Loss** (BCE) или\n",
    "#        `BCEWithLogitsLoss` (более стабильна, применяется к логитам до Sigmoid).\n",
    "#        Лосс вычисляется для каждой метки и затем усредняется или суммируется.\n",
    "#    - Преимущества: Автоматическое извлечение признаков, state-of-the-art результаты,\n",
    "#      естественным образом обрабатывает multi-label выход.\n",
    "#    - Библиотеки: PyTorch, TensorFlow/Keras, Hugging Face Transformers (модели\n",
    "#      `ForSequenceClassification` можно использовать с BCEWithLogitsLoss).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Представление Текста и Подготовка Данных\n",
    "\n",
    "# 1. Представление Текста (Feature Extraction):\n",
    "#    - **TF-IDF:** Хороший выбор для методов трансформации проблемы с классическими ML алгоритмами (SVM, Logistic Regression).\n",
    "#    - **Эмбеддинги (Word2Vec, GloVe, FastText, Sentence-BERT):** Могут использоваться как признаки для классических ML или как входные слои для DL моделей.\n",
    "#    - **Выходы Трансформеров (BERT и др.):** Используются как вход для DL моделей.\n",
    "\n",
    "# 2. Представление Меток (Labels):\n",
    "#    - Обычно метки представляются в виде **бинарного вектора (или матрицы)**.\n",
    "#    - Если у вас `L` возможных меток, то каждый текст будет иметь вектор длины `L`,\n",
    "#      где `1` стоит на позициях, соответствующих присвоенным меткам, и `0` на остальных.\n",
    "#    - Пример: Метки = [\"Спорт\", \"Политика\", \"Технологии\"].\n",
    "#      Текст про спорт и политику: `[1, 1, 0]`\n",
    "#      Текст только про технологии: `[0, 0, 1]`\n",
    "#      Текст ни о чем из этого: `[0, 0, 0]`\n",
    "#    - Scikit-learn предоставляет `MultiLabelBinarizer` для такого преобразования.\n",
    "\n",
    "# 3. Предобработка Текста:\n",
    "#    - Стандартные шаги: нижний регистр, удаление шума (HTML, URL), токенизация.\n",
    "#    - Удаление стоп-слов, стемминг/лемматизация - по необходимости, в зависимости от выбранного метода векторизации и модели.\n",
    "\n",
    "# 4. Разделение Данных: Train / Validation / Test.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Метрики Оценки для Multi-label Классификации\n",
    "\n",
    "# Стандартная Accuracy (доля полностью правильно предсказанных *наборов* меток)\n",
    "# часто бывает слишком строгой и низкой. Используют другие метрики:\n",
    "\n",
    "# 1. Метрики на Основе Примеров (Example-Based):\n",
    "#    - Вычисляются для каждого примера, затем усредняются по всем примерам.\n",
    "#    - **Subset Accuracy (Exact Match Ratio):** Доля примеров, для которых *все* метки предсказаны абсолютно правильно. `accuracy_score` в sklearn.\n",
    "#    - **Hamming Loss:** Доля неправильно предсказанных меток (1 - (TP+TN)/(TP+TN+FP+FN) для каждой метки, усредненное). Чем меньше, тем лучше. `hamming_loss` в sklearn.\n",
    "#    - **Accuracy (Example-Based):** Jaccard-индекс для каждого примера, усредненный. |Intersection| / |Union|.\n",
    "#    - **Precision (Example-Based):** |Intersection| / |Predicted Labels|. Усредненное.\n",
    "#    - **Recall (Example-Based):** |Intersection| / |True Labels|. Усредненное.\n",
    "#    - **F1-Score (Example-Based):** Гармоническое среднее Precision и Recall. Усредненное.\n",
    "\n",
    "# 2. Метрики на Основе Меток (Label-Based):\n",
    "#    - Вычисляются для каждой метки отдельно, затем усредняются.\n",
    "#    - **Micro-Average (Precision, Recall, F1):** Считает метрики глобально по всем парам (пример, метка). Хорошо отражает общую производительность, чувствительна к большим классам.\n",
    "#    - **Macro-Average (Precision, Recall, F1):** Считает метрики для каждой метки и усредняет (не взвешивая). Дает равный вес каждой метке, полезна, если важна производительность на редких метках.\n",
    "#    - **Weighted-Average (Precision, Recall, F1):** Как Macro, но усреднение взвешено по количеству истинных примеров для каждой метки.\n",
    "#    - **Samples-Average (Precision, Recall, F1):** Считает метрики для каждого примера и усредняет (то же, что Example-Based F1 и т.д.).\n",
    "\n",
    "# `classification_report` из Scikit-learn поддерживает вычисление Micro, Macro, Weighted и Samples average для multi-label задач при передаче бинаризованных меток.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Пример Задачи и Решения (Binary Relevance с Scikit-learn)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Классифицировать короткие тексты по нескольким возможным тегам (меткам).\n",
    "# Например: \"python tutorial\", \"machine learning book\", \"python machine learning\".\n",
    "# Метки: \"python\", \"tutorial\", \"machine learning\", \"book\".\n",
    "\n",
    "# --- Решение (Полный Код) ---\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # Для преобразования меток\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier # Реализует Binary Relevance\n",
    "from sklearn.svm import LinearSVC # Базовый бинарный классификатор\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, classification_report\n",
    "\n",
    "# 1. Пример Данных (Тексты и Списки Меток)\n",
    "texts = [\n",
    "    \"learn python programming basics\",\n",
    "    \"advanced machine learning techniques\",\n",
    "    \"a good book on python data science\",\n",
    "    \"tutorial for machine learning beginners\",\n",
    "    \"python for machine learning tutorial and book\",\n",
    "    \"deep learning concepts explained\",\n",
    "    \"introduction to python\",\n",
    "    \"best machine learning books 2024\",\n",
    "    \"python web development tutorial\",\n",
    "    \"natural language processing with python book\"\n",
    "]\n",
    "# Метки для каждого текста (списки строк)\n",
    "labels = [\n",
    "    [\"python\", \"tutorial\"],\n",
    "    [\"machine learning\"],\n",
    "    [\"python\", \"book\", \"data science\"], # Добавим data science для примера\n",
    "    [\"machine learning\", \"tutorial\"],\n",
    "    [\"python\", \"machine learning\", \"tutorial\", \"book\"],\n",
    "    [\"deep learning\"], # Добавим deep learning\n",
    "    [\"python\", \"tutorial\"],\n",
    "    [\"machine learning\", \"book\"],\n",
    "    [\"python\", \"web development\", \"tutorial\"], # Добавим web development\n",
    "    [\"python\", \"natural language processing\", \"book\"] # Добавим nlp\n",
    "]\n",
    "\n",
    "# 2. Подготовка Данных\n",
    "# Бинаризация меток\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_binarized = mlb.fit_transform(labels)\n",
    "\n",
    "# print(\"Classes found by MultiLabelBinarizer:\", mlb.classes_)\n",
    "# print(\"\\nBinarized labels (first 5):\")\n",
    "# print(y_binarized[:5])\n",
    "# print(f\"\\nShape of binarized labels: {y_binarized.shape}\") # (num_samples, num_classes)\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, y_binarized, test_size=0.3, random_state=42\n",
    ")\n",
    "# print(f\"\\nTrain samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "\n",
    "# 3. Создание и Обучение Модели (Pipeline + Binary Relevance)\n",
    "# Используем TF-IDF для векторизации и LinearSVC как базовый классификатор\n",
    "# OneVsRestClassifier обучит по одному LinearSVC для каждой метки\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('clf', OneVsRestClassifier(LinearSVC(random_state=42, dual='auto'), n_jobs=-1)) # n_jobs=-1 для параллелизма\n",
    "])\n",
    "\n",
    "# print(\"\\nTraining the model...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "# print(\"Training complete.\")\n",
    "\n",
    "# 4. Предсказание на Тестовой Выборке\n",
    "y_pred = pipeline.predict(X_test)\n",
    "# print(\"\\nPredictions (binary matrix, first 3):\")\n",
    "# print(y_pred[:3])\n",
    "\n",
    "# 5. Оценка Модели\n",
    "# Subset Accuracy (Exact Match Ratio)\n",
    "subset_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nSubset Accuracy (Exact Match): {subset_acc:.4f}\")\n",
    "\n",
    "# Hamming Loss\n",
    "h_loss = hamming_loss(y_test, y_pred)\n",
    "print(f\"Hamming Loss: {h_loss:.4f}\")\n",
    "\n",
    "# Classification Report (Micro, Macro, Weighted, Samples averages)\n",
    "# target_names передает имена классов для отчета\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "# 6. Предсказание на Новых Данных\n",
    "new_texts = [\n",
    "    \"machine learning tutorial using python\",\n",
    "    \"best books about deep learning\",\n",
    "    \"web development guide\"\n",
    "]\n",
    "new_pred_binarized = pipeline.predict(new_texts)\n",
    "\n",
    "# Преобразование бинарных предсказаний обратно в списки меток\n",
    "new_pred_labels = mlb.inverse_transform(new_pred_binarized)\n",
    "\n",
    "print(\"\\n--- Predictions for New Texts ---\")\n",
    "for text, pred_labels in zip(new_texts, new_pred_labels):\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"  Predicted Labels: {pred_labels if pred_labels else 'None'}\")\n",
    "\n",
    "\n",
    "# --- Конец Примера (Scikit-learn) ---\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 7: Рекомендации по Выбору Подхода\n",
    "\n",
    "# - **Binary Relevance (BR) с TF-IDF + SVM/Logistic Regression:** Отличный и сильный baseline. Прост в реализации (Scikit-learn). Хорошо работает, если корреляции между метками не очень сильные или не критичны.\n",
    "# - **Classifier Chains (CC):** Попробуйте, если BR недостаточно и есть основания полагать, что учет предсказаний предыдущих меток поможет. Требует подбора оптимального порядка цепочки.\n",
    "# - **Label Powerset (LP):** Используйте с осторожностью, только если количество уникальных комбинаций меток не слишком велико.\n",
    "# - **Deep Learning (CNN/RNN/Transformers + Sigmoid + BCE Loss):** Подход выбора для достижения state-of-the-art результатов, особенно при наличии больших данных. Естественным образом обрабатывает multi-label выход и может неявно улавливать корреляции меток через общие слои извлечения признаков. Требует больше ресурсов и данных.\n",
    "\n",
    "# Начинать часто стоит с Binary Relevance как с базовой модели.\n",
    "# --------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Задача Multi-label Классификации с Нейронными Сетями\n",
    "\n",
    "# Цель: Классифицировать тексты по нескольким возможным тегам (меткам)\n",
    "#       (\"python\", \"tutorial\", \"machine learning\", \"book\", и т.д.),\n",
    "#       используя нейронную сеть (BiLSTM) на PyTorch.\n",
    "\n",
    "# Подход:\n",
    "# 1. Предобработка текста и меток (токенизация, словарь, паддинг, бинаризация меток).\n",
    "# 2. Создание PyTorch Dataset и DataLoader.\n",
    "# 3. Определение архитектуры модели (Embedding -> BiLSTM -> Linear).\n",
    "# 4. Использование Sigmoid активации (неявно через лосс) и Binary Cross-Entropy Loss.\n",
    "# 5. Обучение и оценка модели.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Импорты и Настройки\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # Используем для удобства бинаризации\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import hamming_loss, accuracy_score, classification_report\n",
    "\n",
    "# Настройки\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Параметры модели и обучения\n",
    "VOCAB_SIZE = 0\n",
    "EMBED_DIM = 100\n",
    "HIDDEN_DIM = 128\n",
    "NUM_CLASSES = 0 # Определится после бинаризации меток\n",
    "NUM_LAYERS = 1 # Упростим до 1 слоя BiLSTM\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT_PROB = 0.4\n",
    "NUM_EPOCHS = 20 # Увеличим эпохи для DL\n",
    "BATCH_SIZE = 2 # Маленький батч для примера\n",
    "LEARNING_RATE = 0.002\n",
    "MAX_SEQ_LEN = 20 # Макс длина для коротких текстов\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Подготовка Данных (Схожа, но с бинаризацией меток)\n",
    "\n",
    "# --- 3.1 Пример Данных (Те же) ---\n",
    "texts = [\n",
    "    \"learn python programming basics\",\n",
    "    \"advanced machine learning techniques\",\n",
    "    \"a good book on python data science\",\n",
    "    \"tutorial for machine learning beginners\",\n",
    "    \"python for machine learning tutorial and book\",\n",
    "    \"deep learning concepts explained\",\n",
    "    \"introduction to python\",\n",
    "    \"best machine learning books 2024\",\n",
    "    \"python web development tutorial\",\n",
    "    \"natural language processing with python book\"\n",
    "]\n",
    "labels = [\n",
    "    [\"python\", \"tutorial\"], [\"machine learning\"],\n",
    "    [\"python\", \"book\", \"data science\"], [\"machine learning\", \"tutorial\"],\n",
    "    [\"python\", \"machine learning\", \"tutorial\", \"book\"], [\"deep learning\"],\n",
    "    [\"python\", \"tutorial\"], [\"machine learning\", \"book\"],\n",
    "    [\"python\", \"web development\", \"tutorial\"],\n",
    "    [\"python\", \"natural language processing\", \"book\"]\n",
    "]\n",
    "\n",
    "# --- 3.2 Предобработка Текста (Та же) ---\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "processed_texts = [preprocess_text(text) for text in texts]\n",
    "\n",
    "# --- 3.3 Построение Словаря (То же) ---\n",
    "word_counts = Counter(token for text in processed_texts for token in text)\n",
    "vocab = {word: i+2 for i, (word, count) in enumerate(word_counts.items())}\n",
    "vocab[PAD_TOKEN] = 0\n",
    "vocab[UNK_TOKEN] = 1\n",
    "VOCAB_SIZE = len(vocab)\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")\n",
    "\n",
    "# --- 3.4 Конвертация Текста в Индексы и Паддинг (Те же) ---\n",
    "def text_to_indices(text_tokens, vocab):\n",
    "    return [vocab.get(token, vocab[UNK_TOKEN]) for token in text_tokens]\n",
    "\n",
    "def pad_sequence(indices, max_len, pad_idx):\n",
    "    current_len = len(indices)\n",
    "    if current_len >= max_len: return indices[:max_len]\n",
    "    else: return indices + [pad_idx] * (max_len - current_len)\n",
    "\n",
    "text_indices = [pad_sequence(text_to_indices(text, vocab), MAX_SEQ_LEN, vocab[PAD_TOKEN]) for text in processed_texts]\n",
    "\n",
    "# --- 3.5 Бинаризация Меток ---\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_binarized = mlb.fit_transform(labels)\n",
    "NUM_CLASSES = len(mlb.classes_) # Обновляем количество классов\n",
    "print(\"Classes found by MultiLabelBinarizer:\", mlb.classes_)\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "# print(\"Binarized labels shape:\", y_binarized.shape)\n",
    "\n",
    "# --- 3.6 Разделение Данных ---\n",
    "X_train_idx, X_test_idx, y_train_bin, y_test_bin = train_test_split(\n",
    "    text_indices, y_binarized, test_size=0.3, random_state=42\n",
    ")\n",
    "print(f\"\\nTrain samples: {len(X_train_idx)}, Test samples: {len(X_test_idx)}\")\n",
    "\n",
    "# --- 3.7 Создание PyTorch Dataset ---\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, indices, labels):\n",
    "        self.indices = indices\n",
    "        # **Важно:** BCEWithLogitsLoss ожидает метки типа float\n",
    "        self.labels = labels.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = torch.tensor(self.indices[idx], dtype=torch.long)\n",
    "        label_vector = torch.tensor(self.labels[idx], dtype=torch.float) # Float для BCE\n",
    "        return sequence, label_vector\n",
    "\n",
    "train_dataset = MultiLabelDataset(X_train_idx, y_train_bin)\n",
    "test_dataset = MultiLabelDataset(X_test_idx, y_test_bin)\n",
    "\n",
    "# --- 3.8 Создание DataLoader ---\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Определение Модели (BiLSTM для Multi-label)\n",
    "\n",
    "class BiLSTMMultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes,\n",
    "                 num_layers, bidirectional, dropout_prob, pad_idx):\n",
    "        super(BiLSTMMultiLabelClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.rnn = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                           bidirectional=bidirectional, batch_first=True,\n",
    "                           dropout=dropout_prob if num_layers > 1 else 0)\n",
    "\n",
    "        fc_in_features = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.fc = nn.Linear(fc_in_features, num_classes) # Выход = количество классов\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, text_indices):\n",
    "        # text_indices: [batch_size, seq_len]\n",
    "        embedded = self.dropout(self.embedding(text_indices))\n",
    "        # embedded: [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "\n",
    "        # hidden: [batch_size, hidden_dim * num_directions]\n",
    "        logits = self.fc(hidden) # Получаем логиты для каждого класса\n",
    "        # logits: [batch_size, num_classes]\n",
    "        # **НЕ применяем Sigmoid здесь**, т.к. будем использовать BCEWithLogitsLoss\n",
    "        return logits\n",
    "\n",
    "# Инициализация модели\n",
    "model = BiLSTMMultiLabelClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=EMBED_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    dropout_prob=DROPOUT_PROB,\n",
    "    pad_idx=vocab[PAD_TOKEN]\n",
    ")\n",
    "model.to(DEVICE)\n",
    "print(\"\\nModel Initialized:\")\n",
    "# print(model)\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters')\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Обучение Модели\n",
    "\n",
    "# Функция потерь и оптимизатор\n",
    "# Используем BCEWithLogitsLoss для multi-label классификации\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"\\nStarting Training...\")\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Training]\")\n",
    "    for sequences, labels in pbar:\n",
    "        sequences = sequences.to(DEVICE)\n",
    "        labels = labels.to(DEVICE) # Метки уже float\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(sequences) # Прямой проход -> логиты\n",
    "        loss = criterion(logits, labels) # Считаем BCE лосс\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({'Loss': loss.item()})\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Summary: Train Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "training_end_time = time.time()\n",
    "print(f\"Training finished in {training_end_time - training_start_time:.2f} seconds.\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Оценка Модели\n",
    "\n",
    "print(\"\\nStarting Evaluation...\")\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    pbar_test = tqdm(test_loader, desc=\"[Evaluating]\")\n",
    "    for sequences, labels in pbar_test:\n",
    "        sequences = sequences.to(DEVICE)\n",
    "        # labels остаются на CPU для sklearn метрик, но копируем для сравнения\n",
    "        labels_cpu = labels.numpy()\n",
    "        all_labels.extend(labels_cpu)\n",
    "\n",
    "        logits = model(sequences)\n",
    "        # Применяем Sigmoid к логитам, чтобы получить вероятности\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "        # Применяем порог (0.5) для получения бинарных предсказаний\n",
    "        preds = (probabilities > 0.5).cpu().numpy().astype(int)\n",
    "        all_preds.extend(preds)\n",
    "\n",
    "# Конвертируем списки в numpy массивы\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Расчет метрик\n",
    "subset_acc = accuracy_score(all_labels, all_preds) # Exact Match Ratio\n",
    "h_loss = hamming_loss(all_labels, all_preds)\n",
    "\n",
    "print(f\"\\nEvaluation Results:\")\n",
    "print(f\"Subset Accuracy (Exact Match): {subset_acc:.4f}\")\n",
    "print(f\"Hamming Loss: {h_loss:.4f}\")\n",
    "print(\"\\nClassification Report (Label-Based Averages):\")\n",
    "# Используем MultiLabelBinarizer для получения имен классов\n",
    "print(classification_report(all_labels, all_preds, target_names=mlb.classes_, zero_division=0))\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 7: Инференс (Предсказание на Новом Тексте)\n",
    "\n",
    "def predict_multilabel(text, model, vocab, max_len, pad_idx, mlb_instance, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    tokens = preprocess_text(text)\n",
    "    indices = text_to_indices(tokens, vocab)\n",
    "    padded_indices = pad_sequence(indices, max_len, pad_idx)\n",
    "    tensor = torch.tensor(padded_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(tensor)\n",
    "        probabilities = torch.sigmoid(logits).squeeze(0) # [num_classes]\n",
    "\n",
    "    # Применяем порог\n",
    "    predictions_binary = (probabilities > threshold).cpu().numpy().astype(int)\n",
    "    # Преобразуем бинарный вектор обратно в метки\n",
    "    predicted_labels = mlb_instance.inverse_transform(predictions_binary.reshape(1, -1)) # reshape для одного примера\n",
    "\n",
    "    # Собираем вероятности для предсказанных меток\n",
    "    predicted_confidences = {}\n",
    "    for i, label_name in enumerate(mlb_instance.classes_):\n",
    "        if predictions_binary[i] == 1:\n",
    "            predicted_confidences[label_name] = probabilities[i].item()\n",
    "\n",
    "    return predicted_labels[0], predicted_confidences # [0] т.к. inverse_transform возвращает список списков\n",
    "\n",
    "# Примеры предсказаний\n",
    "print(\"\\n--- Inference Examples (Neural Network) ---\")\n",
    "new_texts = [\n",
    "    \"machine learning tutorial using python\",\n",
    "    \"best books about deep learning\",\n",
    "    \"web development guide\"\n",
    "]\n",
    "\n",
    "for text in new_texts:\n",
    "    pred_labels, pred_confs = predict_multilabel(text, model, vocab, MAX_SEQ_LEN, vocab[PAD_TOKEN], mlb, DEVICE)\n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"  Predicted Labels: {pred_labels if pred_labels else 'None'}\")\n",
    "    if pred_confs:\n",
    "        conf_str = \", \".join([f\"{k}: {v:.2f}\" for k, v in pred_confs.items()])\n",
    "        print(f\"  Confidences: {{{conf_str}}}\")\n",
    "\n",
    "# --- Конец Примера ---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
