{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Задача Классификации Изображений (PyTorch)\n",
    "\n",
    "# Что такое Классификация Изображений?\n",
    "# Это задача Computer Vision по присвоению изображению одной метки (класса)\n",
    "# из предопределенного набора (например, \"кошка\", \"собака\").\n",
    "# Вход: Изображение (представленное как тензор в PyTorch).\n",
    "# Выход: Индекс предсказанного класса или вероятности для каждого класса.\n",
    "\n",
    "# Цель Классификации:\n",
    "# Автоматически категоризировать изображения.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Основные Подходы (Фокус на PyTorch)\n",
    "\n",
    "# Современный подход (Deep Learning с PyTorch):\n",
    "# - Сверточные Нейронные Сети (Convolutional Neural Networks - CNNs)\n",
    "# - PyTorch предоставляет мощные инструменты для построения, обучения и развертывания CNN:\n",
    "#   - `torch.nn`: Модули для слоев (Conv2d, Linear, ReLU, MaxPool2d и т.д.).\n",
    "#   - `torchvision`: Утилиты для CV (датасеты, трансформеры, предобученные модели).\n",
    "#   - `torch.optim`: Оптимизаторы (Adam, SGD).\n",
    "#   - `torch.utils.data.DataLoader`: Эффективная загрузка данных.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: CNNs для Классификации в PyTorch\n",
    "\n",
    "# Ключевые слои (`torch.nn`):\n",
    "\n",
    "# 1. Сверточный слой (Convolutional Layer):\n",
    "#    - Применяет фильтры для извлечения пространственных признаков.\n",
    "#    - `in_channels`: Количество каналов входного изображения/карты признаков.\n",
    "#    - `out_channels`: Количество фильтров (определяет глубину выходной карты признаков).\n",
    "#    - `kernel_size`: Размер фильтра (например, 3 или (3, 3)).\n",
    "#    - `stride`: Шаг перемещения фильтра.\n",
    "#    - `padding`: Добавление пикселей по краям для контроля размера выхода.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# 2. Функция Активации (Activation Function):\n",
    "#    - Вносит нелинейность.\n",
    "#    - Применяются как модули `torch.nn` или функции из `torch.nn.functional`.\n",
    "relu_activation = nn.ReLU()\n",
    "# или\n",
    "# import torch.nn.functional as F\n",
    "# output = F.relu(input_tensor)\n",
    "\n",
    "# 3. Слой Субдискретизации (Pooling Layer):\n",
    "#    - Уменьшает пространственный размер.\n",
    "#    - `kernel_size`: Размер окна пулинга.\n",
    "#    - `stride`: Шаг окна пулинга.\n",
    "pool_layer = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "# 4. Слой Выравнивания (Flatten):\n",
    "#    - Преобразует многомерный тензор в 1D вектор для полносвязных слоев.\n",
    "#    - Можно использовать `nn.Flatten()` или `tensor.view(batch_size, -1)`.\n",
    "flatten_layer = nn.Flatten()\n",
    "# или\n",
    "# flattened_tensor = output_from_pooling.view(output_from_pooling.size(0), -1)\n",
    "\n",
    "# 5. Полносвязный слой (Fully Connected / Dense Layer):\n",
    "#    - Применяет линейное преобразование к входному вектору.\n",
    "#    - `in_features`: Размер входного вектора (выход flatten).\n",
    "#    - `out_features`: Количество нейронов в слое.\n",
    "dense_layer = nn.Linear(in_features=128 * 7 * 7, out_features=512) # Пример размеров\n",
    "\n",
    "# 6. Выходной слой (Output Layer):\n",
    "#    - Полносвязный слой с количеством нейронов = количество классов.\n",
    "#    - Для мультиклассовой классификации: активация Softmax неявно встроена в `nn.CrossEntropyLoss`, поэтому последний слой обычно просто `nn.Linear`.\n",
    "#    - Для бинарной классификации: `nn.Linear(..., out_features=1)` + `nn.Sigmoid()` (или использовать `nn.BCEWithLogitsLoss` без Sigmoid).\n",
    "output_layer_multi = nn.Linear(in_features=512, out_features=10) # 10 классов\n",
    "output_layer_binary = nn.Linear(in_features=512, out_features=1)\n",
    "sigmoid_activation = nn.Sigmoid()\n",
    "\n",
    "# Определение Модели в PyTorch:\n",
    "# - Наследуемся от `nn.Module`.\n",
    "# - Определяем слои как атрибуты в `__init__`.\n",
    "# - Определяем прямой проход (forward pass) в методе `forward`.\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Определяем слои здесь\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Размер после пулинга нужно рассчитать или определить динамически\n",
    "        # Например, для входа 32x32: 32 -> pool(16) -> pool(8). Размер = 32 * 8 * 8\n",
    "        # self.fc1 = nn.Linear(32 * 8 * 8, 128)\n",
    "        # self.relu3 = nn.ReLU()\n",
    "        # self.fc2 = nn.Linear(128, num_classes)\n",
    "        # Вместо жесткого кодирования размера можно использовать nn.AdaptiveAvgPool2d\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1)) # Сводит к 1x1\n",
    "        self.fc = nn.Linear(32 * 1 * 1, num_classes) # Теперь размер известен\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Определяем последовательность применения слоев\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        # x = self.flatten(x)\n",
    "        # x = self.relu3(self.fc1(x))\n",
    "        # x = self.fc2(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Процесс Обучения и Использования (PyTorch)\n",
    "\n",
    "# 1. Сбор и Подготовка Данных (`torchvision`):\n",
    "#    - `torchvision.datasets`: Загрузка стандартных датасетов (CIFAR10, ImageNet) или своих (`ImageFolder`).\n",
    "#    - `torchvision.transforms`: Предобработка и аугментация изображений.\n",
    "#      - `transforms.ToTensor()`: Конвертирует PIL Image/numpy array в PyTorch тензор и масштабирует в [0, 1].\n",
    "#      - `transforms.Normalize(mean, std)`: Нормализует тензор.\n",
    "#      - `transforms.Resize()`, `transforms.CenterCrop()`, `transforms.RandomHorizontalFlip()`, etc.\n",
    "#    - `torch.utils.data.Dataset`: Базовый класс для создания своих датасетов.\n",
    "#    - `torch.utils.data.DataLoader`: Загружает данные батчами, перемешивает, использует несколько воркеров.\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Пример трансформаций\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # Пример размера для ResNet\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Стандартные для ImageNet\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Пример загрузки с ImageFolder (ожидает структуру: root/class_a/img1.jpg, root/class_b/img2.jpg)\n",
    "# train_dataset = datasets.ImageFolder(root='path/to/train', transform=train_transform)\n",
    "# val_dataset = datasets.ImageFolder(root='path/to/validation', transform=val_transform)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# 2. Определение/Выбор Модели:\n",
    "#    - Создать свою (`nn.Module`).\n",
    "#    - Использовать предобученную из `torchvision.models`.\n",
    "import torchvision.models as models\n",
    "\n",
    "# Пример использования предобученного ResNet18\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# # Заменить последний слой для своего количества классов\n",
    "# num_ftrs = model.fc.in_features\n",
    "# model.fc = nn.Linear(num_ftrs, num_classes) # num_classes - ваше количество классов\n",
    "\n",
    "# 3. Определение Устройств, Функции Потерь и Оптимизатора:\n",
    "#    - Перенос модели и данных на GPU (если доступно).\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "#    - Функция потерь (`torch.nn`).\n",
    "criterion = nn.CrossEntropyLoss() # Для мультиклассовой классификации\n",
    "# criterion = nn.BCEWithLogitsLoss() # Для бинарной (более стабильна, чем Sigmoid + BCELoss)\n",
    "\n",
    "#    - Оптимизатор (`torch.optim`).\n",
    "import torch.optim as optim\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# 4. Цикл Обучения (Training Loop):\n",
    "#    - Итерация по эпохам.\n",
    "#    - Внутри эпохи итерация по батчам из `DataLoader`.\n",
    "#    - Перенос данных на `device`.\n",
    "#    - Обнуление градиентов (`optimizer.zero_grad()`).\n",
    "#    - Прямой проход (`outputs = model(inputs)`).\n",
    "#    - Вычисление потерь (`loss = criterion(outputs, labels)`).\n",
    "#    - Обратный проход (`loss.backward()`).\n",
    "#    - Шаг оптимизатора (`optimizer.step()`).\n",
    "#    - (Опционально) Расчет метрик (accuracy).\n",
    "\n",
    "# # --- Псевдокод Тренировочного Цикла ---\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train() # Перевести модель в режим обучения\n",
    "#     running_loss = 0.0\n",
    "#     correct_train = 0\n",
    "#     total_train = 0\n",
    "#     for i, data in enumerate(train_loader, 0):\n",
    "#         inputs, labels = data[0].to(device), data[1].to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#\n",
    "#         running_loss += loss.item()\n",
    "#         # Расчет accuracy (пример)\n",
    "#         _, predicted = torch.max(outputs.data, 1) # Для CrossEntropyLoss\n",
    "#         # Для BCEWithLogitsLoss: predicted = (torch.sigmoid(outputs.data) > 0.5).float()\n",
    "#         total_train += labels.size(0)\n",
    "#         correct_train += (predicted == labels).sum().item()\n",
    "#\n",
    "#     epoch_loss = running_loss / len(train_loader)\n",
    "#     epoch_acc = 100 * correct_train / total_train\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}%')\n",
    "#\n",
    "#     # --- Валидационный Цикл ---\n",
    "#     model.eval() # Перевести модель в режим оценки (отключает dropout, batchnorm использует накопленную статистику)\n",
    "#     val_loss = 0.0\n",
    "#     correct_val = 0\n",
    "#     total_val = 0\n",
    "#     with torch.no_grad(): # Отключить вычисление градиентов\n",
    "#         for data in val_loader:\n",
    "#             inputs, labels = data[0].to(device), data[1].to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             val_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total_val += labels.size(0)\n",
    "#             correct_val += (predicted == labels).sum().item()\n",
    "#\n",
    "#     val_epoch_loss = val_loss / len(val_loader)\n",
    "#     val_epoch_acc = 100 * correct_val / total_val\n",
    "#     print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_acc:.2f}%')\n",
    "# # --- Конец Псевдокода ---\n",
    "\n",
    "# 5. Оценка Модели (Evaluation):\n",
    "#    - Аналогично валидационному циклу, но на тестовой выборке.\n",
    "#    - Использование `model.eval()` и `torch.no_grad()`.\n",
    "#    - Расчет финальных метрик (Accuracy, Precision, Recall, F1, Confusion Matrix).\n",
    "\n",
    "# 6. Использование Модели (Inference/Prediction):\n",
    "#    - Загрузка обученных весов (`model.load_state_dict(torch.load(PATH))`).\n",
    "#    - Перевод модели в режим оценки (`model.eval()`).\n",
    "#    - Подготовка входного изображения (трансформации, добавление batch dimension, перенос на `device`).\n",
    "#    - Отключение градиентов (`with torch.no_grad():`).\n",
    "#    - Получение выхода модели (`outputs = model(input_tensor)`).\n",
    "#    - Интерпретация выхода:\n",
    "#      - `torch.max(outputs, 1)` -> получение индекса класса с максимальной логитом/вероятностью.\n",
    "#      - `torch.softmax(outputs, dim=1)` -> получение вероятностей для всех классов.\n",
    "#      - `torch.sigmoid(outputs)` -> получение вероятности для бинарной классификации (если использовался `nn.Linear` без Sigmoid на выходе).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Популярные Датасеты (см. предыдущий ответ)\n",
    "# - MNIST, Fashion-MNIST, CIFAR-10, CIFAR-100, ImageNet, etc.\n",
    "# - `torchvision.datasets` имеет встроенные классы для многих из них.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Метрики Оценки (см. предыдущий ответ)\n",
    "# - Accuracy, Precision, Recall, F1-Score, Confusion Matrix, AUC-ROC.\n",
    "# - Библиотеки вроде `scikit-learn` могут помочь в расчете этих метрик из выходов модели и истинных меток.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 7: Пример Задачи и Решения (Концептуально, PyTorch)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Классифицировать изображения из датасета CIFAR-10 (10 классов: самолет, автомобиль, птица, кошка, олень, собака, лягушка, лошадь, корабль, грузовик).\n",
    "# Использовать PyTorch и `torchvision`.\n",
    "\n",
    "# --- Решение (концептуальное, PyTorch) ---\n",
    "\n",
    "# Шаг 1: Импорт библиотек\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Шаг 2: Подготовка данных (CIFAR-10)\n",
    "# Трансформации (примерные)\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # Нормализация для CIFAR\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Загрузка датасетов\n",
    "# trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "#                                         download=True, transform=transform)\n",
    "# trainloader = DataLoader(trainset, batch_size=batch_size,\n",
    "#                                           shuffle=True, num_workers=2)\n",
    "#\n",
    "# testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "#                                        download=True, transform=transform)\n",
    "# testloader = DataLoader(testset, batch_size=batch_size,\n",
    "#                                          shuffle=False, num_workers=2)\n",
    "#\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "#            'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "num_classes = 10\n",
    "\n",
    "# Шаг 3: Определение архитектуры CNN модели\n",
    "# Можно использовать SimpleCNN из Блока 3 или предобученную\n",
    "# model = SimpleCNN(num_classes=num_classes)\n",
    "# Или, например, ResNet18\n",
    "# model = models.resnet18(pretrained=False) # Обучаем с нуля или pretrained=True для transfer learning\n",
    "# num_ftrs = model.fc.in_features\n",
    "# model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Шаг 4: Определение устройства, потерь и оптимизатора\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Шаг 5: Обучение модели\n",
    "# # Используем цикл обучения, как в псевдокоде из Блока 4\n",
    "# # ... (код цикла обучения с trainloader и testloader для валидации) ...\n",
    "# print('Finished Training')\n",
    "#\n",
    "# # (Опционально) Сохранение модели\n",
    "# # PATH = './cifar_net.pth'\n",
    "# # torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# Шаг 6: Оценка модели на тестовой выборке\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# model.eval() # Перевод в режим оценки\n",
    "# with torch.no_grad():\n",
    "#     for data in testloader:\n",
    "#         images, labels = data[0].to(device), data[1].to(device)\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "#\n",
    "# accuracy = 100 * correct / total\n",
    "# print(f'Accuracy of the network on the 10000 test images: {accuracy:.2f} %')\n",
    "\n",
    "# Шаг 7: Использование модели для предсказания на одном изображении\n",
    "# # Загрузка модели (если нужно)\n",
    "# # model = SimpleCNN(num_classes=num_classes) # Создать экземпляр той же архитектуры\n",
    "# # model.load_state_dict(torch.load(PATH))\n",
    "# # model.to(device)\n",
    "# model.eval()\n",
    "#\n",
    "# # Получение одного изображения из тестового набора\n",
    "# dataiter = iter(testloader)\n",
    "# images, labels = next(dataiter)\n",
    "# image_to_predict = images[0].unsqueeze(0).to(device) # Взять первое, добавить batch dim, отправить на device\n",
    "# true_label = labels[0]\n",
    "#\n",
    "# # Предсказание\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(image_to_predict)\n",
    "#     probabilities = torch.softmax(outputs, dim=1)\n",
    "#     confidence, predicted_index = torch.max(probabilities, 1)\n",
    "#\n",
    "# predicted_class = classes[predicted_index.item()]\n",
    "# confidence_percent = confidence.item() * 100\n",
    "# true_class = classes[true_label.item()]\n",
    "#\n",
    "# print(f'Predicted: \"{predicted_class}\" with {confidence_percent:.2f}% confidence.')\n",
    "# print(f'True label: \"{true_class}\"')\n",
    "\n",
    "# --- Конец Примера ---\n",
    "\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Введение в Архитектуры Классификации в torchvision\n",
    "\n",
    "# `torchvision.models` предоставляет доступ к множеству популярных\n",
    "# архитектур компьютерного зрения, включая предобученные на ImageNet модели.\n",
    "# Использование предобученных моделей (Transfer Learning) - это очень\n",
    "# эффективный подход, особенно когда ваш собственный датасет не очень большой.\n",
    "\n",
    "# Основные идеи Transfer Learning:\n",
    "# 1. Feature Extraction (Извлечение признаков):\n",
    "#    - Используем предобученную модель как фиксированный извлекатель признаков.\n",
    "#    - Замораживаем веса всех слоев, кроме последнего (классификатора).\n",
    "#    - Обучаем только новый классификатор на своих данных.\n",
    "#    - Подходит, когда ваш датасет мал или очень похож на ImageNet.\n",
    "# 2. Fine-tuning (Дообучение):\n",
    "#    - Инициализируем модель предобученными весами.\n",
    "#    - Заменяем классификатор на новый.\n",
    "#    - Размораживаем часть или все слои \"тела\" модели (backbone).\n",
    "#    - Обучаем всю модель (или ее часть) на своих данных, обычно с маленькой скоростью обучения (learning rate).\n",
    "#    - Подходит, когда ваш датасет больше или отличается от ImageNet.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "# Предполагается, что у вас есть DataLoader'ы: train_loader, val_loader\n",
    "# и определено количество классов: num_classes\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: AlexNet\n",
    "\n",
    "# Краткое описание:\n",
    "# Одна из первых глубоких сетей, выигравшая ImageNet LSVRC 2012.\n",
    "# Показала эффективность CNN для сложных задач CV.\n",
    "# Состоит из 5 сверточных и 3 полносвязных слоев. Использует ReLU, MaxPool, Dropout.\n",
    "# Сейчас используется реже, но имеет историческое значение.\n",
    "\n",
    "# Загрузка модели:\n",
    "# Загрузить предобученную на ImageNet\n",
    "alexnet_pretrained = models.alexnet(pretrained=True)\n",
    "# Загрузить только архитектуру (случайные веса)\n",
    "alexnet_scratch = models.alexnet(pretrained=False)\n",
    "\n",
    "# Модификация для своих данных (Transfer Learning):\n",
    "# AlexNet имеет блок 'classifier', последний слой - 6-й (индекс).\n",
    "num_classes = 10 # Пример: для CIFAR-10\n",
    "num_ftrs_alexnet = alexnet_pretrained.classifier[6].in_features\n",
    "# Заменяем последний слой\n",
    "alexnet_pretrained.classifier[6] = nn.Linear(num_ftrs_alexnet, num_classes)\n",
    "\n",
    "# То же самое для модели с нуля\n",
    "num_ftrs_scratch_alexnet = alexnet_scratch.classifier[6].in_features\n",
    "alexnet_scratch.classifier[6] = nn.Linear(num_ftrs_scratch_alexnet, num_classes)\n",
    "\n",
    "# Пример дообучения (Fine-tuning Setup - Концептуально):\n",
    "# model_to_train = alexnet_pretrained.to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# # Оптимизируем только параметры нового классификатора (Feature Extraction)\n",
    "# # optimizer = optim.SGD(model_to_train.classifier[6].parameters(), lr=0.001, momentum=0.9)\n",
    "# # Или оптимизируем все параметры (Full Fine-tuning, с малым lr)\n",
    "# optimizer = optim.SGD(model_to_train.parameters(), lr=1e-4, momentum=0.9)\n",
    "# # Далее следует стандартный цикл обучения PyTorch...\n",
    "# # for epoch in range(num_epochs):\n",
    "# #     model_to_train.train()\n",
    "# #     for inputs, labels in train_loader:\n",
    "# #         inputs, labels = inputs.to(device), labels.to(device)\n",
    "# #         optimizer.zero_grad()\n",
    "# #         outputs = model_to_train(inputs)\n",
    "# #         loss = criterion(outputs, labels)\n",
    "# #         loss.backward()\n",
    "# #         optimizer.step()\n",
    "# #     # Валидация с model_to_train.eval() и torch.no_grad()...\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: VGG (Visual Geometry Group)\n",
    "\n",
    "# Краткое описание:\n",
    "# Углубили AlexNet, используя очень маленькие сверточные фильтры (3x3).\n",
    "# Показали, что глубина сети важна для производительности.\n",
    "# Простая и однородная архитектура (стеки сверток 3x3 + MaxPool).\n",
    "# Популярные варианты: VGG16, VGG19 (число означает количество весовых слоев).\n",
    "# Достаточно \"тяжелая\" по количеству параметров.\n",
    "\n",
    "# Загрузка модели:\n",
    "# Загрузить предобученную VGG16\n",
    "vgg16_pretrained = models.vgg16(pretrained=True)\n",
    "# Загрузить только архитектуру VGG16\n",
    "vgg16_scratch = models.vgg16(pretrained=False)\n",
    "\n",
    "# Модификация для своих данных:\n",
    "# VGG также имеет блок 'classifier', последний слой - 6-й.\n",
    "num_classes = 100 # Пример: для CIFAR-100\n",
    "num_ftrs_vgg = vgg16_pretrained.classifier[6].in_features\n",
    "vgg16_pretrained.classifier[6] = nn.Linear(num_ftrs_vgg, num_classes)\n",
    "\n",
    "num_ftrs_scratch_vgg = vgg16_scratch.classifier[6].in_features\n",
    "vgg16_scratch.classifier[6] = nn.Linear(num_ftrs_scratch_vgg, num_classes)\n",
    "\n",
    "# Пример дообучения (Fine-tuning Setup - Концептуально):\n",
    "# model_to_train = vgg16_pretrained.to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# # Оптимизация только классификатора\n",
    "# # optimizer = optim.Adam(model_to_train.classifier[6].parameters(), lr=0.001)\n",
    "# # Оптимизация всей сети (с малым lr)\n",
    "# optimizer = optim.Adam(model_to_train.parameters(), lr=1e-5)\n",
    "# # Далее стандартный цикл обучения...\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: ResNet (Residual Network)\n",
    "\n",
    "# Краткое описание:\n",
    "# Революционная архитектура, позволившая эффективно обучать очень глубокие сети (до 152 слоев и больше).\n",
    "# Ввели \"остаточные блоки\" (residual blocks) с skip connections (проброс входа на выход блока).\n",
    "# Это решает проблему затухания градиента в глубоких сетях.\n",
    "# Очень популярный и сильный выбор для многих задач CV.\n",
    "# Варианты: ResNet18, ResNet34, ResNet50, ResNet101, ResNet152.\n",
    "\n",
    "# Загрузка модели:\n",
    "# Загрузить предобученную ResNet50\n",
    "resnet50_pretrained = models.resnet50(pretrained=True)\n",
    "# Загрузить только архитектуру ResNet50\n",
    "resnet50_scratch = models.resnet50(pretrained=False)\n",
    "\n",
    "# Модификация для своих данных:\n",
    "# У ResNet последний слой называется 'fc' (fully connected).\n",
    "num_classes = 200 # Пример: для CUB-200 Birds\n",
    "num_ftrs_resnet = resnet50_pretrained.fc.in_features\n",
    "resnet50_pretrained.fc = nn.Linear(num_ftrs_resnet, num_classes)\n",
    "\n",
    "num_ftrs_scratch_resnet = resnet50_scratch.fc.in_features\n",
    "resnet50_scratch.fc = nn.Linear(num_ftrs_scratch_resnet, num_classes)\n",
    "\n",
    "# Пример дообучения (Fine-tuning Setup - Концептуально):\n",
    "# model_to_train = resnet50_pretrained.to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "#\n",
    "# # --- Стратегия 1: Feature Extraction ---\n",
    "# # Заморозить все слои, кроме последнего\n",
    "# # for param in model_to_train.parameters():\n",
    "# #     param.requires_grad = False\n",
    "# # # Убедиться, что параметры нового слоя разморожены (они такие по умолчанию)\n",
    "# # model_to_train.fc.requires_grad = True\n",
    "# # # Оптимизировать только параметры fc слоя\n",
    "# # optimizer = optim.Adam(model_to_train.fc.parameters(), lr=0.001)\n",
    "#\n",
    "# # --- Стратегия 2: Full Fine-tuning ---\n",
    "# # Разморозить все слои (они разморожены по умолчанию после замены fc)\n",
    "# # Оптимизировать все параметры, но с очень маленьким lr для backbone\n",
    "# optimizer = optim.SGD(model_to_train.parameters(), lr=1e-4, momentum=0.9)\n",
    "# # Можно задать разные lr для backbone и head (см. документацию optim)\n",
    "#\n",
    "# # Далее стандартный цикл обучения...\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: GoogLeNet / Inception\n",
    "\n",
    "# Краткое описание:\n",
    "# Ввели \"Inception module\", который выполняет свертки с разными размерами ядер (1x1, 3x3, 5x5) и MaxPool параллельно,\n",
    "# а затем конкатенирует результаты. Это позволяет сети выбирать наиболее подходящие признаки на разных масштабах.\n",
    "# Более эффективна по вычислениям, чем VGG.\n",
    "# Использовала вспомогательные классификаторы во время обучения (в `torchvision` они активны только при обучении).\n",
    "# `torchvision` предоставляет `googlenet` (Inception v1) и `inception_v3`.\n",
    "\n",
    "# Загрузка модели:\n",
    "# Загрузить предобученную GoogLeNet\n",
    "googlenet_pretrained = models.googlenet(pretrained=True)\n",
    "# Загрузить только архитектуру GoogLeNet\n",
    "googlenet_scratch = models.googlenet(pretrained=False)\n",
    "\n",
    "# Модификация для своих данных:\n",
    "# У GoogLeNet последний слой также называется 'fc'.\n",
    "num_classes = 50 # Пример\n",
    "num_ftrs_googlenet = googlenet_pretrained.fc.in_features\n",
    "googlenet_pretrained.fc = nn.Linear(num_ftrs_googlenet, num_classes)\n",
    "# Важно: Если используете pretrained=True, вспомогательные классификаторы (aux1, aux2)\n",
    "# тоже нужно модифицировать или отключить, если они есть в используемой версии.\n",
    "# В стандартной реализации torchvision они обычно не мешают при inference (model.eval()).\n",
    "# При обучении с нуля (pretrained=False) их тоже нужно адаптировать.\n",
    "# googlenet_pretrained.aux1.fc2 = nn.Linear(googlenet_pretrained.aux1.fc2.in_features, num_classes)\n",
    "# googlenet_pretrained.aux2.fc2 = nn.Linear(googlenet_pretrained.aux2.fc2.in_features, num_classes)\n",
    "\n",
    "\n",
    "num_ftrs_scratch_googlenet = googlenet_scratch.fc.in_features\n",
    "googlenet_scratch.fc = nn.Linear(num_ftrs_scratch_googlenet, num_classes)\n",
    "# googlenet_scratch.aux1.fc2 = nn.Linear(googlenet_scratch.aux1.fc2.in_features, num_classes)\n",
    "# googlenet_scratch.aux2.fc2 = nn.Linear(googlenet_scratch.aux2.fc2.in_features, num_classes)\n",
    "\n",
    "\n",
    "# Пример дообучения (Fine-tuning Setup - Концептуально):\n",
    "# model_to_train = googlenet_pretrained.to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model_to_train.parameters(), lr=1e-4)\n",
    "# # При обучении GoogLeNet часто считают суммарные потери от основного и вспомогательных выходов\n",
    "# # В цикле обучения:\n",
    "# # if model_to_train.training: # Если в режиме обучения\n",
    "# #     outputs, aux1_outputs, aux2_outputs = model_to_train(inputs)\n",
    "# #     loss1 = criterion(outputs, labels)\n",
    "# #     loss2 = criterion(aux1_outputs, labels)\n",
    "# #     loss3 = criterion(aux2_outputs, labels)\n",
    "# #     loss = loss1 + 0.3 * loss2 + 0.3 * loss3 # Взвешенная сумма\n",
    "# # else: # В режиме model.eval()\n",
    "# #     outputs = model_to_train(inputs)\n",
    "# #     loss = criterion(outputs, labels)\n",
    "# # loss.backward()\n",
    "# # optimizer.step()\n",
    "# # Далее стандартный цикл обучения...\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: DenseNet (Densely Connected Convolutional Network)\n",
    "\n",
    "# Краткое описание:\n",
    "# Каждый слой получает на вход карты признаков всех предыдущих слоев (внутри одного \"dense block\").\n",
    "# Это способствует лучшему потоку градиентов и повторному использованию признаков.\n",
    "# Очень эффективна по параметрам (меньше параметров, чем ResNet при схожей точности).\n",
    "# Варианты: DenseNet121, DenseNet169, DenseNet201, DenseNet161.\n",
    "\n",
    "# Загрузка модели:\n",
    "# Загрузить предобученную DenseNet121\n",
    "densenet121_pretrained = models.densenet121(pretrained=True)\n",
    "# Загрузить только архитектуру DenseNet121\n",
    "densenet121_scratch = models.densenet121(pretrained=False)\n",
    "\n",
    "# Модификация для своих данных:\n",
    "# У DenseNet последний слой называется 'classifier'.\n",
    "num_classes = 102 # Пример: Oxford Flowers 102\n",
    "num_ftrs_densenet = densenet121_pretrained.classifier.in_features\n",
    "densenet121_pretrained.classifier = nn.Linear(num_ftrs_densenet, num_classes)\n",
    "\n",
    "num_ftrs_scratch_densenet = densenet121_scratch.classifier.in_features\n",
    "densenet121_scratch.classifier = nn.Linear(num_ftrs_scratch_densenet, num_classes)\n",
    "\n",
    "# Пример дообучения (Fine-tuning Setup - Концептуально):\n",
    "# model_to_train = densenet121_pretrained.to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# # Оптимизация только классификатора\n",
    "# # optimizer = optim.Adam(model_to_train.classifier.parameters(), lr=0.001)\n",
    "# # Оптимизация всей сети (с малым lr)\n",
    "# optimizer = optim.Adam(model_to_train.parameters(), lr=1e-4)\n",
    "# # Далее стандартный цикл обучения...\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 7: MobileNet\n",
    "\n",
    "# Краткое описание:\n",
    "# Семейство легких и быстрых архитектур, оптимизированных для мобильных устройств и встраиваемых систем.\n",
    "# Используют \"depthwise separable convolutions\" для значительного снижения количества вычислений и параметров.\n",
    "# Версии: MobileNetV2, MobileNetV3 (Small/Large).\n",
    "\n",
    "# Загрузка модели:\n",
    "# Загрузить предобученную MobileNetV2\n",
    "mobilenet_v2_pretrained = models.mobilenet_v2(pretrained=True)\n",
    "# Загрузить только архитектуру MobileNetV2\n",
    "mobilenet_v2_scratch = models.mobilenet_v2(pretrained=False)\n",
    "# Аналогично для MobileNetV3\n",
    "# mobilenet_v3_large_pretrained = models.mobilenet_v3_large(pretrained=True)\n",
    "\n",
    "# Модификация для своих данных:\n",
    "# У MobileNetV2/V3 последний блок называется 'classifier'. Обычно это Sequential с Dropout и Linear.\n",
    "num_classes = 2 # Пример: Бинарная классификация (cat/dog)\n",
    "# Для MobileNetV2\n",
    "num_ftrs_mobilenet_v2 = mobilenet_v2_pretrained.classifier[1].in_features\n",
    "mobilenet_v2_pretrained.classifier[1] = nn.Linear(num_ftrs_mobilenet_v2, num_classes)\n",
    "# Для MobileNetV3 Large (структура classifier может немного отличаться)\n",
    "# num_ftrs_mobilenet_v3 = mobilenet_v3_large_pretrained.classifier[3].in_features\n",
    "# mobilenet_v3_large_pretrained.classifier[3] = nn.Linear(num_ftrs_mobilenet_v3, num_classes)\n",
    "\n",
    "# То же для scratch моделей\n",
    "num_ftrs_scratch_mobilenet_v2 = mobilenet_v2_scratch.classifier[1].in_features\n",
    "mobilenet_v2_scratch.classifier[1] = nn.Linear(num_ftrs_scratch_mobilenet_v2, num_classes)\n",
    "\n",
    "# Пример дообучения (Fine-tuning Setup - Концептуально):\n",
    "# model_to_train = mobilenet_v2_pretrained.to(device)\n",
    "# criterion = nn.CrossEntropyLoss() # или nn.BCEWithLogitsLoss для бинарной\n",
    "# optimizer = optim.Adam(model_to_train.parameters(), lr=1e-4)\n",
    "# # Далее стандартный цикл обучения...\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 8: EfficientNet\n",
    "\n",
    "# Краткое описание:\n",
    "# Семейство моделей, достигающих state-of-the-art точности при значительно меньшем количестве параметров и FLOPS.\n",
    "# Используют метод \"compound scaling\", который оптимально масштабирует глубину, ширину и разрешение сети одновременно.\n",
    "# Начинается с базовой сети EfficientNet-B0 и масштабируется до B7.\n",
    "# `torchvision` предоставляет B0-B7.\n",
    "\n",
    "# Загрузка модели:\n",
    "# Загрузить предобученную EfficientNet-B0\n",
    "efficientnet_b0_pretrained = models.efficientnet_b0(pretrained=True)\n",
    "# Загрузить только архитектуру EfficientNet-B0\n",
    "efficientnet_b0_scratch = models.efficientnet_b0(pretrained=False)\n",
    "\n",
    "# Модификация для своих данных:\n",
    "# У EfficientNet последний блок называется 'classifier', содержащий Dropout и Linear слой.\n",
    "num_classes = 1000 # Пример: ImageNet\n",
    "num_ftrs_efficientnet = efficientnet_b0_pretrained.classifier[1].in_features\n",
    "efficientnet_b0_pretrained.classifier[1] = nn.Linear(num_ftrs_efficientnet, num_classes)\n",
    "\n",
    "num_ftrs_scratch_efficientnet = efficientnet_b0_scratch.classifier[1].in_features\n",
    "efficientnet_b0_scratch.classifier[1] = nn.Linear(num_ftrs_scratch_efficientnet, num_classes)\n",
    "\n",
    "# Пример дообучения (Fine-tuning Setup - Концептуально):\n",
    "# model_to_train = efficientnet_b0_pretrained.to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model_to_train.parameters(), lr=1e-4) # EfficientNet часто обучают с RMSprop или AdamW\n",
    "# # Далее стандартный цикл обучения...\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 9: Vision Transformer (ViT)\n",
    "\n",
    "# Краткое описание:\n",
    "# Адаптация архитектуры Transformer (изначально из NLP) для задач CV.\n",
    "# Делит изображение на патчи (patches), линейно их встраивает (embeds), добавляет позиционные эмбеддинги\n",
    "# и подает последовательность патчей в стандартный Transformer Encoder.\n",
    "# Для классификации используется специальный токен [CLS] или усреднение выходов.\n",
    "# Требует больших датасетов для обучения с нуля, но хорошо работает с transfer learning.\n",
    "# `torchvision` предоставляет несколько вариантов ViT.\n",
    "\n",
    "# Загрузка модели:\n",
    "# Загрузить предобученную ViT B/16 (Base модель, размер патча 16x16)\n",
    "vit_b16_pretrained = models.vit_b_16(pretrained=True)\n",
    "# Загрузить только архитектуру ViT B/16\n",
    "vit_b16_scratch = models.vit_b_16(pretrained=False)\n",
    "\n",
    "# Модификация для своих данных:\n",
    "# У ViT классификационная голова называется 'heads'. Обычно это один Linear слой.\n",
    "num_classes = 37 # Пример: Oxford-IIIT Pet Dataset\n",
    "num_ftrs_vit = vit_b16_pretrained.heads.head.in_features # Доступ к слою 'head' внутри 'heads'\n",
    "vit_b16_pretrained.heads.head = nn.Linear(num_ftrs_vit, num_classes)\n",
    "\n",
    "num_ftrs_scratch_vit = vit_b16_scratch.heads.head.in_features\n",
    "vit_b16_scratch.heads.head = nn.Linear(num_ftrs_scratch_vit, num_classes)\n",
    "\n",
    "# Пример дообучения (Fine-tuning Setup - Концептуально):\n",
    "# model_to_train = vit_b16_pretrained.to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# # ViT часто требует специфических настроек оптимизатора (AdamW) и расписания lr\n",
    "# optimizer = optim.AdamW(model_to_train.parameters(), lr=1e-5, weight_decay=0.01)\n",
    "# # Далее стандартный цикл обучения...\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Общие Замечания по Дообучению:\n",
    "# 1. Предобработка данных: Убедитесь, что ваши данные предобработаны так же,\n",
    "#    как данные, на которых обучалась предобученная модель (особенно нормализация).\n",
    "#    `torchvision` обычно предоставляет нужные параметры `mean` и `std`.\n",
    "# 2. Learning Rate: При fine-tuning всей сети используйте значительно меньший learning rate\n",
    "#    (например, 1e-4, 1e-5), чем при обучении с нуля, чтобы не разрушить предобученные веса.\n",
    "# 3. Оптимизатор: Adam, AdamW, SGD с моментумом - частый выбор. Иногда RMSprop.\n",
    "# 4. Заморозка слоев: Для feature extraction заморозьте параметры с помощью `param.requires_grad = False`.\n",
    "#    Не забудьте передать в оптимизатор только размороженные параметры.\n",
    "# 5. Разморозка слоев: При full fine-tuning можно постепенно размораживать слои,\n",
    "#    начиная с верхних (ближе к выходу) и двигаясь к нижним, с разными learning rates.\n",
    "# 6. `model.train()` и `model.eval()`: Не забывайте переключать режимы модели.\n",
    "#    `eval()` важен для отключения Dropout и использования статистики Batch Normalization,\n",
    "#    накопленной во время обучения.\n",
    "\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Общая Подготовка и Контекст\n",
    "\n",
    "# Импорты\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, TensorDataset # TensorDataset для примера\n",
    "import time\n",
    "import copy # Для копирования состояния модели\n",
    "import os # Для создания директорий\n",
    "\n",
    "# Определение устройства (GPU или CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Параметры (Примеры)\n",
    "num_classes = 10 # Количество классов в вашем датасете (например, CIFAR-10)\n",
    "batch_size = 32\n",
    "learning_rate_fe = 0.001 # Learning rate для Feature Extraction\n",
    "learning_rate_ft = 0.0001 # Learning rate для Fine-tuning (обычно ниже)\n",
    "num_epochs = 5 # Количество эпох для обучения (в реальной задаче нужно больше)\n",
    "# Путь для сохранения моделей\n",
    "model_save_dir = \"./saved_models\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# --- Создание Фиктивных Данных для Демонстрации ---\n",
    "# В реальной задаче здесь будут ваши DataLoader'ы с реальными данными\n",
    "# Используем случайные тензоры как имитацию изображений и меток\n",
    "# ResNet обычно ожидает вход 224x224\n",
    "print(\"Creating dummy data...\")\n",
    "dummy_data = torch.randn(batch_size * 10, 3, 224, 224) # Увеличим немного размер\n",
    "dummy_labels = torch.randint(0, num_classes, (batch_size * 10,))\n",
    "dummy_dataset = TensorDataset(dummy_data, dummy_labels)\n",
    "# Разделим на train/val условно\n",
    "train_size = int(0.8 * len(dummy_dataset))\n",
    "val_size = len(dummy_dataset) - train_size\n",
    "# Используем генератор для воспроизводимости разделения\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "dummy_train_dataset, dummy_val_dataset = torch.utils.data.random_split(dummy_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "dummy_train_loader = DataLoader(dummy_train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "dummy_val_loader = DataLoader(dummy_val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Словарь с загрузчиками данных\n",
    "dataloaders = {'train': dummy_train_loader, 'val': dummy_val_loader}\n",
    "dataset_sizes = {'train': len(dummy_train_dataset), 'val': len(dummy_val_dataset)}\n",
    "print(\"Dummy data created.\")\n",
    "\n",
    "# Функция для обучения модели (общая для обоих подходов)\n",
    "# Она будет вызываться с разной конфигурацией модели и оптимизатора\n",
    "def train_model(model, criterion, optimizer, scheduler=None, num_epochs=5, model_name=\"model\"):\n",
    "    since = time.time()\n",
    "    # Сохраняем лучшие веса модели\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    print(f\"\\nStarting training for {model_name}...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Каждая эпоха имеет фазу обучения и валидации\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Установить модель в режим обучения\n",
    "            else:\n",
    "                model.eval()   # Установить модель в режим оценки\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Итерация по данным\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Обнулить градиенты параметра\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Прямой проход\n",
    "                # Отслеживать историю только в режиме обучения\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Обратный проход + оптимизация только в фазе обучения\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Статистика\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            history[f'{phase}_acc'].append(epoch_acc.item()) # Сохраняем как float\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Глубокое копирование модели, если достигнута лучшая точность на валидации\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                # Сохраняем лучшую модель\n",
    "                torch.save(model.state_dict(), os.path.join(model_save_dir, f'{model_name}_best.pth'))\n",
    "                print(f\"Saved best model weights to {os.path.join(model_save_dir, f'{model_name}_best.pth')}\")\n",
    "\n",
    "\n",
    "        # Шаг планировщика LR (если используется)\n",
    "        if phase == 'train' and scheduler:\n",
    "             scheduler.step() # Некоторые планировщики требуют loss на шаге (ReduceLROnPlateau)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # Загрузить лучшие веса модели\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Feature Extraction (Извлечение Признаков)\n",
    "\n",
    "# Концепция:\n",
    "# 1. Загружаем предобученную модель ResNet (например, ResNet18 или ResNet50).\n",
    "# 2. \"Замораживаем\" веса всех слоев модели, кроме последнего полносвязного слоя (классификатора).\n",
    "#    Это означает, что во время обучения будут обновляться только веса этого последнего слоя.\n",
    "#    Сверточные слои используются как фиксированный экстрактор признаков.\n",
    "# 3. Заменяем последний слой на новый, соответствующий количеству классов в нашем датасете.\n",
    "# 4. Обучаем модель. Обновляются только веса нового классификатора.\n",
    "\n",
    "print(\"\\n--- Feature Extraction ---\")\n",
    "\n",
    "# 1. Загрузка предобученной модели ResNet18\n",
    "model_fe = models.resnet18(pretrained=True)\n",
    "\n",
    "# 2. Заморозка весов всех слоев\n",
    "for param in model_fe.parameters():\n",
    "    param.requires_grad = False # Отключаем расчет градиентов для всех существующих параметров\n",
    "\n",
    "# 3. Замена последнего слоя (fc - fully connected)\n",
    "num_ftrs_fe = model_fe.fc.in_features # Получаем количество входов последнего слоя\n",
    "# Создаем новый слой. По умолчанию requires_grad=True для новых слоев.\n",
    "model_fe.fc = nn.Linear(num_ftrs_fe, num_classes)\n",
    "\n",
    "# Перемещаем модель на нужное устройство (GPU/CPU)\n",
    "model_fe = model_fe.to(device)\n",
    "\n",
    "# Убедимся, что только параметры последнего слоя будут обновляться\n",
    "print(\"Params to learn in Feature Extraction:\")\n",
    "params_to_update_fe = []\n",
    "for name, param in model_fe.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update_fe.append(param)\n",
    "        print(\"\\t\", name)\n",
    "\n",
    "# 4. Определение функции потерь и оптимизатора\n",
    "criterion_fe = nn.CrossEntropyLoss()\n",
    "# В оптимизатор передаем ТОЛЬКО параметры, которые нужно обучать (параметры нового fc слоя)\n",
    "optimizer_fe = optim.Adam(params_to_update_fe, lr=learning_rate_fe)\n",
    "\n",
    "# Запуск обучения (используем общую функцию train_model)\n",
    "# Раскомментируйте для запуска обучения\n",
    "model_fe_trained, history_fe = train_model(model_fe, criterion_fe, optimizer_fe,\n",
    "                                            num_epochs=num_epochs, model_name=\"resnet18_fe\")\n",
    "print(\"Feature Extraction Training Finished.\")\n",
    "\n",
    "# После обучения модель model_fe_trained готова к использованию\n",
    "# или может служить инициализацией для Fine-tuning.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Fine-tuning (Дообучение)\n",
    "\n",
    "# Концепция:\n",
    "# 1. Загружаем предобученную модель ResNet.\n",
    "# 2. Заменяем последний слой на новый, соответствующий нашему количеству классов.\n",
    "# 3. \"Размораживаем\" все слои модели (или часть из них, например, несколько последних блоков ResNet).\n",
    "#    Это означает, что градиенты будут рассчитываться для всех (или выбранных) параметров.\n",
    "# 4. Обучаем всю модель на новом датасете, но с очень маленькой скоростью обучения (learning rate).\n",
    "#    Это позволяет \"тонко настроить\" предобученные веса под специфику новых данных,\n",
    "#    не разрушая при этом полезные признаки, изученные на ImageNet.\n",
    "\n",
    "print(\"\\n--- Fine-tuning ---\")\n",
    "\n",
    "# 1. Загрузка предобученной модели ResNet18\n",
    "# Можно начать с нуля или взять модель после Feature Extraction\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "# Если хотите начать с модели после FE:\n",
    "# model_ft = model_fe_trained # Загрузить уже обученную модель FE\n",
    "# Или загрузить сохраненные веса FE:\n",
    "# model_ft = models.resnet18(pretrained=False) # Загрузить архитектуру\n",
    "# num_ftrs_ft_load = model_ft.fc.in_features\n",
    "# model_ft.fc = nn.Linear(num_ftrs_ft_load, num_classes) # Адаптировать fc слой\n",
    "# fe_weights_path = os.path.join(model_save_dir, 'resnet18_fe_best.pth')\n",
    "# if os.path.exists(fe_weights_path):\n",
    "#     model_ft.load_state_dict(torch.load(fe_weights_path))\n",
    "#     print(f\"Loaded weights from {fe_weights_path}\")\n",
    "# else:\n",
    "#     print(\"FE weights not found, starting fine-tuning from ImageNet weights.\")\n",
    "#     model_ft = models.resnet18(pretrained=True) # Начать с ImageNet, если FE веса не найдены\n",
    "\n",
    "# 2. Замена последнего слоя (если не загрузили модель после FE)\n",
    "num_ftrs_ft = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Linear(num_ftrs_ft, num_classes)\n",
    "\n",
    "# 3. Разморозка всех слоев (по умолчанию они разморожены, если не замораживали ранее)\n",
    "# Убедимся, что requires_grad = True для всех параметров\n",
    "for param in model_ft.parameters():\n",
    "     param.requires_grad = True\n",
    "\n",
    "# Перемещаем модель на устройство\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Выведем параметры для обучения (должны быть все параметры модели)\n",
    "print(\"Params to learn in Fine-tuning:\")\n",
    "params_to_update_ft = []\n",
    "total_params = 0\n",
    "for name, param in model_ft.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update_ft.append(param)\n",
    "        total_params += param.numel() # Считаем общее количество обучаемых параметров\n",
    "        # print(\"\\t\", name) # Раскомментируйте, если хотите увидеть все параметры\n",
    "print(f\"\\tTotal trainable parameters: {total_params}\")\n",
    "\n",
    "\n",
    "# 4. Определение функции потерь и оптимизатора\n",
    "criterion_ft = nn.CrossEntropyLoss()\n",
    "# В оптимизатор передаем ВСЕ параметры модели, но с МАЛЕНЬКИМ learning rate\n",
    "optimizer_ft = optim.Adam(params_to_update_ft, lr=learning_rate_ft) # Используем lr для fine-tuning\n",
    "# Часто используют SGD с моментумом для fine-tuning\n",
    "# optimizer_ft = optim.SGD(params_to_update_ft, lr=learning_rate_ft, momentum=0.9)\n",
    "\n",
    "# (Опционально) Добавление планировщика скорости обучения (Learning Rate Scheduler)\n",
    "# Уменьшает LR во время обучения, что часто улучшает сходимость\n",
    "from torch.optim import lr_scheduler\n",
    "# Уменьшать LR в gamma раз каждые step_size эпох\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)\n",
    "\n",
    "# Запуск обучения\n",
    "# Раскомментируйте для запуска обучения\n",
    "# model_ft_trained, history_ft = train_model(model_ft, criterion_ft, optimizer_ft,\n",
    "#                                            scheduler=exp_lr_scheduler, # Передаем планировщик\n",
    "#                                            num_epochs=num_epochs, model_name=\"resnet18_ft\")\n",
    "# print(\"Fine-tuning Training Finished.\")\n",
    "\n",
    "# Модель model_ft_trained готова к использованию.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Важные Замечания (Продолжение)\n",
    "\n",
    "# 2. Выбор между Feature Extraction и Fine-tuning:\n",
    "#    - Feature Extraction:\n",
    "#      - Быстрее обучается, требует меньше вычислительных ресурсов.\n",
    "#      - Хороший выбор, если ваш датасет мал и/или очень похож на ImageNet.\n",
    "#      - Меньше риск переобучения на маленьких датасетах.\n",
    "#    - Fine-tuning:\n",
    "#      - Потенциально может дать более высокую точность, так как адаптирует больше весов.\n",
    "#      - Требует больше времени и ресурсов.\n",
    "#      - Лучше подходит для больших датасетов или датасетов, которые отличаются от ImageNet.\n",
    "#      - Требует аккуратного выбора learning rate, чтобы не испортить предобученные веса.\n",
    "#    - Часто начинают с Feature Extraction, а затем делают Fine-tuning, используя\n",
    "#      веса, полученные после Feature Extraction, как начальные.\n",
    "\n",
    "# 3. Learning Rate Schedulers (Планировщики Скорости Обучения):\n",
    "#    - Уменьшение learning rate во время обучения (learning rate decay) часто помогает\n",
    "#      модели лучше сойтись к хорошему минимуму функции потерь.\n",
    "#    - `torch.optim.lr_scheduler` предоставляет разные стратегии:\n",
    "#      - `StepLR`: Уменьшает LR в `gamma` раз каждые `step_size` эпох.\n",
    "#      - `MultiStepLR`: Уменьшает LR в `gamma` раз на заданных эпохах (`milestones`).\n",
    "#      - `ExponentialLR`: Умножает LR на `gamma` каждую эпоху.\n",
    "#      - `ReduceLROnPlateau`: Уменьшает LR, когда метрика перестает улучшаться (например, val_loss).\n",
    "#    - Планировщик вызывается после шага оптимизатора в каждой эпохе (`scheduler.step()`).\n",
    "\n",
    "# 4. Дифференциальные Learning Rates:\n",
    "#    - При Fine-tuning иногда полезно задавать разные learning rates для разных частей сети.\n",
    "#    - Например, можно установить очень маленький LR для замороженных слоев (backbone)\n",
    "#      и больший LR для нового классификатора (head).\n",
    "#    - Это можно сделать, передав в оптимизатор список словарей с параметрами:\n",
    "#      ```python\n",
    "#      # Пример (не запускается здесь)\n",
    "#      # optimizer = optim.Adam([\n",
    "#      #     {'params': model.conv1.parameters(), 'lr': learning_rate_ft * 0.1}, # Меньший LR для ранних слоев\n",
    "#      #     {'params': model.layer1.parameters(), 'lr': learning_rate_ft * 0.1},\n",
    "#      #     # ... другие слои backbone ...\n",
    "#      #     {'params': model.fc.parameters(), 'lr': learning_rate_ft} # Больший LR для головы\n",
    "#      # ], lr=learning_rate_ft) # lr по умолчанию для параметров не в группах\n",
    "#      ```\n",
    "\n",
    "# 5. Сохранение и Загрузка Моделей:\n",
    "#    - Важно сохранять веса модели во время обучения (особенно лучшие веса на валидации).\n",
    "#    - `torch.save(model.state_dict(), PATH)`: Сохраняет только параметры модели (рекомендуется).\n",
    "#    - `torch.save(model, PATH)`: Сохраняет всю модель (менее гибко).\n",
    "#    - Для загрузки весов:\n",
    "#      ```python\n",
    "#      # model = TheModelClass(*args, **kwargs) # Сначала создайте экземпляр модели\n",
    "#      # model.load_state_dict(torch.load(PATH))\n",
    "#      # model.eval() # Перевести в режим оценки перед использованием\n",
    "#      ```\n",
    "\n",
    "# 6. Оценка Модели:\n",
    "#    - После обучения необходимо оценить финальную модель на отдельной тестовой выборке\n",
    "#      (которая не использовалась ни для обучения, ни для валидации/выбора лучших весов).\n",
    "#    - Используйте `model.eval()` и `with torch.no_grad():` для оценки.\n",
    "#    - Рассчитайте метрики (Accuracy, Precision, Recall, F1-score, Confusion Matrix).\n",
    "\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Проблема Разной Размерности Входных Изображений\n",
    "\n",
    "# Контекст:\n",
    "# Большинство популярных моделей в `torchvision.models` (ResNet, VGG, EfficientNet и т.д.)\n",
    "# были предобучены на датасете ImageNet.\n",
    "# ImageNet содержит изображения, которые обычно приводятся к размеру 224x224 или 299x299 пикселей\n",
    "# перед подачей в модель во время её оригинального обучения.\n",
    "\n",
    "# Проблема:\n",
    "# Ваши реальные данные могут иметь совершенно другие размеры (например, 640x480, 1024x768, или даже разные размеры для разных изображений).\n",
    "# Что произойдет, если подать изображение другого размера в предобученную модель?\n",
    "\n",
    "# Почему возникает ошибка (или неоптимальная работа)?\n",
    "# 1. Сверточные слои (Conv2d): Сами по себе относительно гибки к размеру входа. Они оперируют локально.\n",
    "#    Изменение размера входа изменит размер выходных карт признаков.\n",
    "# 2. Пулинг слои (MaxPool2d, AvgPool2d): Также гибки, уменьшают пространственный размер карт признаков.\n",
    "# 3. Adaptive Pooling (nn.AdaptiveAvgPool2d, nn.AdaptiveMaxPool2d): Ключевой элемент!\n",
    "#    Многие современные архитектуры (ResNet, DenseNet, EfficientNet, MobileNetV2/V3) используют\n",
    "#    адаптивный пулинг перед последним полносвязным слоем.\n",
    "#    Например, `nn.AdaptiveAvgPool2d((1, 1))` возьмет карту признаков ЛЮБОГО пространственного размера (H x W)\n",
    "#    и выдаст карту фиксированного размера (1 x 1), сохраняя глубину каналов.\n",
    "#    Это делает модель более устойчивой к разным размерам входа *до* полносвязного слоя.\n",
    "# 4. Полносвязные слои (Linear): Вот здесь основная проблема!\n",
    "#    Слой `nn.Linear(in_features, out_features)` ожидает на вход вектор строго определенной длины `in_features`.\n",
    "#    Эта длина `in_features` в предобученной модели была рассчитана исходя из ожидаемого размера карт признаков\n",
    "#    после сверточных/пулинг слоев (часто после адаптивного пулинга или flatten), который, в свою очередь,\n",
    "#    зависел от исходного размера входного изображения (например, 224x224).\n",
    "#    Если подать изображение другого размера, то размер вектора перед `nn.Linear` может измениться\n",
    "#    (особенно если нет адаптивного пулинга), что вызовет ошибку несоответствия размерности (size mismatch error).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Решение 1 - Трансформация Входных Изображений (Самый Распространенный и Рекомендуемый)\n",
    "\n",
    "# Идея:\n",
    "# Привести все ваши входные изображения к тому размеру, который ожидала модель при обучении на ImageNet,\n",
    "# используя трансформации из `torchvision.transforms`.\n",
    "\n",
    "# Как реализовать:\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Размер, ожидаемый моделью (например, для ResNet, VGG, MobileNetV2)\n",
    "input_size = 224\n",
    "\n",
    "# Трансформации для Обучения (включают аугментацию и изменение размера)\n",
    "# RandomResizedCrop вырезает случайную часть изображения и изменяет её размер до input_size.\n",
    "# Это хорошая аугментация и способ обработки разных размеров/соотношений сторон.\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(input_size), # Вырезать и изменить размер\n",
    "    transforms.RandomHorizontalFlip(),      # Случайное горизонтальное отражение\n",
    "    transforms.ToTensor(),                  # Конвертировать в PyTorch тензор ([0, 1])\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Нормализация ImageNet\n",
    "])\n",
    "\n",
    "# Трансформации для Валидации/Тестирования/Инференса (без случайных аугментаций)\n",
    "# Обычно сначала изменяют размер меньшей стороны до чуть большего значения (e.g., 256),\n",
    "# а затем вырезают центральную часть нужного размера (e.g., 224).\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),                 # Изменить размер меньшей стороны до 256\n",
    "    transforms.CenterCrop(input_size),      # Вырезать центральный квадрат 224x224\n",
    "    transforms.ToTensor(),                  # Конвертировать в PyTorch тензор\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Нормализация ImageNet\n",
    "])\n",
    "\n",
    "# Применение:\n",
    "# Эти объекты `transforms` передаются в `torchvision.datasets.ImageFolder`\n",
    "# или применяются вручную к вашим изображениям перед подачей в модель.\n",
    "# train_dataset = datasets.ImageFolder('path/to/train', transform=train_transforms)\n",
    "# val_dataset = datasets.ImageFolder('path/to/val', transform=val_test_transforms)\n",
    "# train_loader = DataLoader(train_dataset, ...)\n",
    "# val_loader = DataLoader(val_dataset, ...)\n",
    "\n",
    "# Преимущества:\n",
    "# + Простота: Не требует изменения архитектуры модели.\n",
    "# + Эффективность: Позволяет напрямую использовать предобученные веса ImageNet.\n",
    "# + Стандартный подход: Широко используется и хорошо работает на практике.\n",
    "# + Совместимость: Работает с любой предобученной моделью из torchvision.\n",
    "\n",
    "# Недостатки:\n",
    "# - Потеря информации: Изменение размера может исказить соотношение сторон. Обрезка (cropping) может удалить важные части изображения, если объект находится у края или занимает не центр кадра.\n",
    "# - `RandomResizedCrop` во время обучения помогает модели стать более устойчивой к этому.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Решение 2 - Модификация Архитектуры Модели (Менее Распространено для Классификации)\n",
    "\n",
    "# Идея:\n",
    "# Адаптировать архитектуру модели так, чтобы она могла принимать изображения разного размера.\n",
    "# Это обычно включает замену или модификацию слоев перед полносвязным классификатором.\n",
    "\n",
    "# Вариант А: Использование Моделей с Adaptive Pooling (Большинство современных моделей)\n",
    "# Если модель УЖЕ использует `nn.AdaptiveAvgPool2d((1, 1))` или `nn.AdaptiveMaxPool2d((1, 1))`\n",
    "# перед последним `nn.Linear` слоем (как в ResNet, DenseNet, EfficientNet, MobileNetV2/V3),\n",
    "# то она *уже* может обрабатывать разные пространственные размеры карт признаков *до* этого пулинга.\n",
    "# Проблема несоответствия размера для `nn.Linear` все еще может возникнуть, если\n",
    "# количество каналов перед адаптивным пулингом не совпадает с ожидаемым `in_features`\n",
    "# (хотя обычно это не так при использовании стандартных архитектур).\n",
    "# Главное, что нужно сделать в этом случае - это заменить последний `nn.Linear` слой,\n",
    "# чтобы он соответствовал ВАШЕМУ количеству классов (как мы делали в примерах FE/FT).\n",
    "# Подача изображений другого размера *может* сработать без ошибки, но результаты могут быть\n",
    "# неоптимальными, так как сверточные слои будут извлекать признаки из другого масштаба/разрешения,\n",
    "# чем тот, на котором они обучались.\n",
    "\n",
    "# Вариант Б: Добавление/Замена на Adaptive Pooling (Если его нет)\n",
    "# Если у вас старая модель или кастомная архитектура без адаптивного пулинга,\n",
    "# вы можете вручную заменить последний слой MaxPool/AvgPool + Flatten на `nn.AdaptiveAvgPool2d((1, 1))` + `nn.Flatten(1)`.\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# # Пример (гипотетический, для модели без адаптивного пулинга):\n",
    "# model = models.vgg16(pretrained=True) # VGG использует адаптивный пулинг в `classifier` блоке, но представим что нет\n",
    "# # Представим, что последние слои были:\n",
    "# # model.features -> nn.MaxPool2d -> nn.Flatten -> nn.Linear\n",
    "# # Мы можем заменить их:\n",
    "# model.avgpool = nn.AdaptiveAvgPool2d((7, 7)) # Стандартный для VGG перед классификатором\n",
    "# # Классификатор VGG уже ожидает выход avgpool (7x7), так что здесь замена не нужна,\n",
    "# # но если бы его не было, мы бы добавили AdaptiveAvgPool2d((1, 1))\n",
    "# # и соответствующим образом изменили бы in_features первого Linear слоя.\n",
    "\n",
    "# # Главное, что нужно сделать - заменить классификатор под свои классы:\n",
    "# num_classes = 10 # Ваш пример\n",
    "# num_ftrs_vgg = model.classifier[6].in_features\n",
    "# model.classifier[6] = nn.Linear(num_ftrs_vgg, num_classes)\n",
    "\n",
    "# Преимущества:\n",
    "# + Потенциальное сохранение информации: Не происходит обрезки или искажения исходного изображения.\n",
    "# + Гибкость: Модель может обрабатывать разные размеры на лету.\n",
    "\n",
    "# Недостатки:\n",
    "# - Сложность: Требует модификации архитектуры модели.\n",
    "# - Неоптимальные признаки: Сверточные слои, обученные на признаках из изображений 224x224, могут работать хуже при извлечении признаков из изображений значительно другого размера. Результат не гарантирован.\n",
    "# - Необходимость Fine-tuning: Почти наверняка потребуется дообучение (fine-tuning) модели на ваших данных с новыми размерами, чтобы адаптировать веса.\n",
    "# - Не всегда нужно: Если модель уже имеет адаптивный пулинг, основная проблема решается им, и остается только вопрос оптимальности признаков.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Решение 3 - Патчинг / Скользящее Окно (Редко для Классификации, Чаще для Детекции/Сегментации)\n",
    "\n",
    "# Идея:\n",
    "# Если у вас очень большие изображения и важно сохранить высокое разрешение:\n",
    "# 1. Разделить большое входное изображение на несколько перекрывающихся (или неперекрывающихся) патчей (участков) размера, ожидаемого моделью (например, 224x224).\n",
    "# 2. Прогнать каждый патч через предобученную модель (после замены классификатора).\n",
    "# 3. Агрегировать результаты (например, усреднить предсказания, взять максимум или использовать более сложную логику) для получения финального предсказания для всего изображения.\n",
    "\n",
    "# Преимущества:\n",
    "# + Сохранение деталей: Работает с исходным разрешением по частям.\n",
    "\n",
    "# Недостатки:\n",
    "# - Вычислительно дорого: Модель применяется много раз для одного изображения.\n",
    "# - Сложная реализация: Требует логики нарезки на патчи и агрегации результатов.\n",
    "# - Избыточно для классификации: Обычно для классификации всего изображения достаточно информации из измененного/обрезанного изображения (Решение 1). Этот подход более актуален для задач, где важна локализация (детекция, сегментация).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Вывод и Рекомендации\n",
    "\n",
    "# 1.  **Начните с Решения 1 (Трансформация Входа):** Это самый простой, стандартный и часто наиболее эффективный способ использования предобученных моделей для классификации с данными нестандартного размера. Используйте `transforms.RandomResizedCrop` для обучения и `transforms.Resize` + `transforms.CenterCrop` для валидации/тестирования. Не забудьте про `transforms.Normalize` с параметрами ImageNet.\n",
    "\n",
    "# 2.  **Замените Последний Слой:** Независимо от того, как вы решаете проблему размера, вам *всегда* нужно заменить последний полносвязный слой (`fc` у ResNet, `classifier` у VGG/DenseNet/MobileNet/EfficientNet, `heads.head` у ViT) на новый `nn.Linear`, у которого `out_features` равно количеству классов в вашем датасете.\n",
    "\n",
    "# 3.  **Рассмотрите Fine-tuning:** Даже если вы используете Решение 1, дообучение (fine-tuning) модели на ваших данных (с правильной предобработкой) обычно приводит к лучшим результатам, чем простое использование модели \"как есть\" (даже после замены классификатора).\n",
    "\n",
    "# 4.  **Модификация Архитектуры (Решение 2):** Используйте с осторожностью. Если ваша модель уже имеет адаптивный пулинг, она технически может справиться с разными размерами до FC слоя, но качество признаков может пострадать. Если адаптивного пулинга нет, его добавление может помочь, но все равно потребуется тщательное дообучение.\n",
    "\n",
    "# 5.  **Патчинг (Решение 3):** Обычно избыточен для стандартной задачи классификации изображений.\n",
    "\n",
    "# В подавляющем большинстве случаев для задач классификации с использованием предобученных моделей torchvision, **изменение размера входных изображений с помощью `torchvision.transforms` является предпочтительным методом.**\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Введение в Обнаружение Объектов (Object Detection)\n",
    "\n",
    "# Что такое Обнаружение Объектов?\n",
    "# Это задача Computer Vision, которая заключается не только в классификации объектов на изображении,\n",
    "# но и в определении их точного местоположения с помощью ограничивающих рамок (bounding boxes).\n",
    "# Вход: Изображение.\n",
    "# Выход: Список обнаруженных объектов, где каждый элемент содержит:\n",
    "#   - Координаты ограничивающей рамки (bounding box), обычно [xmin, ymin, xmax, ymax].\n",
    "#   - Метку класса объекта (например, \"человек\", \"автомобиль\", \"собака\").\n",
    "#   - Оценку уверенности (confidence score) для каждого обнаружения.\n",
    "\n",
    "# Цель Обнаружения Объектов:\n",
    "# Научить модель \"видеть\", где находятся объекты разных классов на изображении.\n",
    "\n",
    "# Отличие от Классификации Изображений:\n",
    "# - Классификация: Что изображено на картинке в целом? (1 метка на изображение)\n",
    "# - Обнаружение: Что изображено и где именно? (Много меток + координаты рамок на изображение)\n",
    "\n",
    "# Отличие от Сегментации Изображений:\n",
    "# - Обнаружение: Прямоугольные рамки вокруг объектов.\n",
    "# - Сегментация: Попиксельная маска для каждого объекта (более точная локализация формы).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Ключевые Концепции\n",
    "\n",
    "# 1. Ограничивающая Рамка (Bounding Box):\n",
    "#    - Прямоугольник, описывающий положение объекта.\n",
    "#    - Форматы представления:\n",
    "#      - `[xmin, ymin, xmax, ymax]`: Координаты левого верхнего и правого нижнего углов. (Часто используется в PyTorch/torchvision)\n",
    "#      - `[x_center, y_center, width, height]`: Координаты центра, ширина и высота. (Часто используется в YOLO)\n",
    "#    - Координаты обычно нормализованы (от 0 до 1) или абсолютные (в пикселях). `torchvision` обычно работает с абсолютными пиксельными координатами.\n",
    "\n",
    "# 2. Intersection over Union (IoU):\n",
    "#    - Метрика для оценки того, насколько хорошо предсказанная рамка (`pred_box`) совпадает с истинной рамкой (`gt_box`).\n",
    "#    - Рассчитывается как площадь пересечения рамок, деленная на площадь их объединения.\n",
    "#    - IoU = Area(Intersection) / Area(Union)\n",
    "#    - Значение от 0 (нет пересечения) до 1 (идеальное совпадение).\n",
    "#    - Используется для определения, является ли предсказание \"правильным\" (True Positive) во время оценки (например, если IoU > 0.5).\n",
    "\n",
    "def calculate_iou(boxA, boxB):\n",
    "    # box format: [xmin, ymin, xmax, ymax]\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "    # compute the area of both the prediction and ground-truth rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the intersection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "# 3. Non-Maximum Suppression (NMS):\n",
    "#    - Пост-обработка для удаления избыточных, сильно перекрывающихся рамок для одного и того же объекта.\n",
    "#    - Алгоритм:\n",
    "#      1. Отсортировать все предсказанные рамки по убыванию уверенности (confidence score).\n",
    "#      2. Выбрать рамку с наивысшей уверенностью и добавить ее в финальный список.\n",
    "#      3. Удалить все остальные рамки, которые сильно перекрываются с выбранной (IoU > порога NMS, например, 0.45).\n",
    "#      4. Повторять шаги 2-3, пока не останется рамок.\n",
    "#    - `torchvision.ops.nms()` предоставляет реализацию NMS.\n",
    "\n",
    "# 4. Mean Average Precision (mAP):\n",
    "#    - Стандартная метрика для оценки качества моделей обнаружения объектов.\n",
    "#    - Рассчитывается на основе кривой Precision-Recall для каждого класса.\n",
    "#    - Сначала вычисляется Average Precision (AP) для каждого класса (площадь под кривой Precision-Recall).\n",
    "#    - Затем AP усредняется по всем классам для получения mAP.\n",
    "#    - Часто указывается с порогом IoU (например, mAP@0.5, mAP@0.75, mAP@[0.5:0.95]).\n",
    "\n",
    "# 5. Подходы к Обнаружению:\n",
    "#    - Двухэтапные (Two-stage) детекторы:\n",
    "#      - Сначала генерируют \"предложения\" регионов (Region Proposals), где могут быть объекты (например, с помощью Region Proposal Network - RPN).\n",
    "#      - Затем классифицируют объекты в этих регионах и уточняют их рамки.\n",
    "#      - Примеры: R-CNN, Fast R-CNN, Faster R-CNN, Mask R-CNN.\n",
    "#      - Обычно более точные, но медленнее.\n",
    "#    - Одноэтапные (One-stage) детекторы:\n",
    "#      - Предсказывают классы и координаты рамок напрямую из карт признаков за один проход.\n",
    "#      - Примеры: YOLO (You Only Look Once), SSD (Single Shot MultiBox Detector), RetinaNet, FCOS.\n",
    "#      - Обычно быстрее, но могут быть менее точными (хотя современные версии очень конкурентоспособны).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Модели Обнаружения в `torchvision.models.detection`\n",
    "\n",
    "# `torchvision` предоставляет готовые реализации популярных моделей обнаружения,\n",
    "# включая предобученные на датасете COCO (Common Objects in Context).\n",
    "\n",
    "# 1. Faster R-CNN:\n",
    "#    - Классический и мощный двухэтапный детектор.\n",
    "#    - Использует ResNet (или другие сети) как backbone для извлечения признаков.\n",
    "#    - Включает Region Proposal Network (RPN) для генерации кандидатов.\n",
    "#    - Использует RoIAlign (улучшенный RoIPooling) для извлечения признаков из регионов.\n",
    "#    - Имеет отдельные \"головы\" для классификации и регрессии рамок.\n",
    "#    - Пример загрузки: `models.detection.fasterrcnn_resnet50_fpn(pretrained=True)`\n",
    "#      - `fpn` (Feature Pyramid Network) улучшает обнаружение объектов разного масштаба.\n",
    "\n",
    "# 2. Mask R-CNN:\n",
    "#    - Расширение Faster R-CNN, которое добавляет ветвь для предсказания маски сегментации для каждого объекта.\n",
    "#    - Может использоваться и для обнаружения (просто игнорируя выход маски).\n",
    "#    - Пример загрузки: `models.detection.maskrcnn_resnet50_fpn(pretrained=True)`\n",
    "\n",
    "# 3. RetinaNet:\n",
    "#    - Популярный одноэтапный детектор.\n",
    "#    - Использует Focal Loss для решения проблемы сильного дисбаланса между фоновыми и объектовыми примерами во время обучения.\n",
    "#    - Пример загрузки: `models.detection.retinanet_resnet50_fpn(pretrained=True)`\n",
    "\n",
    "# 4. SSD (Single Shot MultiBox Detector):\n",
    "#    - Еще один эффективный одноэтапный детектор.\n",
    "#    - Делает предсказания на разных уровнях карт признаков для обнаружения объектов разного масштаба.\n",
    "#    - `torchvision` предоставляет SSDlite (облегченная версия) и VGG-based SSD.\n",
    "#    - Пример загрузки: `models.detection.ssd300_vgg16(pretrained=True)`\n",
    "#    - Пример загрузки: `models.detection.ssdlite320_mobilenet_v3_large(pretrained=True)`\n",
    "\n",
    "# 5. FCOS (Fully Convolutional One-Stage Object Detection):\n",
    "#    - Одноэтапный детектор без \"якорей\" (anchor-free). Предсказывает объекты напрямую.\n",
    "#    - Пример загрузки: `models.detection.fcos_resnet50_fpn(pretrained=True)`\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Подготовка Данных для Обнаружения\n",
    "\n",
    "# Это критически важный этап. Модели обнаружения ожидают данные в определенном формате.\n",
    "\n",
    "# Формат Аннотаций (для `torchvision` моделей):\n",
    "# - Каждому изображению должен соответствовать словарь (`target`), содержащий как минимум:\n",
    "#   - `boxes`: тензор типа `torch.float32` формы `[N, 4]`, где `N` - количество объектов на изображении.\n",
    "#              Каждая строка содержит `[xmin, ymin, xmax, ymax]` в абсолютных пиксельных координатах.\n",
    "#   - `labels`: тензор типа `torch.int64` формы `[N]`. Каждое значение - это целочисленный индекс класса для соответствующей рамки.\n",
    "#              **Важно:** Класс 0 зарезервирован для фона (`__background__`). Ваши классы должны начинаться с 1.\n",
    "#              Например, если у вас 2 класса (\"person\", \"car\"), их метки будут 1 и 2.\n",
    "# - Словарь `target` может содержать и другие поля, например, `masks` для Mask R-CNN или `image_id`.\n",
    "\n",
    "# Пользовательский `Dataset`:\n",
    "# - Вам почти всегда нужно будет создать свой класс, наследуемый от `torch.utils.data.Dataset`.\n",
    "# - Метод `__getitem__(self, idx)` должен возвращать:\n",
    "#   - Изображение (обычно как PIL Image или тензор).\n",
    "#   - Словарь `target` с аннотациями в описанном выше формате.\n",
    "# - Метод `__len__(self)` должен возвращать общее количество изображений в датасете.\n",
    "\n",
    "# Пример структуры `__getitem__`:\n",
    "# def __getitem__(self, idx):\n",
    "#     img_path = self.image_files[idx]\n",
    "#     img = Image.open(img_path).convert(\"RGB\")\n",
    "#     # Загрузка аннотаций для этого изображения (например, из XML, JSON файла)\n",
    "#     annotations = self.load_annotations(idx) # Ваша функция загрузки\n",
    "#     boxes = annotations['boxes'] # Получить список [[xmin, ymin, xmax, ymax], ...]\n",
    "#     labels = annotations['labels'] # Получить список [label1, label2, ...]\n",
    "#\n",
    "#     target = {}\n",
    "#     target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "#     target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "#     # target[\"image_id\"] = torch.tensor([idx]) # Полезно для оценки\n",
    "#     # target[\"area\"] = (target[\"boxes\"][:, 3] - target[\"boxes\"][:, 1]) * (target[\"boxes\"][:, 2] - target[\"boxes\"][:, 0])\n",
    "#     # target[\"iscrowd\"] = torch.zeros((len(boxes),), dtype=torch.int64) # Обычно 0 для не-COCO датасетов\n",
    "#\n",
    "#     # Применение трансформаций (если есть)\n",
    "#     if self.transforms is not None:\n",
    "#         # Важно: Трансформации должны применяться и к изображению, и к рамкам!\n",
    "#         # Библиотеки типа Albumentations хорошо подходят для этого.\n",
    "#         # Стандартные torchvision transforms не всегда корректно работают с рамками.\n",
    "#         img, target = self.transforms(img, target) # Пример сигнатуры трансформера\n",
    "#\n",
    "#     return img, target\n",
    "\n",
    "# Трансформации и Аугментация:\n",
    "# - Изменение размера, кроп, отражения и т.д. должны применяться СИНХРОННО к изображению и его рамкам.\n",
    "# - Библиотека `Albumentations` очень популярна для задач обнаружения и сегментации, так как она умеет корректно трансформировать и рамки/маски.\n",
    "# - Стандартные `torchvision.transforms` в основном предназначены для классификации и могут некорректно обрабатывать bounding boxes.\n",
    "\n",
    "# `DataLoader` и `collate_fn`:\n",
    "# - Поскольку каждое изображение может иметь разное количество объектов (и, следовательно, разный размер тензоров `boxes` и `labels` в `target`), стандартный `collate_fn` в `DataLoader` не сработает.\n",
    "# - Нужно предоставить свою функцию `collate_fn`, которая будет принимать список кортежей `(image, target)` из датасета и правильно их батчевать. Обычно изображения собираются в тензор, а таргеты остаются списком словарей.\n",
    "def collate_fn(batch):\n",
    "    # batch - это список кортежей [(img1, target1), (img2, target2), ...]\n",
    "    # Функция list(zip(*batch)) преобразует его в ([img1, img2, ...], [target1, target2, ...])\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# train_dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Использование Предобученных Моделей и Fine-tuning\n",
    "\n",
    "# Загрузка Предобученной Модели:\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Модификация для Своего Числа Классов:\n",
    "# Предобученные модели (на COCO) имеют классификатор, рассчитанный на 91 класс (80 + фон).\n",
    "# Вам нужно заменить \"голову\" классификатора на новую, с нужным количеством выходов.\n",
    "# **Помните про класс фона!** Если у вас `N` классов, нужно `N + 1` выходов.\n",
    "\n",
    "# num_classes = 3 # Например, 2 ваших класса + 1 фон (__background__)\n",
    "#\n",
    "# # Получаем количество входных признаков для классификатора\n",
    "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "#\n",
    "# # Заменяем предобученную голову на новую\n",
    "# model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Загрузка Модели с Другим Backbone (если нужно):\n",
    "# backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# backbone.out_channels = 1280 # Указать количество выходных каналов backbone\n",
    "#\n",
    "# # Настроить генератор якорей (RPN), если стандартные не подходят\n",
    "# anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "#                                    aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "#\n",
    "# # Настроить RoIPooler\n",
    "# roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], # Имена слоев backbone для RoIAlign\n",
    "#                                                 output_size=7,\n",
    "#                                                 sampling_ratio=2)\n",
    "#\n",
    "# # Собрать модель Faster R-CNN\n",
    "# model_custom_backbone = FasterRCNN(backbone,\n",
    "#                                    num_classes=num_classes,\n",
    "#                                    rpn_anchor_generator=anchor_generator,\n",
    "#                                    box_roi_pool=roi_pooler)\n",
    "\n",
    "\n",
    "# Fine-tuning:\n",
    "# - Обычно размораживают все слои или только часть (например, начиная с последних блоков ResNet).\n",
    "# - Используют маленький learning rate.\n",
    "# - Оптимизатор (Adam, SGD) передает параметры модели: `optimizer = optim.SGD(model.parameters(), ...)`\n",
    "# - Планировщик LR часто полезен.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Обучение Модели Обнаружения\n",
    "\n",
    "# Цикл Обучения:\n",
    "# - Значительно отличается от классификации.\n",
    "# - Модели `torchvision.models.detection` ведут себя по-разному в режимах `train()` и `eval()`.\n",
    "#   - `model.train()`: Модель принимает на вход список изображений и список таргетов (`targets`). Возвращает словарь лоссов (`{'loss_classifier': ..., 'loss_box_reg': ..., 'loss_objectness': ..., 'loss_rpn_box_reg': ...}`).\n",
    "#   - `model.eval()`: Модель принимает на вход список изображений. Возвращает список предсказаний (`[{'boxes': ..., 'labels': ..., 'scores': ...}, ...]`) для каждого изображения.\n",
    "\n",
    "# Пример Цикла Обучения (Упрощенный):\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# model.to(device)\n",
    "#\n",
    "# # Параметры для оптимизатора\n",
    "# params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "#\n",
    "# num_epochs = 10\n",
    "#\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train() # Установить режим обучения\n",
    "#     epoch_loss = 0\n",
    "#     for images, targets in train_dataloader: # Загрузчик с collate_fn\n",
    "#         images = list(image.to(device) for image in images)\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "#\n",
    "#         # Прямой проход (в режиме train возвращает лоссы)\n",
    "#         loss_dict = model(images, targets)\n",
    "#         losses = sum(loss for loss in loss_dict.values()) # Суммируем все лоссы\n",
    "#\n",
    "#         # Обратный проход\n",
    "#         optimizer.zero_grad()\n",
    "#         losses.backward()\n",
    "#         optimizer.step()\n",
    "#\n",
    "#         epoch_loss += losses.item()\n",
    "#\n",
    "#     # Обновить планировщик LR\n",
    "#     if lr_scheduler is not None:\n",
    "#         lr_scheduler.step()\n",
    "#\n",
    "#     print(f\"Epoch {epoch}: Loss: {epoch_loss / len(train_dataloader)}\")\n",
    "#\n",
    "#     # --- Фаза Валидации (Оценка) ---\n",
    "#     # Для оценки обычно используют внешние инструменты (например, pycocotools)\n",
    "#     # для расчета mAP на валидационном датасете.\n",
    "#     # Это более сложный процесс, чем просто расчет accuracy.\n",
    "#     # Здесь для простоты опустим детальную оценку mAP.\n",
    "#     # model.eval()\n",
    "#     # with torch.no_grad():\n",
    "#     #     for images, targets in val_dataloader:\n",
    "#     #         images = list(img.to(device) for img in images)\n",
    "#     #         outputs = model(images) # Получаем предсказания\n",
    "#     #         # ... (логика сравнения outputs с targets и расчета mAP) ...\n",
    "\n",
    "# Оценка (Evaluation):\n",
    "# - Требует сравнения предсказанных рамок (`outputs`) с истинными (`targets`) с использованием порога IoU.\n",
    "# - Библиотека `pycocotools` является стандартом для оценки на датасетах формата COCO.\n",
    "# - `torchvision` предоставляет примеры скриптов для обучения и оценки (`references/detection/`).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 7: Инференс (Получение Предсказаний)\n",
    "\n",
    "# - Перевести модель в режим оценки: `model.eval()`.\n",
    "# - Подготовить входное изображение:\n",
    "#   - Применить необходимые трансформации (Resize, ToTensor, Normalize), как при валидации.\n",
    "#   - Добавить измерение батча (даже если изображение одно): `input_tensor = image_tensor.unsqueeze(0)`.\n",
    "#   - Переместить тензор на нужное устройство (`.to(device)`).\n",
    "# - Выполнить предсказание в блоке `with torch.no_grad():`.\n",
    "# - Интерпретировать выход:\n",
    "#   - Модель вернет список словарей (один словарь на изображение в батче).\n",
    "#   - Каждый словарь содержит ключи `'boxes'`, `'labels'`, `'scores'`.\n",
    "#   - Обычно фильтруют предсказания по порогу уверенности (`scores > threshold`).\n",
    "#   - Координаты рамок (`boxes`) могут потребовать конвертации в `int` для отрисовки.\n",
    "\n",
    "# Пример Инференса:\n",
    "# img = Image.open(\"path/to/your/image.jpg\").convert(\"RGB\")\n",
    "# transform = val_test_transforms # Используем трансформации валидации/теста\n",
    "# input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "#\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     predictions = model(input_tensor)\n",
    "#\n",
    "# # predictions - это список с одним элементом (словарем) для нашего одного изображения\n",
    "# pred_boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "# pred_labels = predictions[0]['labels'].cpu().numpy()\n",
    "# pred_scores = predictions[0]['scores'].cpu().numpy()\n",
    "#\n",
    "# # Фильтрация по порогу уверенности\n",
    "# confidence_threshold = 0.5\n",
    "# filtered_indices = pred_scores > confidence_threshold\n",
    "# filtered_boxes = pred_boxes[filtered_indices]\n",
    "# filtered_labels = pred_labels[filtered_indices]\n",
    "# filtered_scores = pred_scores[filtered_indices]\n",
    "#\n",
    "# # Дальнейшая обработка или визуализация filtered_boxes, filtered_labels, filtered_scores\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 8: Пример Задачи и Решения (Полный Код)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Обучить модель Faster R-CNN (с ResNet50-FPN backbone) обнаруживать\n",
    "# два класса объектов: \"Красный квадрат\" (метка 1) и \"Синий круг\" (метка 2)\n",
    "# на синтетических изображениях. Использовать предобученные веса COCO и дообучить модель.\n",
    "\n",
    "# --- Решение (Полный Код) ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# --- 0. Настройки ---\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "NUM_CLASSES = 3 # 2 класса (квадрат, круг) + 1 фон\n",
    "BATCH_SIZE = 2 # Маленький батч для примера\n",
    "NUM_EPOCHS = 5 # Мало эпох для быстрого примера\n",
    "LEARNING_RATE = 0.001\n",
    "IMG_SIZE = 256 # Размер синтетических изображений\n",
    "\n",
    "# --- 1. Создание Синтетического Датасета ---\n",
    "class SyntheticShapesDataset(Dataset):\n",
    "    def __init__(self, num_samples=100, img_size=256, num_classes=3, transform=None):\n",
    "        self.num_samples = num_samples\n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes # Включая фон\n",
    "        self.transform = transform # Пока не используем сложные трансформации\n",
    "        self.images = []\n",
    "        self.targets = []\n",
    "        self._generate_data()\n",
    "\n",
    "    def _generate_data(self):\n",
    "        print(\"Generating synthetic data...\")\n",
    "        for idx in range(self.num_samples):\n",
    "            img = Image.new('RGB', (self.img_size, self.img_size), color='white')\n",
    "            draw = ImageDraw.Draw(img)\n",
    "            boxes = []\n",
    "            labels = []\n",
    "\n",
    "            # Добавим 1-3 фигуры\n",
    "            num_shapes = np.random.randint(1, 4)\n",
    "            for _ in range(num_shapes):\n",
    "                shape_type = np.random.choice(['square', 'circle'])\n",
    "                size = np.random.randint(20, 60)\n",
    "                x = np.random.randint(0, self.img_size - size)\n",
    "                y = np.random.randint(0, self.img_size - size)\n",
    "                xmin, ymin, xmax, ymax = x, y, x + size, y + size\n",
    "\n",
    "                if shape_type == 'square':\n",
    "                    color = 'red'\n",
    "                    label = 1 # Метка для красного квадрата\n",
    "                    draw.rectangle([xmin, ymin, xmax, ymax], fill=color, outline='black')\n",
    "                else: # circle\n",
    "                    color = 'blue'\n",
    "                    label = 2 # Метка для синего круга\n",
    "                    draw.ellipse([xmin, ymin, xmax, ymax], fill=color, outline='black')\n",
    "\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                labels.append(label)\n",
    "\n",
    "            self.images.append(img)\n",
    "            target = {}\n",
    "            target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            target[\"labels\"] = torch.as_tensor(labels, dtype=torch.int64)\n",
    "            target[\"image_id\"] = torch.tensor([idx])\n",
    "            # Добавим area и iscrowd для совместимости с некоторыми функциями оценки\n",
    "            if boxes:\n",
    "                 target[\"area\"] = (target[\"boxes\"][:, 3] - target[\"boxes\"][:, 1]) * (target[\"boxes\"][:, 2] - target[\"boxes\"][:, 0])\n",
    "            else:\n",
    "                 target[\"area\"] = torch.empty((0,), dtype=torch.float32)\n",
    "            target[\"iscrowd\"] = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "            self.targets.append(target)\n",
    "        print(\"Synthetic data generated.\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        # Простое преобразование в тензор\n",
    "        img_tensor = torchvision.transforms.functional.to_tensor(img)\n",
    "\n",
    "        # В реальной задаче здесь были бы сложные трансформации (Albumentations)\n",
    "        # if self.transform:\n",
    "        #     # Применить трансформации к img и target['boxes']\n",
    "        #     pass\n",
    "\n",
    "        return img_tensor, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "# --- 2. Загрузчики Данных ---\n",
    "def collate_fn_shapes(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_dataset = SyntheticShapesDataset(num_samples=80, img_size=IMG_SIZE, num_classes=NUM_CLASSES)\n",
    "val_dataset = SyntheticShapesDataset(num_samples=20, img_size=IMG_SIZE, num_classes=NUM_CLASSES) # Генерируем отдельно для валидации\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          collate_fn=collate_fn_shapes, num_workers=0) # num_workers=0 для Windows/простоты\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        collate_fn=collate_fn_shapes, num_workers=0)\n",
    "\n",
    "# --- 3. Загрузка и Модификация Модели ---\n",
    "def get_model_instance_detection(num_classes):\n",
    "    # Загружаем предобученную модель Faster R-CNN\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "\n",
    "    # Получаем количество входных признаков для классификатора\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # Заменяем предобученную голову на новую\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model_instance_detection(NUM_CLASSES)\n",
    "model.to(device)\n",
    "\n",
    "# --- 4. Оптимизатор и Планировщик ---\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(params, lr=LEARNING_RATE) # Adam может быть проще для старта\n",
    "# optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=0.9, weight_decay=0.0005)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# --- 5. Цикл Обучения ---\n",
    "print(\"\\nStarting Training...\")\n",
    "training_start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    batch_count = 0\n",
    "    for images, targets in train_loader:\n",
    "        batch_start_time = time.time()\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += losses.item()\n",
    "        batch_count += 1\n",
    "        batch_end_time = time.time()\n",
    "        # print(f\"  Batch {batch_count}/{len(train_loader)}, Loss: {losses.item():.4f}, Time: {batch_end_time - batch_start_time:.2f}s\")\n",
    "\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Average Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    # Простая валидация (просто прогоняем и смотрим лосс, без mAP)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "         for images, targets in val_loader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            # В режиме eval модель возвращает предсказания, но если передать таргеты,\n",
    "            # она все равно может посчитать лоссы (удобно для валидации лосса)\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            val_loss += losses.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # if lr_scheduler:\n",
    "    #     lr_scheduler.step()\n",
    "\n",
    "training_end_time = time.time()\n",
    "print(f\"Training finished in {training_end_time - training_start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 6. Инференс и Визуализация ---\n",
    "print(\"\\nRunning Inference on a sample image...\")\n",
    "\n",
    "# Возьмем изображение из валидационного набора\n",
    "img_tensor, target = val_dataset[np.random.randint(len(val_dataset))] # Случайное изображение\n",
    "img_pil = torchvision.transforms.ToPILImage()(img_tensor) # Конвертируем обратно в PIL для отрисовки\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img_tensor.to(device)]) # Подаем как список\n",
    "\n",
    "# Извлекаем предсказания (для первого изображения в батче)\n",
    "pred_boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "pred_labels = prediction[0]['labels'].cpu().numpy()\n",
    "pred_scores = prediction[0]['scores'].cpu().numpy()\n",
    "\n",
    "# Истинные рамки для сравнения\n",
    "true_boxes = target['boxes'].cpu().numpy()\n",
    "true_labels = target['labels'].cpu().numpy()\n",
    "\n",
    "# Функция для отрисовки\n",
    "def plot_image_with_boxes(image, pred_boxes, pred_labels, pred_scores, true_boxes, true_labels, threshold=0.5):\n",
    "    fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Истинные рамки (зеленые)\n",
    "    for box, label in zip(true_boxes, true_labels):\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        width, height = xmax - xmin, ymax - ymin\n",
    "        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='g', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        label_text = f\"True: {label}\" # 1: Square, 2: Circle\n",
    "        ax.text(xmin, ymin - 5, label_text, color='green', fontsize=10, bbox=dict(facecolor='white', alpha=0.5, pad=0))\n",
    "\n",
    "\n",
    "    # Предсказанные рамки (красные)\n",
    "    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):\n",
    "        if score >= threshold:\n",
    "            xmin, ymin, xmax, ymax = box\n",
    "            width, height = xmax - xmin, ymax - ymin\n",
    "            rect = patches.Rectangle((xmin, ymin), width, height, linewidth=2, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "            label_text = f\"Pred: {label} ({score:.2f})\"\n",
    "            ax.text(xmin, ymax + 15, label_text, color='red', fontsize=10, bbox=dict(facecolor='white', alpha=0.5, pad=0))\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Отрисовка результата\n",
    "plot_image_with_boxes(img_pil, pred_boxes, pred_labels, pred_scores, true_boxes, true_labels, threshold=0.5)\n",
    "\n",
    "print(\"Inference and visualization complete.\")\n",
    "# --- Конец Примера ---\n",
    "\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Введение в Сегментацию Изображений\n",
    "\n",
    "# Что такое Сегментация Изображений?\n",
    "# Это задача Computer Vision, цель которой - разделить изображение на несколько сегментов (областей),\n",
    "# присваивая каждому пикселю изображения метку класса.\n",
    "# В отличие от классификации (1 метка на изображение) или детекции (рамки вокруг объектов),\n",
    "# сегментация обеспечивает понимание содержимого изображения на уровне пикселей.\n",
    "\n",
    "# Цель Сегментации:\n",
    "# Создать детальную карту изображения, где каждый пиксель отнесен к определенному классу объектов или фону.\n",
    "\n",
    "# Типы Сегментации:\n",
    "# 1. Семантическая Сегментация (Semantic Segmentation):\n",
    "#    - Каждому пикселю присваивается метка класса (например, \"дорога\", \"здание\", \"небо\", \"человек\").\n",
    "#    - Не различает экземпляры одного класса (все люди имеют одну метку).\n",
    "#    - Этот туториал фокусируется в основном на семантической сегментации.\n",
    "# 2. Экземплярная Сегментация (Instance Segmentation):\n",
    "#    - Каждому пикселю присваивается метка класса И идентификатор экземпляра.\n",
    "#    - Различает отдельные объекты одного класса (например, \"человек 1\", \"человек 2\").\n",
    "#    - Mask R-CNN (из раздела детекции) является популярной моделью для этой задачи.\n",
    "# 3. Паноптическая Сегментация (Panoptic Segmentation):\n",
    "#    - Объединяет семантическую и экземплярную сегментацию.\n",
    "#    - Каждому пикселю присваивается метка класса и (если пиксель принадлежит объекту-\"экземпляру\") идентификатор экземпляра.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Ключевые Концепции\n",
    "\n",
    "# 1. Маска Сегментации (Segmentation Mask):\n",
    "#    - Выход модели сегментации. Представляет собой изображение (или тензор),\n",
    "#      того же размера, что и входное изображение.\n",
    "#    - Каждый пиксель маски содержит целочисленное значение - ID класса, к которому относится\n",
    "#      соответствующий пиксель входного изображения.\n",
    "#    - Например, 0 - фон, 1 - дорога, 2 - здание, 3 - человек.\n",
    "#    - Формат: Обычно тензор `[H, W]` типа `torch.int64`.\n",
    "\n",
    "# 2. Архитектуры \"Энкодер-Декодер\":\n",
    "#    - Многие современные модели сегментации (FCN, U-Net, DeepLab) используют эту структуру.\n",
    "#    - Энкодер (Encoder): Обычно это предобученная классификационная сеть (ResNet, MobileNet),\n",
    "#      которая извлекает иерархические признаки и уменьшает пространственное разрешение (downsampling).\n",
    "#      Захватывает семантический контекст.\n",
    "#    - Декодер (Decoder): Постепенно восстанавливает пространственное разрешение (upsampling),\n",
    "#      используя признаки из энкодера (часто с помощью skip connections), чтобы создать\n",
    "#      полноразмерную маску сегментации. Уточняет локализацию.\n",
    "\n",
    "# 3. Skip Connections:\n",
    "#    - Соединения, передающие признаки из слоев энкодера напрямую в соответствующие (по разрешению)\n",
    "#      слои декодера.\n",
    "#    - Помогают декодеру использовать как низкоуровневые (детализированные), так и высокоуровневые\n",
    "#      (семантические) признаки для более точной сегментации границ объектов.\n",
    "#    - Ключевая идея в архитектурах типа U-Net и FPN (используется в DeepLabV3).\n",
    "\n",
    "# 4. Транспонированная Свертка (Transposed Convolution / Deconvolution):\n",
    "#    - Слой, используемый в декодере для увеличения пространственного разрешения карт признаков (upsampling).\n",
    "#    - Можно рассматривать как \"обратную\" свертку. Имеет обучаемые параметры.\n",
    "\n",
    "# 5. Atrous (Dilated) Convolution (Расширенная Свертка):\n",
    "#    - Свертка с \"пробелами\" между весами ядра. Позволяет увеличить поле обзора (receptive field)\n",
    "#      слоя без увеличения количества параметров и без уменьшения пространственного разрешения.\n",
    "#    - Ключевой компонент в моделях DeepLab для захвата контекста на разных масштабах.\n",
    "\n",
    "# 6. Atrous Spatial Pyramid Pooling (ASPP):\n",
    "#    - Модуль (используется в DeepLab), который применяет несколько параллельных расширенных сверток\n",
    "#      с разными коэффициентами расширения (dilation rates) к одной и той же карте признаков.\n",
    "#    - Позволяет эффективно захватывать информацию об объектах и контексте на разных масштабах.\n",
    "\n",
    "# 7. Метрики Оценки:\n",
    "#    - Pixel Accuracy (Пиксельная Точность): Процент пикселей, классифицированных правильно.\n",
    "#      Простая, но может быть обманчива при сильном дисбалансе классов (например, много фона).\n",
    "#    - Intersection over Union (IoU) / Jaccard Index:\n",
    "#      - Для одного класса: IoU = TP / (TP + FP + FN)\n",
    "#        (TP - True Positives, FP - False Positives, FN - False Negatives для пикселей этого класса)\n",
    "#      - Площадь пересечения предсказанной маски класса и истинной маски, деленная на площадь их объединения.\n",
    "#    - Mean IoU (mIoU): IoU, усредненное по всем классам (кроме фона, иногда).\n",
    "#      Стандартная и наиболее важная метрика для семантической сегментации.\n",
    "#    - Dice Coefficient (Коэффициент Дайса / F1-score для пикселей):\n",
    "#      - Dice = 2 * TP / (2 * TP + FP + FN) = 2 * |Intersection| / (|Prediction| + |GroundTruth|)\n",
    "#      - Тесно связан с IoU: Dice = 2 * IoU / (IoU + 1). Часто используется в медицинской сегментации.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Модели Сегментации в `torchvision.models.segmentation`\n",
    "\n",
    "# `torchvision` предоставляет готовые реализации семантической сегментации.\n",
    "\n",
    "# 1. FCN (Fully Convolutional Network):\n",
    "#    - Одна из первых моделей, показавших эффективность глубокого обучения для сегментации.\n",
    "#    - Заменяет полносвязные слои в классификационных сетях на сверточные, позволяя обрабатывать изображения произвольного размера.\n",
    "#    - Использует skip connections для комбинирования карт признаков разного разрешения.\n",
    "#    - Пример загрузки: `models.segmentation.fcn_resnet50(pretrained=True)` или `fcn_resnet101`.\n",
    "\n",
    "# 2. DeepLabV3:\n",
    "#    - State-of-the-art модель, использующая расширенные свертки (atrous convolutions) и ASPP.\n",
    "#    - Эффективно работает с объектами разного масштаба.\n",
    "#    - Обычно использует ResNet или MobileNet как backbone.\n",
    "#    - Пример загрузки: `models.segmentation.deeplabv3_resnet50(pretrained=True)`, `deeplabv3_resnet101`, `deeplabv3_mobilenet_v3_large`.\n",
    "\n",
    "# 3. LR-ASPP (Lite R-ASPP):\n",
    "#    - Облегченная версия DeepLab, использующая MobileNetV3 в качестве backbone.\n",
    "#    - Предназначена для мобильных устройств с ограниченными ресурсами.\n",
    "#    - Пример загрузки: `models.segmentation.lraspp_mobilenet_v3_large(pretrained=True)`.\n",
    "\n",
    "# Предобученные Веса:\n",
    "# - Модели обычно предобучены на подмножестве COCO 2017, включающем 20 классов объектов (+ 1 фон), которые также есть в Pascal VOC.\n",
    "# - Классы: __background__, aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, diningtable, dog, horse, motorbike, person, pottedplant, sheep, sofa, train, tvmonitor.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Подготовка Данных для Сегментации\n",
    "\n",
    "# Формат Аннотаций (Масок):\n",
    "# - Каждому входному изображению `[3, H, W]` должна соответствовать маска сегментации `[H, W]`.\n",
    "# - Маска должна быть тензором типа `torch.int64`.\n",
    "# - Значения пикселей в маске - это ID классов: 0 для фона, 1 для класса 1, 2 для класса 2 и т.д.\n",
    "# - Количество классов `num_classes` должно соответствовать максимальному ID класса + 1.\n",
    "\n",
    "# Пользовательский `Dataset`:\n",
    "# - Наследуемся от `torch.utils.data.Dataset`.\n",
    "# - `__getitem__(self, idx)` возвращает:\n",
    "#   - Изображение (PIL Image или тензор).\n",
    "#   - Маску сегментации (PIL Image в режиме 'L' или 'P', или тензор `[H, W]`).\n",
    "# - `__len__(self)` возвращает количество изображений.\n",
    "\n",
    "# Пример структуры `__getitem__`:\n",
    "# def __getitem__(self, idx):\n",
    "#     img_path = self.image_files[idx]\n",
    "#     mask_path = self.mask_files[idx]\n",
    "#\n",
    "#     img = Image.open(img_path).convert(\"RGB\")\n",
    "#     # Маски часто сохраняют как grayscale изображения, где значение пикселя = ID класса\n",
    "#     mask = Image.open(mask_path) # Не конвертировать в RGB! Оставить как есть ('L' или 'P')\n",
    "#\n",
    "#     # Применение трансформаций СИНХРОННО к изображению и маске\n",
    "#     if self.transforms is not None:\n",
    "#         # Используйте библиотеки типа Albumentations или напишите свои трансформеры\n",
    "#         # Важно: интерполяция для маски должна быть Nearest Neighbor, чтобы не создавать новые ID классов\n",
    "#         img, mask = self.transforms(img, mask) # Пример сигнатуры\n",
    "#\n",
    "#     # Конвертация маски в тензор int64 ПОСЛЕ трансформаций (если они возвращают PIL)\n",
    "#     # mask_tensor = torch.from_numpy(np.array(mask)).long()\n",
    "#\n",
    "#     return img, mask # или img_tensor, mask_tensor\n",
    "\n",
    "# Трансформации и Аугментация (`Albumentations`):\n",
    "# - Это КРИТИЧЕСКИ важно. Геометрические аугментации (повороты, масштабирование, кропы, флипы)\n",
    "#   должны применяться абсолютно одинаково к изображению и маске.\n",
    "# - `Albumentations` - лучшая библиотека для этого.\n",
    "#   ```python\n",
    "#   import albumentations as A\n",
    "#   from albumentations.pytorch import ToTensorV2\n",
    "#\n",
    "#   train_transform = A.Compose([\n",
    "#       A.Resize(height=256, width=256),\n",
    "#       A.HorizontalFlip(p=0.5),\n",
    "#       A.Rotate(limit=35, p=0.3),\n",
    "#       A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "#       ToTensorV2(), # Конвертирует и img, и mask в тензоры\n",
    "#   ])\n",
    "#\n",
    "#   # В __getitem__:\n",
    "#   # augmented = train_transform(image=np.array(img), mask=np.array(mask))\n",
    "#   # img_tensor = augmented['image']\n",
    "#   # mask_tensor = augmented['mask'].long() # Убедиться, что тип long (int64)\n",
    "#   # return img_tensor, mask_tensor\n",
    "#   ```\n",
    "# - Для валидации/теста используйте только необходимые трансформации (Resize, Normalize, ToTensorV2).\n",
    "\n",
    "# `DataLoader`:\n",
    "# - Стандартный `collate_fn` обычно работает, так как изображения и маски после трансформаций\n",
    "#   имеют одинаковый размер и могут быть собраны в батчи `[N, C, H, W]` и `[N, H, W]`.\n",
    "# train_loader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4) # collate_fn не нужен\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Использование Предобученных Моделей и Fine-tuning\n",
    "\n",
    "# Загрузка Предобученной Модели:\n",
    "import torchvision.models as models\n",
    "\n",
    "# model = models.segmentation.deeplabv3_resnet50(pretrained=True)\n",
    "# model = models.segmentation.fcn_resnet50(pretrained=True)\n",
    "\n",
    "# Модификация для Своего Числа Классов:\n",
    "# Предобученные модели (на COCO/VOC) имеют классификатор на 21 класс (20 + фон).\n",
    "# Нужно заменить последний слой(и) классификатора.\n",
    "\n",
    "# num_classes = 5 # Например, 4 ваших класса + 1 фон\n",
    "\n",
    "# Для DeepLabV3 (имеет основной и вспомогательный классификатор)\n",
    "# model = models.segmentation.deeplabv3_resnet50(weights=models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT)\n",
    "# # Заменяем основной классификатор\n",
    "# model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "# # Заменяем вспомогательный классификатор (если он есть и используется)\n",
    "# model.aux_classifier[4] = nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "# Для FCN (имеет основной и вспомогательный классификатор)\n",
    "# model = models.segmentation.fcn_resnet50(weights=models.segmentation.FCN_ResNet50_Weights.DEFAULT)\n",
    "# model.classifier[4] = nn.Conv2d(512, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "# model.aux_classifier[4] = nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "\n",
    "# Fine-tuning:\n",
    "# - Аналогично классификации/детекции.\n",
    "# - Можно заморозить backbone и обучать только классификаторы (Feature Extraction).\n",
    "# - Можно разморозить все или часть слоев и обучать с маленьким LR (Fine-tuning).\n",
    "# - Передать параметры в оптимизатор: `optimizer = optim.Adam(model.parameters(), lr=...)`\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Обучение Модели Сегментации\n",
    "\n",
    "# Цикл Обучения:\n",
    "# - Похож на классификацию, но работает с пиксельными предсказаниями и масками.\n",
    "# - Модели `torchvision.models.segmentation` возвращают словарь OrderedDict.\n",
    "#   - Ключ `'out'`: Основной выход модели, тензор с логитами формы `[N, C, H, W]`, где `C` - количество классов.\n",
    "#   - Ключ `'aux'` (если есть): Выход вспомогательного классификатора, той же формы.\n",
    "\n",
    "# Функция Потерь (Loss Function):\n",
    "# - `nn.CrossEntropyLoss` - стандартный выбор.\n",
    "# - Она ожидает:\n",
    "#   - Input (логиты): `[N, C, H, W]`\n",
    "#   - Target (маска с ID классов): `[N, H, W]` типа `torch.int64`.\n",
    "# - Автоматически применяет Softmax к логитам и вычисляет NLLLoss.\n",
    "# - Можно задать `weight` для классов (если есть дисбаланс) или `ignore_index` (если нужно игнорировать пиксели с определенным ID в маске).\n",
    "\n",
    "# Пример Цикла Обучения (Упрощенный):\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# model.to(device)\n",
    "#\n",
    "# params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "# criterion = nn.CrossEntropyLoss() # Веса или ignore_index можно добавить сюда\n",
    "#\n",
    "# num_epochs = 10\n",
    "#\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for images, masks in train_loader:\n",
    "#         images = images.to(device)\n",
    "#         masks = masks.to(device) # Ожидаемый тип: torch.int64\n",
    "#\n",
    "#         optimizer.zero_grad()\n",
    "#\n",
    "#         # Прямой проход\n",
    "#         outputs = model(images) # Получаем словарь {'out': ..., 'aux': ...}\n",
    "#\n",
    "#         # Расчет лосса\n",
    "#         loss = criterion(outputs['out'], masks)\n",
    "#         # Если используется вспомогательный лосс:\n",
    "#         if 'aux' in outputs:\n",
    "#             loss_aux = criterion(outputs['aux'], masks)\n",
    "#             loss = loss + 0.4 * loss_aux # Вес для aux_loss (часто 0.4 или 0.5)\n",
    "#\n",
    "#         # Обратный проход\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#\n",
    "#         running_loss += loss.item()\n",
    "#\n",
    "#     epoch_loss = running_loss / len(train_loader)\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}\")\n",
    "#\n",
    "#     # --- Фаза Валидации (Оценка mIoU) ---\n",
    "#     model.eval()\n",
    "#     # ... (Код для расчета Pixel Accuracy и mIoU на val_loader) ...\n",
    "#     # Расчет mIoU требует построения confusion matrix по всем пикселям валидационного сета.\n",
    "\n",
    "# Оценка (mIoU):\n",
    "# - Требует итерации по валидационному сету, получения предсказанных масок (`argmax` по выходу)\n",
    "#   и сравнения их с истинными масками для построения суммарной матрицы ошибок (confusion matrix)\n",
    "#   размера `num_classes x num_classes`.\n",
    "# - Затем из матрицы ошибок вычисляется IoU для каждого класса и усредняется.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 7: Инференс (Получение Предсказаний)\n",
    "\n",
    "# - Перевести модель в режим оценки: `model.eval()`.\n",
    "# - Подготовить входное изображение (трансформации как для валидации, unsqueeze(0), to(device)).\n",
    "# - Выполнить предсказание `with torch.no_grad(): outputs = model(input_tensor)`.\n",
    "# - Получить тензор логитов из словаря: `logits = outputs['out']`. Форма `[1, C, H, W]`.\n",
    "# - Получить предсказанную маску с ID классов: `pred_mask = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy()`. Форма `[H, W]`.\n",
    "# - (Опционально) Применить `softmax` к `logits` для получения вероятностей для каждого пикселя/класса.\n",
    "# - (Опционально) Если входное изображение изменяло размер, нужно изменить размер `pred_mask` обратно к исходному размеру изображения (используя интерполяцию `Nearest Neighbor`).\n",
    "# - Визуализировать `pred_mask`, например, раскрасив пиксели в соответствии с предсказанными ID классов.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 8: Пример Задачи и Решения (Полный Код)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Обучить модель DeepLabV3 (с ResNet50 backbone) для семантической сегментации\n",
    "# двух классов объектов: \"Красный квадрат\" (метка 1) и \"Синий круг\" (метка 2)\n",
    "# на синтетических изображениях. Фон имеет метку 0. Использовать предобученные веса и дообучить модель.\n",
    "\n",
    "# --- Решение (Полный Код) ---\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# --- 0. Настройки ---\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "NUM_CLASSES = 3 # 1: Квадрат, 2: Круг, 0: Фон\n",
    "BATCH_SIZE = 4 # Увеличим немного батч\n",
    "NUM_EPOCHS = 5 # Мало эпох для быстрого примера\n",
    "LEARNING_RATE = 0.001\n",
    "IMG_SIZE = 224 # DeepLab обычно работает с этим размером или больше\n",
    "MODEL_SAVE_PATH = \"deeplabv3_shapes_best.pth\"\n",
    "\n",
    "# --- 1. Создание Синтетического Датасета с Масками ---\n",
    "class SyntheticShapesMaskDataset(Dataset):\n",
    "    def __init__(self, num_samples=100, img_size=224, transform=None):\n",
    "        self.num_samples = num_samples\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.masks = []\n",
    "        self._generate_data()\n",
    "\n",
    "    def _generate_data(self):\n",
    "        print(\"Generating synthetic data with masks...\")\n",
    "        for idx in range(self.num_samples):\n",
    "            img = Image.new('RGB', (self.img_size, self.img_size), color='white')\n",
    "            mask = Image.new('L', (self.img_size, self.img_size), color=0) # Маска фона (класс 0)\n",
    "            draw_img = ImageDraw.Draw(img)\n",
    "            draw_mask = ImageDraw.Draw(mask)\n",
    "\n",
    "            num_shapes = np.random.randint(1, 4)\n",
    "            for _ in range(num_shapes):\n",
    "                shape_type = np.random.choice(['square', 'circle'])\n",
    "                size = np.random.randint(20, 60)\n",
    "                x = np.random.randint(0, self.img_size - size)\n",
    "                y = np.random.randint(0, self.img_size - size)\n",
    "                xmin, ymin, xmax, ymax = x, y, x + size, y + size\n",
    "\n",
    "                if shape_type == 'square':\n",
    "                    img_color = 'red'\n",
    "                    mask_value = 1 # Метка для квадрата\n",
    "                    draw_img.rectangle([xmin, ymin, xmax, ymax], fill=img_color, outline='black')\n",
    "                    draw_mask.rectangle([xmin, ymin, xmax, ymax], fill=mask_value) # Заливаем маску ID класса\n",
    "                else: # circle\n",
    "                    img_color = 'blue'\n",
    "                    mask_value = 2 # Метка для круга\n",
    "                    draw_img.ellipse([xmin, ymin, xmax, ymax], fill=img_color, outline='black')\n",
    "                    draw_mask.ellipse([xmin, ymin, xmax, ymax], fill=mask_value) # Заливаем маску ID класса\n",
    "\n",
    "            self.images.append(img)\n",
    "            self.masks.append(mask)\n",
    "        print(\"Synthetic data generated.\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        mask = self.masks[idx]\n",
    "\n",
    "        # Конвертируем в numpy перед Albumentations\n",
    "        img_np = np.array(img)\n",
    "        mask_np = np.array(mask)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img_np, mask=mask_np)\n",
    "            img_tensor = augmented['image']\n",
    "            mask_tensor = augmented['mask'].long() # Albumentations ToTensorV2 не делает long\n",
    "        else:\n",
    "            # Если нет трансформаций, просто конвертируем\n",
    "            img_tensor = torchvision.transforms.functional.to_tensor(img)\n",
    "            mask_tensor = torch.from_numpy(mask_np).long()\n",
    "\n",
    "        return img_tensor, mask_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "# --- 2. Трансформации и Загрузчики Данных ---\n",
    "# Используем Albumentations\n",
    "train_transform = A.Compose([\n",
    "    A.Resize(height=IMG_SIZE, width=IMG_SIZE), # Убедимся, что размер правильный\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    # A.Rotate(limit=20, p=0.3), # Можно добавить еще аугментаций\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), # ImageNet нормы\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Resize(height=IMG_SIZE, width=IMG_SIZE),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "train_dataset = SyntheticShapesMaskDataset(num_samples=100, img_size=IMG_SIZE, transform=train_transform)\n",
    "val_dataset = SyntheticShapesMaskDataset(num_samples=20, img_size=IMG_SIZE, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# --- 3. Загрузка и Модификация Модели ---\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # Загружаем предобученную модель DeepLabV3\n",
    "    model = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.DEFAULT)\n",
    "\n",
    "    # Заменяем основной классификатор\n",
    "    # У DeepLabV3 классификатор находится в model.classifier[4]\n",
    "    model.classifier[4] = nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "    # Заменяем вспомогательный классификатор (если он есть)\n",
    "    # У DeepLabV3 с ResNet он в model.aux_classifier[4]\n",
    "    model.aux_classifier[4] = nn.Conv2d(256, num_classes, kernel_size=(1, 1), stride=(1, 1))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model_instance_segmentation(NUM_CLASSES)\n",
    "model.to(device)\n",
    "\n",
    "# --- 4. Оптимизатор и Функция Потерь ---\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.Adam(params, lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss() # Стандартный лосс для семантической сегментации\n",
    "\n",
    "# --- 5. Цикл Обучения ---\n",
    "print(\"\\nStarting Training...\")\n",
    "training_start_time = time.time()\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, masks in train_loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device) # Тип должен быть Long (int64)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs['out'], masks)\n",
    "        if 'aux' in outputs:\n",
    "            loss_aux = criterion(outputs['aux'], masks)\n",
    "            loss = loss + 0.4 * loss_aux\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    # Валидация\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    # Здесь можно добавить расчет mIoU, но для простоты считаем только лосс\n",
    "    with torch.no_grad():\n",
    "        for images, masks in val_loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs['out'], masks) # Считаем лосс только по основному выходу на валидации\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Сохранение лучшей модели по валидационному лоссу\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f\"Saved best model to {MODEL_SAVE_PATH}\")\n",
    "\n",
    "training_end_time = time.time()\n",
    "print(f\"Training finished in {training_end_time - training_start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 6. Инференс и Визуализация ---\n",
    "print(\"\\nLoading best model for inference...\")\n",
    "# Загружаем лучшую модель\n",
    "model = get_model_instance_segmentation(NUM_CLASSES) # Создаем инстанс\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Running Inference on a sample image...\")\n",
    "# Возьмем изображение из валидационного набора\n",
    "img_tensor, true_mask_tensor = val_dataset[np.random.randint(len(val_dataset))]\n",
    "\n",
    "# Денормализация и конвертация для визуализации\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "img_vis = inv_normalize(img_tensor).permute(1, 2, 0).cpu().numpy() # CHW -> HWC\n",
    "img_vis = (img_vis * 255).astype(np.uint8)\n",
    "true_mask_vis = true_mask_tensor.cpu().numpy()\n",
    "\n",
    "# Получение предсказания\n",
    "with torch.no_grad():\n",
    "    input_batch = img_tensor.unsqueeze(0).to(device)\n",
    "    output = model(input_batch)['out'] # Берем основной выход\n",
    "    pred_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy() # NCHW -> HW\n",
    "\n",
    "# Функция для отрисовки масок с цветами\n",
    "def decode_segmap(mask, num_classes):\n",
    "    # Простая палитра: 0-черный(фон), 1-красный(квадрат), 2-синий(круг)\n",
    "    palette = np.array([[0, 0, 0], [255, 0, 0], [0, 0, 255]], dtype=np.uint8)\n",
    "    rgb_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    for label in range(num_classes):\n",
    "        rgb_mask[mask == label] = palette[label]\n",
    "    return rgb_mask\n",
    "\n",
    "# Визуализация\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "ax[0].imshow(img_vis)\n",
    "ax[0].set_title('Input Image')\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(decode_segmap(true_mask_vis, NUM_CLASSES))\n",
    "ax[1].set_title('Ground Truth Mask')\n",
    "ax[1].axis('off')\n",
    "\n",
    "ax[2].imshow(decode_segmap(pred_mask, NUM_CLASSES))\n",
    "ax[2].set_title('Predicted Mask')\n",
    "ax[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Inference and visualization complete.\")\n",
    "# --- Конец Примера ---\n",
    "\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Введение в Фильтрацию Изображений\n",
    "\n",
    "# Что такое Фильтрация Изображений?\n",
    "# Это процесс модификации или улучшения изображения путем применения математических операций\n",
    "# к каждому пикселю и его соседям. Эти операции обычно выполняются с помощью \"ядра\" или \"фильтра\".\n",
    "# Фильтрация является фундаментальной операцией во многих задачах Computer Vision и обработки изображений.\n",
    "\n",
    "# Цель Фильтрации:\n",
    "# - Снижение шума (Noise Reduction).\n",
    "# - Увеличение резкости (Sharpening) и выделение деталей.\n",
    "# - Размытие (Blurring) для сглаживания или как предварительный этап для других операций.\n",
    "# - Обнаружение краев (Edge Detection).\n",
    "# - Извлечение определенных признаков.\n",
    "\n",
    "# Как это работает (Концепция Конволюции):\n",
    "# - Маленькая матрица (ядро/фильтр) скользит по всему изображению.\n",
    "# - В каждой позиции вычисляется взвешенная сумма значений пикселей, попадающих под ядро.\n",
    "# - Веса задаются значениями в ядре.\n",
    "# - Результат этой суммы становится новым значением центрального пикселя (под якорем ядра) в выходном изображении.\n",
    "# - Этот процесс называется конволюцией (или корреляцией, в зависимости от реализации).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Ядра (Фильтры) и Конволюция\n",
    "\n",
    "# Ядро (Kernel / Filter / Mask):\n",
    "# - Небольшая 2D матрица чисел.\n",
    "# - Размер ядра (например, 3x3, 5x5) определяет окрестность пикселя, которая учитывается при фильтрации.\n",
    "# - Значения в ядре определяют характер фильтрации (размытие, резкость и т.д.).\n",
    "# - Якорь (Anchor Point): Центральный элемент ядра, который совмещается с текущим пикселем изображения.\n",
    "\n",
    "# Пример ядра 3x3 (усредняющий фильтр):\n",
    "# kernel_avg = [[1/9, 1/9, 1/9],\n",
    "#               [1/9, 1/9, 1/9],\n",
    "#               [1/9, 1/9, 1/9]]\n",
    "\n",
    "# Пример ядра 3x3 (увеличение резкости):\n",
    "# kernel_sharpen = [[ 0, -1,  0],\n",
    "#                   [-1,  5, -1],\n",
    "#                   [ 0, -1,  0]]\n",
    "\n",
    "# Обработка Границ (Padding):\n",
    "# - Когда ядро находится у края изображения, часть его выходит за пределы.\n",
    "# - Способы обработки:\n",
    "#   - Игнорировать пиксели у края (выходное изображение будет меньше).\n",
    "#   - Дополнить изображение (Padding):\n",
    "#     - Нулевое дополнение (Zero Padding): Добавить пиксели со значением 0.\n",
    "#     - Дополнение репликацией (Replicate): Повторить значения крайних пикселей.\n",
    "#     - Дополнение отражением (Reflect): Отразить пиксели относительно границы.\n",
    "# - Библиотеки (как OpenCV) обычно предоставляют опции для выбора способа обработки границ.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Линейные Фильтры\n",
    "\n",
    "# Линейные фильтры выполняют взвешенное суммирование пикселей в окрестности.\n",
    "\n",
    "# 1. Усредняющий Фильтр (Averaging / Box Filter):\n",
    "#    - Ядро: Все элементы равны `1 / (ширина * высота ядра)`.\n",
    "#    - Принцип: Заменяет пиксель средним значением его соседей (включая себя).\n",
    "#    - Цель: Простое размытие, снижение шума. Может сильно размывать края.\n",
    "#    - Реализация (OpenCV): `cv2.blur(image, (kernel_width, kernel_height))`\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def apply_average_blur(image, kernel_size=(5, 5)):\n",
    "    # kernel_size должен быть кортежем с нечетными числами, например (3, 3), (5, 5)\n",
    "    blurred_image = cv2.blur(image, kernel_size)\n",
    "    return blurred_image\n",
    "\n",
    "# 2. Гауссовский Фильтр (Gaussian Filter):\n",
    "#    - Ядро: Значения определяются 2D функцией Гаусса. Центральные пиксели имеют больший вес,\n",
    "#      вес уменьшается по мере удаления от центра.\n",
    "#    - Принцип: Взвешенное усреднение, где вес соседей зависит от их удаленности от центра по Гауссу.\n",
    "#    - Цель: Более гладкое и естественное размытие по сравнению с усредняющим фильтром.\n",
    "#      Очень эффективен для удаления Гауссовского шума. Часто используется как препроцессинг.\n",
    "#    - Параметры:\n",
    "#      - `ksize`: Размер ядра (ширина, высота), должны быть нечетными положительными числами.\n",
    "#      - `sigmaX`: Стандартное отклонение Гаусса по оси X. Контролирует степень размытия по горизонтали.\n",
    "#      - `sigmaY` (опционально): Стандартное отклонение по оси Y. Если 0, вычисляется из `sigmaX`. Если оба 0, вычисляются из `ksize`.\n",
    "#    - Реализация (OpenCV): `cv2.GaussianBlur(image, ksize, sigmaX, sigmaY=...)`\n",
    "def apply_gaussian_blur(image, kernel_size=(5, 5), sigmaX=0):\n",
    "    # kernel_size должен быть кортежем с нечетными положительными числами\n",
    "    # sigmaX=0 означает, что сигма будет вычислена из размера ядра\n",
    "    blurred_image = cv2.GaussianBlur(image, kernel_size, sigmaX)\n",
    "    return blurred_image\n",
    "\n",
    "# 3. Фильтр Резкости (Sharpening Filter):\n",
    "#    - Ядро: Обычно имеет положительное значение в центре и отрицательные значения вокруг,\n",
    "#      сумма элементов ядра часто равна 1 (чтобы сохранить общую яркость).\n",
    "#      Пример: [[0, -1, 0], [-1, 5, -1], [0, -1, 0]]\n",
    "#    - Принцип: Увеличивает разницу между пикселем и его соседями, подчеркивая детали и края.\n",
    "#    - Цель: Сделать изображение более четким. Может усилить существующий шум.\n",
    "#    - Реализация (OpenCV): Используется общая функция конволюции `cv2.filter2D`.\n",
    "def apply_sharpening(image):\n",
    "    # Пример ядра для увеличения резкости\n",
    "    kernel = np.array([[ 0, -1,  0],\n",
    "                       [-1,  5, -1],\n",
    "                       [ 0, -1,  0]])\n",
    "    # Применяем ядро с помощью cv2.filter2D\n",
    "    # -1 означает, что глубина выходного изображения будет такой же, как у входного\n",
    "    sharpened_image = cv2.filter2D(image, -1, kernel)\n",
    "    return sharpened_image\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Нелинейные Фильтры\n",
    "\n",
    "# Нелинейные фильтры выполняют операции, не являющиеся простым взвешенным суммированием (например, сортировка, выбор медианы/минимума/максимума).\n",
    "\n",
    "# 1. Медианный Фильтр (Median Filter):\n",
    "#    - Принцип: Заменяет значение центрального пикселя медианным значением всех пикселей в окрестности ядра.\n",
    "#      (Медиана - значение, которое находится в середине отсортированного списка пикселей).\n",
    "#    - Цель: Очень эффективен для удаления импульсного шума (\"соль и перец\").\n",
    "#      Хорошо сохраняет края по сравнению с линейными фильтрами размытия.\n",
    "#    - Параметры:\n",
    "#      - `ksize`: Размер апертуры (сторона квадратного ядра), должно быть нечетным целым числом больше 1 (например, 3, 5).\n",
    "#    - Реализация (OpenCV): `cv2.medianBlur(image, ksize)`\n",
    "def apply_median_blur(image, ksize=5):\n",
    "    # ksize должно быть нечетным целым > 1\n",
    "    filtered_image = cv2.medianBlur(image, ksize)\n",
    "    return filtered_image\n",
    "\n",
    "# 2. Билатеральный Фильтр (Bilateral Filter):\n",
    "#    - Принцип: Учитывает не только пространственную близость пикселей (как Гауссовский фильтр),\n",
    "#      но и их схожесть по интенсивности/цвету. Пиксели, которые находятся близко пространственно,\n",
    "#      но сильно отличаются по значению, будут иметь меньший вес.\n",
    "#    - Цель: Сглаживание шума при сохранении резких краев.\n",
    "#    - Параметры:\n",
    "#      - `d`: Диаметр окрестности пикселя. -1 означает, что он будет вычислен из `sigmaSpace`.\n",
    "#      - `sigmaColor`: Сигма в цветовом пространстве. Большее значение означает, что более удаленные по цвету пиксели будут влиять друг на друга.\n",
    "#      - `sigmaSpace`: Сигма в координатном пространстве (аналогично Гауссову размытию). Большее значение означает, что более удаленные пиксели будут влиять друг на друга.\n",
    "#    - Реализация (OpenCV): `cv2.bilateralFilter(image, d, sigmaColor, sigmaSpace)`\n",
    "def apply_bilateral_filter(image, d=9, sigmaColor=75, sigmaSpace=75):\n",
    "    # d: Диаметр окрестности.\n",
    "    # sigmaColor: Фильтр сигма в цветовом пространстве.\n",
    "    # sigmaSpace: Фильтр сигма в координатном пространстве.\n",
    "    filtered_image = cv2.bilateralFilter(image, d, sigmaColor, sigmaSpace)\n",
    "    return filtered_image\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Библиотеки для Фильтрации\n",
    "\n",
    "# 1. OpenCV (`cv2`):\n",
    "#    - Де-факто стандарт для задач Computer Vision в Python.\n",
    "#    - Предоставляет высокооптимизированные реализации большинства фильтров.\n",
    "#    - Рекомендуется для большинства задач фильтрации.\n",
    "\n",
    "# 2. SciPy (`scipy.ndimage`):\n",
    "#    - Модуль `scipy.ndimage.filters` содержит функции для Гауссовой, медианной, усредняющей и общей конволюции (`convolve`).\n",
    "#    - Хорошая альтернатива, особенно если вы уже работаете в экосистеме SciPy/NumPy.\n",
    "#    - `from scipy.ndimage import gaussian_filter, median_filter`\n",
    "\n",
    "# 3. Pillow (`PIL.ImageFilter`):\n",
    "#    - Предоставляет базовые фильтры (BLUR, CONTOUR, DETAIL, EDGE_ENHANCE, SHARPEN, SMOOTH, GaussianBlur, MedianFilter, UnsharpMask).\n",
    "#    - Менее гибкая и производительная, чем OpenCV или SciPy, для сложных задач.\n",
    "#    - `from PIL import Image, ImageFilter`\n",
    "#    - `im_blurred = im.filter(ImageFilter.GaussianBlur(radius=2))`\n",
    "\n",
    "# 4. PyTorch (`kornia` или `torch.nn.functional.conv2d`):\n",
    "#    - Можно реализовать линейные фильтры с помощью `F.conv2d`, определив ядро как веса свертки.\n",
    "#    - Библиотека `Kornia` предоставляет GPU-ускоренные функции для многих операций CV, включая фильтрацию (GaussianBlur, MedianBlur и т.д.), совместимые с тензорами PyTorch.\n",
    "#    - Обычно используется, когда фильтрация является частью большей модели глубокого обучения, выполняемой на GPU.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Пример Задачи и Решения (Используя OpenCV)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Загрузить изображение, добавить к нему искусственный шум \"соль и перец\".\n",
    "# Затем применить Гауссовский и Медианный фильтры для удаления этого шума.\n",
    "# Сравнить результаты фильтрации с исходным и зашумленным изображениями.\n",
    "\n",
    "# --- Решение (Полный Код) ---\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os # Для проверки существования файла\n",
    "\n",
    "# --- Функция для добавления шума \"соль и перец\" ---\n",
    "def add_salt_pepper_noise(image, salt_prob=0.02, pepper_prob=0.02):\n",
    "    noisy_image = np.copy(image)\n",
    "    total_pixels = image.size\n",
    "    # Добавляем \"соль\" (белые пиксели)\n",
    "    num_salt = np.ceil(salt_prob * total_pixels)\n",
    "    coords = [np.random.randint(0, i - 1, int(num_salt)) for i in image.shape[:2]]\n",
    "    noisy_image[coords[0], coords[1]] = 255 # Для grayscale/color\n",
    "\n",
    "    # Добавляем \"перец\" (черные пиксели)\n",
    "    num_pepper = np.ceil(pepper_prob * total_pixels)\n",
    "    coords = [np.random.randint(0, i - 1, int(num_pepper)) for i in image.shape[:2]]\n",
    "    noisy_image[coords[0], coords[1]] = 0\n",
    "\n",
    "    return noisy_image\n",
    "\n",
    "# --- 1. Загрузка Изображения ---\n",
    "# Укажите путь к вашему изображению\n",
    "image_path = 'path/to/your/image.jpg' # ЗАМЕНИТЕ НА ВАШ ПУТЬ\n",
    "\n",
    "# Проверка существования файла\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"Ошибка: Файл не найден по пути: {image_path}\")\n",
    "    print(\"Пожалуйста, замените 'path/to/your/image.jpg' на корректный путь к вашему изображению.\")\n",
    "    # Создадим простое серое изображение как заглушку, если файл не найден\n",
    "    original_image = np.full((200, 300, 3), 128, dtype=np.uint8) # Серое изображение 200x300\n",
    "    print(\"Используется серое изображение-заглушка.\")\n",
    "else:\n",
    "    original_image = cv2.imread(image_path)\n",
    "    # Проверка успешности загрузки\n",
    "    if original_image is None:\n",
    "        print(f\"Ошибка: Не удалось загрузить изображение по пути: {image_path}\")\n",
    "        exit()\n",
    "    print(f\"Изображение '{image_path}' успешно загружено.\")\n",
    "\n",
    "# Конвертируем в RGB для Matplotlib (OpenCV читает как BGR)\n",
    "original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# --- 2. Добавление Шума ---\n",
    "noisy_image = add_salt_pepper_noise(original_image_rgb)\n",
    "print(\"Шум 'соль и перец' добавлен.\")\n",
    "\n",
    "# --- 3. Применение Гауссовского Фильтра ---\n",
    "# Параметры: размер ядра (должен быть нечетным), sigmaX\n",
    "gaussian_blurred = cv2.GaussianBlur(noisy_image, (5, 5), 0)\n",
    "print(\"Гауссовский фильтр применен.\")\n",
    "\n",
    "# --- 4. Применение Медианного Фильтра ---\n",
    "# Параметры: размер апертуры (нечетное число > 1)\n",
    "median_filtered = cv2.medianBlur(noisy_image, 5)\n",
    "print(\"Медианный фильтр применен.\")\n",
    "\n",
    "# --- 5. Отображение Результатов ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(original_image_rgb)\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(noisy_image)\n",
    "plt.title('Noisy Image (Salt & Pepper)')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(gaussian_blurred)\n",
    "plt.title('Gaussian Filtered')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(median_filtered)\n",
    "plt.title('Median Filtered')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Отображение результатов завершено.\")\n",
    "# Наблюдение: Медианный фильтр обычно гораздо лучше справляется с шумом \"соль и перец\",\n",
    "# в то время как Гауссовский фильтр просто размывает шумные пиксели вместе с остальным изображением.\n",
    "\n",
    "# --- Конец Примера ---\n",
    "\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Шум в Изображениях - Что это и Зачем Бороться?\n",
    "\n",
    "# Что такое Шум?\n",
    "# Шум в изображениях - это случайные, нежелательные вариации яркости или цветовой информации.\n",
    "# Он искажает исходное визуальное содержимое изображения.\n",
    "# Источники шума:\n",
    "# - Сенсор камеры (тепловой шум, шум считывания при слабом освещении).\n",
    "# - Ошибки передачи данных.\n",
    "# - Сжатие изображения с потерями.\n",
    "# - Сканирование фотографий.\n",
    "\n",
    "# Почему Шум - это Проблема?\n",
    "# - Ухудшает визуальное качество изображения для человека.\n",
    "# - Значительно затрудняет работу алгоритмов Computer Vision:\n",
    "#   - Снижает точность извлечения признаков (SIFT, ORB и т.д.).\n",
    "#   - Ухудшает качество сегментации и обнаружения объектов.\n",
    "#   - Может привести к неверной классификации.\n",
    "\n",
    "# Цель Борьбы с Шумом (Шумоподавление / Denoising):\n",
    "# - Улучшить визуальное восприятие изображения.\n",
    "# - Повысить производительность и точность последующих этапов анализа изображений.\n",
    "# - Восстановить исходное изображение, максимально удалив шум и сохранив важные детали (края, текстуры).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Распространенные Типы Шума и Их Генерация\n",
    "\n",
    "# Понимание типа шума важно для выбора правильного метода фильтрации.\n",
    "\n",
    "# 1. Гауссовский Шум (Gaussian Noise):\n",
    "#    - Характер: Аддитивный шум, значения которого распределены по нормальному (Гауссову) закону с нулевым средним.\n",
    "#      Затрагивает каждый пиксель изображения. `I_noisy = I_original + Noise_gaussian`\n",
    "#    - Причины: Часто возникает из-за теплового шума сенсора и шума считывания.\n",
    "#    - Генерация (NumPy/OpenCV):\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def add_gaussian_noise(image, mean=0, sigma=25):\n",
    "    # Убедимся, что изображение в формате float для добавления шума\n",
    "    if image.dtype != np.float64:\n",
    "        image_float = image.astype(np.float64)\n",
    "    else:\n",
    "        image_float = image.copy()\n",
    "\n",
    "    # Генерируем гауссовский шум\n",
    "    noise = np.random.normal(mean, sigma, image.shape)\n",
    "    noisy_image_float = image_float + noise\n",
    "\n",
    "    # Обрезаем значения, чтобы они остались в допустимом диапазоне [0, 255]\n",
    "    noisy_image_clipped = np.clip(noisy_image_float, 0, 255)\n",
    "\n",
    "    # Возвращаем к исходному типу данных (обычно uint8)\n",
    "    noisy_image = noisy_image_clipped.astype(image.dtype)\n",
    "    return noisy_image\n",
    "\n",
    "# 2. Шум \"Соль и Перец\" (Salt-and-Pepper Noise):\n",
    "#    - Характер: Импульсный шум. Случайные пиксели заменяются на минимальное (0, \"перец\")\n",
    "#      или максимальное (255, \"соль\") значение интенсивности.\n",
    "#    - Причины: Ошибки передачи данных, дефекты ячеек памяти или сенсора.\n",
    "#    - Генерация (NumPy/OpenCV):\n",
    "def add_salt_pepper_noise(image, salt_prob=0.02, pepper_prob=0.02):\n",
    "    noisy_image = np.copy(image)\n",
    "    total_pixels = image.size\n",
    "    num_salt = int(salt_prob * total_pixels)\n",
    "    num_pepper = int(pepper_prob * total_pixels)\n",
    "\n",
    "    # Добавляем \"соль\"\n",
    "    salt_coords = [np.random.randint(0, i - 1, num_salt) for i in image.shape[:2]]\n",
    "    if len(image.shape) == 3: # Цветное изображение\n",
    "        salt_channels = np.random.randint(0, image.shape[2], num_salt)\n",
    "        noisy_image[salt_coords[0], salt_coords[1], salt_channels] = 255\n",
    "        # Можно установить все каналы в 255: noisy_image[salt_coords[0], salt_coords[1], :] = 255\n",
    "    else: # Grayscale\n",
    "        noisy_image[salt_coords[0], salt_coords[1]] = 255\n",
    "\n",
    "    # Добавляем \"перец\"\n",
    "    pepper_coords = [np.random.randint(0, i - 1, num_pepper) for i in image.shape[:2]]\n",
    "    if len(image.shape) == 3: # Цветное изображение\n",
    "        pepper_channels = np.random.randint(0, image.shape[2], num_pepper)\n",
    "        noisy_image[pepper_coords[0], pepper_coords[1], pepper_channels] = 0\n",
    "        # Можно установить все каналы в 0: noisy_image[pepper_coords[0], pepper_coords[1], :] = 0\n",
    "    else: # Grayscale\n",
    "        noisy_image[pepper_coords[0], pepper_coords[1]] = 0\n",
    "\n",
    "    return noisy_image\n",
    "\n",
    "# 3. Пятнистый Шум (Speckle Noise):\n",
    "#    - Характер: Мультипликативный шум. `I_noisy = I_original * (1 + Noise)` или `I_noisy = I_original + I_original * Noise`.\n",
    "#      Интенсивность шума пропорциональна интенсивности пикселя.\n",
    "#    - Причины: Часто встречается в когерентных системах визуализации (радары (SAR), УЗИ, лазеры).\n",
    "#    - Генерация (NumPy/OpenCV):\n",
    "def add_speckle_noise(image, sigma=0.1):\n",
    "     # Убедимся, что изображение в формате float\n",
    "    if image.dtype != np.float64:\n",
    "        image_float = image.astype(np.float64) / 255.0 # Нормализуем в [0, 1]\n",
    "    else:\n",
    "        image_float = image.copy()\n",
    "\n",
    "    # Генерируем гауссовский шум для мультипликативной модели\n",
    "    noise = np.random.normal(0, sigma, image.shape)\n",
    "    noisy_image_float = image_float + image_float * noise\n",
    "\n",
    "    # Обрезаем значения в [0, 1]\n",
    "    noisy_image_clipped = np.clip(noisy_image_float, 0, 1.0)\n",
    "\n",
    "    # Возвращаем к исходному диапазону и типу данных\n",
    "    noisy_image = (noisy_image_clipped * 255).astype(image.dtype)\n",
    "    return noisy_image\n",
    "\n",
    "# 4. Другие типы шума (менее распространенные в общих задачах):\n",
    "#    - Шум Пуассона (Poisson Noise / Shot Noise): Зависит от интенсивности сигнала, важен при слабом освещении (фотография, медицина).\n",
    "#    - Квантования Шум (Quantization Noise): Возникает при преобразовании аналогового сигнала в цифровой с ограниченной битностью.\n",
    "#    - Периодический Шум (Periodic Noise): Регулярные паттерны, часто из-за электрических помех. Удаляется с помощью Фурье-фильтрации.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Методы Фильтрации для Разных Типов Шума\n",
    "\n",
    "# Выбор фильтра зависит от типа шума и необходимости сохранения деталей.\n",
    "\n",
    "# 1. Для Гауссовского Шума:\n",
    "#    - Усредняющий Фильтр (`cv2.blur`):\n",
    "#      - Простое усреднение пикселей в окне.\n",
    "#      - Эффективен для слабого шума, но сильно размывает края.\n",
    "#      - `filtered = cv2.blur(noisy_image, (5, 5))`\n",
    "#    - Гауссовский Фильтр (`cv2.GaussianBlur`):\n",
    "#      - Взвешенное усреднение (веса по Гауссу).\n",
    "#      - **Предпочтительный метод** для Гауссовского шума. Дает более гладкий результат и лучше сохраняет края, чем `cv2.blur`.\n",
    "#      - `filtered = cv2.GaussianBlur(noisy_image, (5, 5), sigmaX=0)`\n",
    "#    - Билатеральный Фильтр (`cv2.bilateralFilter`):\n",
    "#      - Учитывает и пространственную близость, и схожесть интенсивностей.\n",
    "#      - **Хорош, если нужно сохранить резкие края** при удалении Гауссовского шума.\n",
    "#      - Медленнее, чем Гауссовский фильтр.\n",
    "#      - `filtered = cv2.bilateralFilter(noisy_image, d=9, sigmaColor=75, sigmaSpace=75)`\n",
    "#    - Фильтр Non-Local Means (NLM) (`cv2.fastNlMeansDenoisingColored` / `cv2.fastNlMeansDenoising`):\n",
    "#      - Более продвинутый метод. Усредняет пиксели на основе схожести целых окрестностей (патчей), а не только отдельных пикселей.\n",
    "#      - Может дать очень хорошие результаты для Гауссовского шума, хорошо сохраняя текстуры.\n",
    "#      - Значительно медленнее предыдущих.\n",
    "#      - `filtered = cv2.fastNlMeansDenoisingColored(noisy_image, None, h=10, hColor=10, templateWindowSize=7, searchWindowSize=21)` (параметры нужно подбирать)\n",
    "\n",
    "# 2. Для Шума \"Соль и Перец\":\n",
    "#    - Медианный Фильтр (`cv2.medianBlur`):\n",
    "#      - **Наиболее эффективный и рекомендуемый метод**.\n",
    "#      - Заменяет пиксель медианой соседей, эффективно игнорируя экстремальные значения (0 и 255).\n",
    "#      - Хорошо сохраняет края.\n",
    "#      - `filtered = cv2.medianBlur(noisy_image, 5)` (размер ядра должен быть нечетным)\n",
    "#    - Линейные фильтры (Усредняющий, Гауссовский) **плохо** справляются с этим типом шума, так как они просто размазывают \"соль\" и \"перец\", создавая серые пятна.\n",
    "\n",
    "# 3. Для Пятнистого Шума:\n",
    "#    - Удаление пятнистого шума сложнее из-за его мультипликативной природы.\n",
    "#    - Логарифмическое Преобразование + Линейный Фильтр:\n",
    "#      - Иногда применяют логарифм к изображению, чтобы преобразовать мультипликативный шум в аддитивный.\n",
    "#      - Затем применяют Гауссовский или другой фильтр к логарифмированному изображению.\n",
    "#      - После фильтрации применяют экспоненциальное преобразование для возврата в исходное пространство.\n",
    "#    - Специализированные Фильтры: Фильтр Ли (Lee filter), Фильтр Фроста (Frost filter), Фильтр Куана (Kuan filter) - часто используются в обработке SAR изображений. (Менее доступны в стандартном OpenCV).\n",
    "#    - Адаптивные Фильтры: Фильтры, параметры которых меняются в зависимости от локальных характеристик изображения (например, дисперсии).\n",
    "#    - Гауссовский или Медианный фильтр могут дать *некоторое* улучшение, но не являются оптимальными.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Библиотеки (Повторение)\n",
    "\n",
    "# - **OpenCV (`cv2`)**: Основной инструмент для стандартных фильтров (`blur`, `GaussianBlur`, `medianBlur`, `bilateralFilter`, `fastNlMeansDenoising...`). Быстро и эффективно.\n",
    "# - **SciPy (`scipy.ndimage`)**: Альтернатива для `gaussian_filter`, `median_filter`, `uniform_filter`.\n",
    "# - **Scikit-image (`skimage.filters`, `skimage.restoration`)**: Предлагает Гауссовы, медианные фильтры, а также более продвинутые, как билатеральный, NLM (`denoise_nl_means`), BM3D (`denoise_wavelet` с опцией `denoise_pro` может использовать BM3D, если установлен), Total Variation Denoising (`denoise_tv_chambolle`).\n",
    "# - **Kornia**: Для GPU-ускоренной фильтрации в пайплайнах PyTorch.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Пример Задачи и Решения (Сравнение Фильтров для Разных Шумов)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Загрузить изображение. Создать две копии: одну с Гауссовским шумом, другую с шумом \"Соль и Перец\".\n",
    "# Применить Гауссовский и Медианный фильтры к *каждой* зашумленной копии.\n",
    "# Визуально сравнить результаты, чтобы увидеть, какой фильтр лучше работает для какого типа шума.\n",
    "\n",
    "# --- Решение (Полный Код) ---\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- Функции добавления шума (из Блока 2) ---\n",
    "def add_gaussian_noise(image, mean=0, sigma=25):\n",
    "    if image.dtype != np.float64: image_float = image.astype(np.float64)\n",
    "    else: image_float = image.copy()\n",
    "    noise = np.random.normal(mean, sigma, image.shape)\n",
    "    noisy_image_float = image_float + noise\n",
    "    noisy_image_clipped = np.clip(noisy_image_float, 0, 255)\n",
    "    noisy_image = noisy_image_clipped.astype(image.dtype)\n",
    "    return noisy_image\n",
    "\n",
    "def add_salt_pepper_noise(image, salt_prob=0.02, pepper_prob=0.02):\n",
    "    noisy_image = np.copy(image)\n",
    "    total_pixels = image.size\n",
    "    num_salt = int(salt_prob * total_pixels)\n",
    "    num_pepper = int(pepper_prob * total_pixels)\n",
    "    # Соль\n",
    "    salt_coords = [np.random.randint(0, i - 1, num_salt) for i in image.shape[:2]]\n",
    "    if len(image.shape) == 3: noisy_image[salt_coords[0], salt_coords[1], :] = 255\n",
    "    else: noisy_image[salt_coords[0], salt_coords[1]] = 255\n",
    "    # Перец\n",
    "    pepper_coords = [np.random.randint(0, i - 1, num_pepper) for i in image.shape[:2]]\n",
    "    if len(image.shape) == 3: noisy_image[pepper_coords[0], pepper_coords[1], :] = 0\n",
    "    else: noisy_image[pepper_coords[0], pepper_coords[1]] = 0\n",
    "    return noisy_image\n",
    "\n",
    "# --- 1. Загрузка Изображения ---\n",
    "image_path = 'path/to/your/image.jpg' # ЗАМЕНИТЕ НА ВАШ ПУТЬ\n",
    "if not os.path.exists(image_path):\n",
    "    print(f\"Ошибка: Файл не найден: {image_path}. Используется заглушка.\")\n",
    "    original_image = np.full((200, 300, 3), 128, dtype=np.uint8)\n",
    "else:\n",
    "    original_image = cv2.imread(image_path)\n",
    "    if original_image is None:\n",
    "        print(f\"Ошибка: Не удалось загрузить: {image_path}\")\n",
    "        exit()\n",
    "    print(f\"Изображение '{image_path}' загружено.\")\n",
    "\n",
    "original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# --- 2. Добавление Разных Типов Шума ---\n",
    "gaussian_noisy_image = add_gaussian_noise(original_image_rgb, sigma=30)\n",
    "sp_noisy_image = add_salt_pepper_noise(original_image_rgb, salt_prob=0.03, pepper_prob=0.03)\n",
    "print(\"Гауссовский шум и шум 'Соль и Перец' добавлены.\")\n",
    "\n",
    "# --- 3. Применение Фильтров к Обоим Типам Шума ---\n",
    "# Фильтры для Гауссовского шума\n",
    "gauss_filtered_g = cv2.GaussianBlur(gaussian_noisy_image, (5, 5), 0)\n",
    "median_filtered_g = cv2.medianBlur(gaussian_noisy_image, 5)\n",
    "\n",
    "# Фильтры для шума \"Соль и Перец\"\n",
    "gauss_filtered_sp = cv2.GaussianBlur(sp_noisy_image, (5, 5), 0)\n",
    "median_filtered_sp = cv2.medianBlur(sp_noisy_image, 5)\n",
    "print(\"Фильтры применены к зашумленным изображениям.\")\n",
    "\n",
    "# --- 4. Отображение Результатов ---\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Оригинал\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(original_image_rgb)\n",
    "plt.title('Original')\n",
    "plt.axis('off')\n",
    "\n",
    "# Результаты для Гауссовского шума\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.imshow(gaussian_noisy_image)\n",
    "plt.title('Gaussian Noise Added')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.imshow(gauss_filtered_g)\n",
    "plt.title('Gaussian Filter on Gaussian Noise')\n",
    "plt.axis('off')\n",
    "\n",
    "# Результаты для шума \"Соль и Перец\"\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.imshow(sp_noisy_image)\n",
    "plt.title('Salt & Pepper Noise Added')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.imshow(median_filtered_sp)\n",
    "plt.title('Median Filter on S&P Noise')\n",
    "plt.axis('off')\n",
    "\n",
    "# Покажем также \"неправильный\" фильтр для сравнения\n",
    "plt.subplot(2, 3, 4) # Поместим сюда результат Median на Gaussian\n",
    "plt.imshow(median_filtered_g)\n",
    "plt.title('Median Filter on Gaussian Noise')\n",
    "plt.axis('off')\n",
    "\n",
    "# Можно добавить еще один ряд или заменить пустой график\n",
    "# plt.subplot(2, 3, 4) # Пустое место или оригинал снова\n",
    "# plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Отображение результатов завершено.\")\n",
    "# Выводы из визуализации:\n",
    "# - Гауссовский фильтр хорошо сглаживает Гауссовский шум (subplot 2,3,3).\n",
    "# - Медианный фильтр отлично удаляет шум \"Соль и Перец\" (subplot 2,3,6).\n",
    "# - Медианный фильтр не очень хорошо справляется с Гауссовским шумом (subplot 2,3,4) - он может немного сгладить, но не так эффективно, как Гауссовский.\n",
    "# - Гауссовский фильтр плохо удаляет \"Соль и Перец\" - он размазывает точки шума (не показано явно, но можно добавить subplot для gauss_filtered_sp).\n",
    "\n",
    "# --- Конец Примера ---\n",
    "\n",
    "# --------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
