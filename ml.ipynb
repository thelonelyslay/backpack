{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Введение в Задачи Регрессии с Учителем (Supervised Regression)\n",
    "\n",
    "# Что такое Регрессия с Учителем?\n",
    "# Это тип задач машинного обучения с учителем (supervised learning),\n",
    "# где целью является предсказание **непрерывного** числового значения.\n",
    "# В отличие от задач классификации, где предсказывается дискретная метка класса\n",
    "# (например, \"спам\"/\"не спам\", \"кошка\"/\"собака\"), регрессия предсказывает\n",
    "# количественное значение (например, цену дома, температуру, спрос на товар).\n",
    "\n",
    "# Цель Регрессии:\n",
    "# Построить модель, которая изучает взаимосвязь между входными признаками\n",
    "# (независимыми переменными) и непрерывной выходной переменной (зависимой переменной)\n",
    "# на основе размеченных обучающих данных, а затем может предсказывать\n",
    "# это выходное значение для новых, невиданных входных данных.\n",
    "\n",
    "# Примеры Задач Регрессии:\n",
    "# - Предсказание цены дома на основе его площади, количества комнат, района.\n",
    "# - Прогнозирование температуры воздуха на завтра на основе погодных данных за сегодня.\n",
    "# - Оценка стоимости автомобиля на основе его марки, года выпуска, пробега.\n",
    "# - Предсказание спроса на продукт на основе цены, рекламных расходов, сезона.\n",
    "# - Оценка времени выполнения задачи на основе ее сложности и характеристик исполнителя.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Ключевые Концепции\n",
    "\n",
    "# 1. Признаки (Features / Независимые Переменные / Предикторы):\n",
    "#    - Входные переменные ( \\(X\\) ), которые используются для предсказания.\n",
    "#    - Могут быть числовыми (площадь, температура) или категориальными (район, марка).\n",
    "#    - Представляются как вектор или матрица признаков.\n",
    "\n",
    "# 2. Целевая Переменная (Target / Зависимая Переменная / Отклик):\n",
    "#    - Непрерывная числовая переменная ( \\(y\\) ), которую мы хотим предсказать.\n",
    "\n",
    "# 3. Обучающие Данные:\n",
    "#    - Набор примеров \\( (X_i, y_i) \\), где \\(X_i\\) - вектор признаков для i-го примера,\n",
    "#      а \\(y_i\\) - соответствующее ему истинное значение целевой переменной.\n",
    "#    - Модель \"учится\" на этих данных находить закономерности.\n",
    "\n",
    "# 4. Модель Регрессии:\n",
    "#    - Математическая функция \\( f \\), которая отображает пространство признаков \\(X\\)\n",
    "#      в пространство целевой переменной \\(y\\): \\( \\hat{y} = f(X) \\), где \\( \\hat{y} \\) - предсказанное значение.\n",
    "#    - Параметры модели настраиваются в процессе обучения для минимизации ошибки.\n",
    "\n",
    "# 5. Функции Потерь (Loss Functions):\n",
    "#    - Измеряют ошибку между предсказанным значением \\( \\hat{y}_i \\) и истинным значением \\( y_i \\).\n",
    "#    - Используются для оценки модели и в процессе обучения (минимизация потерь).\n",
    "#    - Основные функции потерь для регрессии:\n",
    "#      - **Среднеквадратичная Ошибка (Mean Squared Error - MSE):**\n",
    "#        $$ MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "#        # Сильно штрафует за большие ошибки. Чувствительна к выбросам.\n",
    "#      - **Корень из Среднеквадратичной Ошибки (Root Mean Squared Error - RMSE):**\n",
    "#        $$ RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n",
    "#        # Имеет ту же размерность, что и целевая переменная, что облегчает интерпретацию.\n",
    "#      - **Средняя Абсолютная Ошибка (Mean Absolute Error - MAE):**\n",
    "#        $$ MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
    "#        # Менее чувствительна к выбросам, чем MSE.\n",
    "#      - **Средняя Абсолютная Процентная Ошибка (Mean Absolute Percentage Error - MAPE):**\n",
    "#        $$ MAPE = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| $$\n",
    "#        # Измеряет ошибку в процентах. Не работает, если \\( y_i = 0 \\).\n",
    "#      - **Коэффициент Детерминации (R-squared / \\(R^2\\)):**\n",
    "#        $$ R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $$\n",
    "#        # Где \\( \\bar{y} \\) - среднее значение истинных \\( y_i \\).\n",
    "#        # Показывает долю дисперсии целевой переменной, объясненную моделью.\n",
    "#        # Значения от \\(-\\infty\\) до 1. Ближе к 1 - лучше модель (объясняет больше дисперсии).\n",
    "#        # 0 означает, что модель работает не лучше, чем простое предсказание среднего.\n",
    "#        # Отрицательные значения означают, что модель работает хуже среднего.\n",
    "\n",
    "# 6. Метрики Оценки Качества:\n",
    "#    - Используются для оценки производительности модели на тестовых данных.\n",
    "#    - Обычно используются те же метрики, что и функции потерь: RMSE, MAE, \\(R^2\\).\n",
    "#    - Выбор метрики зависит от задачи и того, как мы хотим интерпретировать ошибку.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Распространенные Алгоритмы Регрессии\n",
    "\n",
    "# --- 3.1 Линейные Модели ---\n",
    "# # Предполагают линейную зависимость между признаками и целевой переменной.\n",
    "# # Модель: \\( \\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + ... + w_p x_p \\)\n",
    "# # Где \\( w_i \\) - веса (коэффициенты) модели, \\( w_0 \\) - свободный член (intercept).\n",
    "\n",
    "# # - Простая Линейная Регрессия (Simple Linear Regression):\n",
    "# #   - Один входной признак (\\( \\hat{y} = w_0 + w_1 x_1 \\)).\n",
    "# # - Множественная Линейная Регрессия (Multiple Linear Regression):\n",
    "# #   - Несколько входных признаков.\n",
    "# #   - Обучение: Метод наименьших квадратов (Ordinary Least Squares - OLS) для минимизации MSE.\n",
    "# #   - Библиотека: `sklearn.linear_model.LinearRegression`\n",
    "\n",
    "# # - Полиномиальная Регрессия (Polynomial Regression):\n",
    "# #   - Моделирует нелинейные зависимости путем добавления полиномиальных признаков\n",
    "# #     (например, \\( x^2, x^3, x_1 x_2 \\)) к исходным данным, а затем применяет линейную регрессию.\n",
    "# #   - Библиотека: `sklearn.preprocessing.PolynomialFeatures` + `LinearRegression`.\n",
    "\n",
    "# # - Гребневая Регрессия (Ridge Regression):\n",
    "# #   - Линейная регрессия с L2-регуляризацией (добавляет штраф к функции потерь, пропорциональный квадрату нормы весов).\n",
    "# #   - Помогает бороться с мультиколлинеарностью (сильной корреляцией между признаками) и переобучением. Уменьшает величину коэффициентов.\n",
    "# #   - Библиотека: `sklearn.linear_model.Ridge`\n",
    "\n",
    "# # - Лассо Регрессия (Lasso Regression):\n",
    "# #   - Линейная регрессия с L1-регуляризацией (штраф пропорционален абсолютной величине весов).\n",
    "# #   - Может обнулять веса некоторых признаков, выполняя таким образом отбор признаков.\n",
    "# #   - Библиотека: `sklearn.linear_model.Lasso`\n",
    "\n",
    "# # - Elastic Net:\n",
    "# #   - Комбинация L1 и L2 регуляризации.\n",
    "# #   - Библиотека: `sklearn.linear_model.ElasticNet`\n",
    "\n",
    "# --- 3.2 Метод Опорных Векторов для Регрессии (Support Vector Regression - SVR) ---\n",
    "# # Адаптация SVM для задач регрессии.\n",
    "# # Идея: Найти гиперплоскость, которая аппроксимирует данные так, чтобы максимальное\n",
    "# # количество точек попадало в \"трубку\" (margin) определенной ширины \\( \\epsilon \\) вокруг нее.\n",
    "# # Штрафуются только точки, выходящие за пределы трубки.\n",
    "# # Может моделировать нелинейные зависимости с помощью ядер (kernel trick: 'linear', 'poly', 'rbf').\n",
    "# # Чувствителен к масштабированию признаков.\n",
    "# # Библиотека: `sklearn.svm.SVR`\n",
    "\n",
    "# --- 3.3 Деревья Решений для Регрессии (Decision Tree Regressor) ---\n",
    "# # Строит дерево, где каждый узел представляет собой проверку значения признака,\n",
    "# # а каждый лист содержит предсказание (обычно среднее значение целевой переменной\n",
    "# # для всех обучающих примеров, попавших в этот лист).\n",
    "# # Легко интерпретируется, не требует масштабирования признаков.\n",
    "# # Склонен к переобучению.\n",
    "# # Библиотека: `sklearn.tree.DecisionTreeRegressor`\n",
    "\n",
    "# --- 3.4 Ансамблевые Методы (Деревянные) ---\n",
    "# # Комбинируют предсказания нескольких деревьев для повышения точности и устойчивости.\n",
    "\n",
    "# # - Случайный Лес для Регрессии (Random Forest Regressor):\n",
    "# #   - Строит множество деревьев решений на случайных подвыборках данных и признаков.\n",
    "# #   - Предсказание - усредненное предсказание всех деревьев.\n",
    "# #   - Устойчив к переобучению, не требует масштабирования. Мощный алгоритм.\n",
    "# #   - Библиотека: `sklearn.ensemble.RandomForestRegressor`\n",
    "\n",
    "# # - Градиентный Бустинг (Gradient Boosting Machines - GBM):\n",
    "# #   - Строит деревья последовательно, каждое следующее дерево пытается исправить ошибки предыдущих.\n",
    "# #   - Очень мощный и точный метод, но может быть чувствителен к настройке гиперпараметров и склонен к переобучению при большом количестве деревьев.\n",
    "# #   - Библиотеки:\n",
    "# #     - `sklearn.ensemble.GradientBoostingRegressor` (классический GBM)\n",
    "# #     - `xgboost.XGBRegressor` (XGBoost - оптимизированная реализация GBM с регуляризацией)\n",
    "# #     - `lightgbm.LGBMRegressor` (LightGBM - быстрая реализация GBM, хорошо работает на больших данных)\n",
    "# #     - `catboost.CatBoostRegressor` (CatBoost - хорошо обрабатывает категориальные признаки, устойчив к переобучению)\n",
    "\n",
    "# --- 3.5 Нейронные Сети для Регрессии ---\n",
    "# # Многослойные перцептроны (MLP) или другие архитектуры нейронных сетей могут\n",
    "# # использоваться для регрессии.\n",
    "# # Выходной слой обычно имеет один нейрон с линейной функцией активации (или без активации).\n",
    "# # Функция потерь - MSE или MAE.\n",
    "# # Могут моделировать очень сложные нелинейные зависимости.\n",
    "# # Требуют много данных, тщательной настройки архитектуры и гиперпараметров, масштабирования признаков.\n",
    "# # Библиотеки: PyTorch (`torch.nn`), TensorFlow/Keras (`tf.keras`).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Подготовка Данных для Регрессии\n",
    "\n",
    "# Качество данных и их подготовка критически важны для регрессионных моделей.\n",
    "\n",
    "# 1. Обработка Пропущенных Значений:\n",
    "#    - Удаление строк/столбцов с пропусками (если их мало).\n",
    "#    - Заполнение пропусков (Imputation):\n",
    "#      - Средним/медианой/модой (для числовых/категориальных).\n",
    "#      - Более сложными методами (например, предсказание пропусков с помощью другой модели).\n",
    "#    - `sklearn.impute.SimpleImputer`, `sklearn.impute.KNNImputer`.\n",
    "\n",
    "# 2. Кодирование Категориальных Признаков:\n",
    "#    - Преобразование текстовых категорий в числа.\n",
    "#    - **One-Hot Encoding:** Создает бинарные столбцы для каждой категории. Подходит для большинства алгоритмов. `sklearn.preprocessing.OneHotEncoder`.\n",
    "#    - **Label Encoding:** Присваивает каждой категории число (0, 1, 2...). **Осторожно:** Может внести ложный порядок, не подходит для линейных моделей или моделей, основанных на расстоянии, но может использоваться для древовидных моделей. `sklearn.preprocessing.LabelEncoder`.\n",
    "#    - Другие методы: Target Encoding, Frequency Encoding.\n",
    "\n",
    "# 3. Масштабирование Числовых Признаков (Feature Scaling):\n",
    "#    - **Критически важно** для алгоритмов, чувствительных к масштабу (Линейная регрессия, Ridge, Lasso, SVR, Нейронные сети, KNN). Менее важно для древовидных моделей.\n",
    "#    - **Стандартизация (Standardization):** Приводит данные к нулевому среднему и единичной дисперсии (\\( (X - \\mu) / \\sigma \\)). `sklearn.preprocessing.StandardScaler`.\n",
    "#    - **Нормализация (Normalization / Min-Max Scaling):** Масштабирует данные в заданный диапазон, обычно [0, 1] (\\( (X - X_{min}) / (X_{max} - X_{min}) \\)). `sklearn.preprocessing.MinMaxScaler`.\n",
    "\n",
    "# 4. Инжиниринг Признаков (Feature Engineering):\n",
    "#    - Создание новых признаков из существующих для улучшения модели.\n",
    "#    - Примеры:\n",
    "#      - Полиномиальные признаки (для моделирования нелинейности).\n",
    "#      - Взаимодействия признаков ( \\(x_1 * x_2\\) ).\n",
    "#      - Извлечение компонентов даты/времени (час, день недели, месяц).\n",
    "#      - Агрегация данных.\n",
    "\n",
    "# 5. Разделение Данных (Train/Test Split):\n",
    "#    - Разделение исходного набора данных на обучающую и тестовую выборки.\n",
    "#    - Обучающая выборка используется для тренировки модели.\n",
    "#    - Тестовая выборка используется для финальной оценки производительности модели на невиданных данных.\n",
    "#    - `sklearn.model_selection.train_test_split`.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Оценка Модели и Выбор\n",
    "\n",
    "# 1. Использование Метрик:\n",
    "#    - Рассчитайте выбранные метрики ( \\(R^2\\), MAE, MSE, RMSE) на тестовой выборке.\n",
    "#    - `sklearn.metrics`: `r2_score`, `mean_absolute_error`, `mean_squared_error`.\n",
    "#    - Интерпретируйте метрики в контексте задачи (например, MAE в $5000 для цен на дома может быть приемлемым, а для цен на продукты - нет).\n",
    "\n",
    "# 2. Кросс-Валидация (Cross-Validation):\n",
    "#    - Более надежный способ оценки модели, чем однократное разделение train/test.\n",
    "#    - Данные делятся на K фолдов (частей). Модель обучается K раз, каждый раз используя K-1 фолд для обучения и 1 фолд для валидации.\n",
    "#    - Метрики усредняются по K итерациям.\n",
    "#    - Помогает оценить стабильность модели и избежать случайного удачного/неудачного разделения.\n",
    "#    - `sklearn.model_selection.cross_val_score`, `sklearn.model_selection.KFold`.\n",
    "\n",
    "# 3. Настройка Гиперпараметров (Hyperparameter Tuning):\n",
    "#    - Большинство моделей имеют гиперпараметры, которые не обучаются на данных, а задаются заранее (например, `alpha` в Ridge/Lasso, `C` и `epsilon` в SVR, глубина дерева, количество деревьев в ансамблях).\n",
    "#    - Подбор оптимальных гиперпараметров улучшает производительность модели.\n",
    "#    - Методы:\n",
    "#      - **Grid Search:** Перебирает все возможные комбинации заданных гиперпараметров. `sklearn.model_selection.GridSearchCV`.\n",
    "#      - **Randomized Search:** Случайным образом выбирает комбинации гиперпараметров из заданных диапазонов. Часто эффективнее Grid Search. `sklearn.model_selection.RandomizedSearchCV`.\n",
    "#    - Используют кросс-валидацию для оценки каждой комбинации гиперпараметров.\n",
    "\n",
    "# 4. Анализ Остатков (Residual Analysis):\n",
    "#    - Остатки - это разница между истинными и предсказанными значениями (\\( e_i = y_i - \\hat{y}_i \\)).\n",
    "#    - Анализ распределения остатков помогает проверить предположения модели (особенно линейной регрессии) и выявить проблемы.\n",
    "#    - Идеально: остатки должны быть случайно распределены вокруг нуля без видимых паттернов.\n",
    "#    - Построение графика остатков против предсказанных значений или признаков.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Пример Задачи и Решения (Линейная Регрессия с Scikit-learn)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Предсказать значение Y на основе одного признака X,\n",
    "# используя простую линейную регрессию. Данные сгенерируем синтетически.\n",
    "\n",
    "# --- Решение (Полный Код) ---\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler # Хотя для простой лин. регрессии не строго обязательно\n",
    "\n",
    "# 1. Генерация Синтетических Данных\n",
    "np.random.seed(42) # для воспроизводимости\n",
    "X = 2 * np.random.rand(100, 1) # 100 примеров, 1 признак (от 0 до 2)\n",
    "# Истинная зависимость y = 4 + 3*X + шум (Гауссовский)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# print(\"Shape of X:\", X.shape)\n",
    "# print(\"Shape of y:\", y.shape)\n",
    "# print(\"Sample X:\", X[:5])\n",
    "# print(\"Sample y:\", y[:5])\n",
    "\n",
    "# Визуализация данных\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(X, y)\n",
    "# plt.title(\"Синтетические данные для регрессии\")\n",
    "# plt.xlabel(\"Признак X\")\n",
    "# plt.ylabel(\"Целевая переменная Y\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# 2. Разделение Данных\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# print(f\"Train samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# 3. Масштабирование Признаков (Хорошая практика, хотя здесь не критично)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "# # Используем немасштабированные для простоты интерпретации коэффициентов в этом примере\n",
    "X_train_scaled = X_train\n",
    "X_test_scaled = X_test\n",
    "\n",
    "\n",
    "# 4. Создание и Обучение Модели Линейной Регрессии\n",
    "lin_reg = LinearRegression()\n",
    "# print(\"\\nTraining Linear Regression model...\")\n",
    "lin_reg.fit(X_train_scaled, y_train)\n",
    "# print(\"Training complete.\")\n",
    "\n",
    "# Вывод коэффициентов модели\n",
    "# print(f\"Intercept (w0): {lin_reg.intercept_[0]:.4f}\") # Ожидаем около 4\n",
    "# print(f\"Coefficient (w1): {lin_reg.coef_[0][0]:.4f}\") # Ожидаем около 3\n",
    "\n",
    "# 5. Предсказание на Тестовой Выборке\n",
    "y_pred = lin_reg.predict(X_test_scaled)\n",
    "\n",
    "# 6. Оценка Модели\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"R-squared (R2): {r2:.4f}\") # Должен быть близок к 1\n",
    "\n",
    "# 7. Визуализация Результатов\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(X_test, y_test, label='Actual Data')\n",
    "# plt.plot(X_test, y_pred, color='red', linewidth=2, label='Linear Regression Fit')\n",
    "# plt.title(\"Линейная Регрессия - Предсказания vs Истинные значения\")\n",
    "# plt.xlabel(\"Признак X\")\n",
    "# plt.ylabel(\"Целевая переменная Y\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# 8. Предсказание на Новом Значении\n",
    "# X_new = np.array([[1.5]]) # Новое значение признака X = 1.5\n",
    "# X_new_scaled = X_new # scaler.transform(X_new) # Масштабируем, если использовали scaler\n",
    "# y_new_pred = lin_reg.predict(X_new_scaled)\n",
    "# print(f\"\\nPrediction for X = {X_new[0][0]}: {y_new_pred[0][0]:.4f}\")\n",
    "# Ожидаемое значение: 4 + 3 * 1.5 = 8.5 (плюс/минус шум)\n",
    "\n",
    "# --- Конец Примера ---\n",
    "\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Введение в Деревья Решений (Decision Trees)\n",
    "\n",
    "# Что такое Дерево Решений?\n",
    "# Дерево решений - это непараметрический алгоритм машинного обучения с учителем,\n",
    "# который используется как для задач классификации, так и для регрессии.\n",
    "# Модель представляет собой древовидную структуру, где:\n",
    "# - Внутренние узлы (Internal Nodes): Представляют проверку значения некоторого признака.\n",
    "# - Ветви (Branches): Представляют результат этой проверки (например, \"признак > порога\" или \"признак <= порога\").\n",
    "# - Листовые узлы (Leaf Nodes / Terminal Nodes): Содержат конечный результат (предсказание).\n",
    "\n",
    "# Деревья Решений для Регрессии:\n",
    "# В отличие от классификационных деревьев, которые предсказывают метку класса в листьях,\n",
    "# регрессионные деревья предсказывают **непрерывное числовое значение**.\n",
    "# Предсказание в листовом узле обычно является **средним** (или медианным) значением\n",
    "# целевой переменной для всех обучающих примеров, которые попали в этот лист.\n",
    "\n",
    "# Как Строится Дерево (Интуиция):\n",
    "# Алгоритм рекурсивно разбивает пространство признаков на все меньшие и меньшие\n",
    "# прямоугольные области. На каждом шаге выбирается такой признак и такой порог\n",
    "# для разбиения, которые наилучшим образом разделяют данные с точки зрения\n",
    "# \"чистоты\" получаемых дочерних узлов (для регрессии - минимизации дисперсии\n",
    "# или среднеквадратичной ошибки целевой переменной внутри узлов).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Ключевые Концепции Регрессионных Деревьев\n",
    "\n",
    "# 1. Рекурсивное Бинарное Разбиение (Recursive Binary Splitting):\n",
    "#    - Процесс начинается с корневого узла, содержащего все обучающие данные.\n",
    "#    - На каждом шаге выбирается один признак \\(j\\) и порог \\(s\\).\n",
    "#    - Данные в узле разделяются на две группы:\n",
    "#      - Левая ветвь: Примеры, у которых значение признака \\(j\\) меньше или равно \\(s\\).\n",
    "#      - Правая ветвь: Примеры, у которых значение признака \\(j\\) больше \\(s\\).\n",
    "#    - Этот процесс повторяется для дочерних узлов.\n",
    "\n",
    "# 2. Критерий Разбиения (Splitting Criterion):\n",
    "#    - Цель: Найти такой признак \\(j\\) и порог \\(s\\), которые минимизируют\n",
    "#      некоторую меру \"нечистоты\" (impurity) или ошибки в дочерних узлах.\n",
    "#    - Для регрессии обычно используются:\n",
    "#      - **Уменьшение Среднеквадратичной Ошибки (MSE Reduction) / Уменьшение Дисперсии:**\n",
    "#        Самый распространенный критерий. Выбирается разбиение, которое максимизирует\n",
    "#        разницу между MSE родительского узла и взвешенной суммой MSE дочерних узлов.\n",
    "#        $$ \\text{Gain} = MSE_{parent} - \\left( \\frac{N_{left}}{N_{parent}} MSE_{left} + \\frac{N_{right}}{N_{parent}} MSE_{right} \\right) $$\n",
    "#        Где \\( MSE_{node} = \\frac{1}{N_{node}} \\sum_{i \\in node} (y_i - \\bar{y}_{node})^2 \\), а \\( \\bar{y}_{node} \\) - среднее \\(y\\) в узле.\n",
    "#      - **Уменьшение Средней Абсолютной Ошибки (MAE Reduction / Friedman MAE):**\n",
    "#        Альтернативный критерий, менее чувствительный к выбросам.\n",
    "#    - Алгоритм перебирает все возможные признаки и пороги для нахождения лучшего разбиения.\n",
    "\n",
    "# 3. Критерии Остановки (Stopping Criteria):\n",
    "#    - Определяют, когда прекратить дальнейшее разбиение узла (и сделать его листом).\n",
    "#    - Предотвращают переобучение (когда дерево становится слишком сложным и подгоняется под шум в данных).\n",
    "#    - Распространенные критерии:\n",
    "#      - `max_depth`: Максимальная глубина дерева.\n",
    "#      - `min_samples_split`: Минимальное количество примеров, необходимое для дальнейшего разбиения узла.\n",
    "#      - `min_samples_leaf`: Минимальное количество примеров, которое должно остаться в листовом узле после разбиения.\n",
    "#      - `max_leaf_nodes`: Максимальное количество листовых узлов.\n",
    "#      - `min_impurity_decrease`: Минимальное уменьшение критерия нечистоты, которое должно дать разбиение.\n",
    "\n",
    "# 4. Предсказание (Prediction):\n",
    "#    - Для нового примера данных: Начинаем с корневого узла.\n",
    "#    - На каждом внутреннем узле проверяем значение соответствующего признака примера.\n",
    "#    - Переходим по левой или правой ветви в зависимости от результата проверки.\n",
    "#    - Повторяем, пока не достигнем листового узла.\n",
    "#    - Предсказанное значение - это значение, хранящееся в этом листовом узле (обычно среднее \\(y\\) обучающих примеров в этом листе).\n",
    "#    - Важно: Предсказания дерева решений являются **кусочно-постоянными**.\n",
    "\n",
    "# 5. Обрезка Дерева (Pruning):\n",
    "#    - Техника для уменьшения сложности дерева и борьбы с переобучением *после* того, как оно построено (post-pruning).\n",
    "#    - Идея: Удалить (обрезать) некоторые ветви (узлы), которые дают наименьший прирост качества на валидационных данных или на основе критерия сложности.\n",
    "#    - Cost Complexity Pruning (Минимальная стоимость-сложность обрезки): Использует параметр `ccp_alpha` в Scikit-learn. Большие значения `ccp_alpha` приводят к большей обрезке.\n",
    "\n",
    "# 6. Важность Признаков (Feature Importance):\n",
    "#    - Деревья решений позволяют оценить, какие признаки были наиболее важны для построения модели.\n",
    "#    - Рассчитывается на основе того, насколько сильно каждый признак уменьшал критерий нечистоты (MSE) при разбиениях, взвешенно по количеству примеров в узлах.\n",
    "#    - Доступно через атрибут `feature_importances_` в Scikit-learn.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Преимущества и Недостатки\n",
    "\n",
    "# Преимущества:\n",
    "# + **Интерпретируемость:** Легко понять и визуализировать логику принятия решений.\n",
    "# + **Не требует масштабирования:** Нечувствительны к масштабу числовых признаков.\n",
    "# + **Обработка разных типов данных:** Могут работать с числовыми и (теоретически) категориальными признаками (хотя sklearn требует числового входа).\n",
    "# + **Нелинейность:** Способны улавливать нелинейные зависимости в данных.\n",
    "# + **Автоматический отбор признаков:** Неявно выполняют отбор признаков, используя наиболее информативные из них для разбиений.\n",
    "# + **Относительно быстрые:** Обучение и предсказание обычно быстрые.\n",
    "\n",
    "# Недостатки:\n",
    "# - **Склонность к переобучению:** Глубокие деревья могут легко переобучиться на обучающих данных, плохо обобщаясь на новые. Требуется контроль сложности (глубина, обрезка).\n",
    "# - **Нестабильность:** Небольшие изменения в данных могут привести к построению совершенно другого дерева. (Проблема решается ансамблями).\n",
    "# - **Кусочно-постоянные предсказания:** Модель предсказывает одно и то же значение для всех точек в пределах одной области (листа), что может быть нежелательно для гладких зависимостей.\n",
    "# - **Жадный алгоритм:** На каждом шаге выбирается локально оптимальное разбиение, что не гарантирует глобально оптимального дерева.\n",
    "# - **Сложность захвата некоторых зависимостей:** Могут испытывать трудности с моделированием некоторых простых линейных зависимостей, если оси не выровнены.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Алгоритм Построения (Концептуально)\n",
    "\n",
    "# 1. Начать с корневого узла, содержащего все обучающие примеры \\( (X, y) \\).\n",
    "# 2. Для каждого узла:\n",
    "#    a. Если выполнен критерий остановки (например, достигнута `max_depth`, или в узле мало примеров `min_samples_leaf`, или узел \"чистый\" - все \\(y\\) одинаковы):\n",
    "#       i. Сделать узел листовым.\n",
    "#       ii. Сохранить в листе предсказываемое значение (например, среднее \\(y\\) примеров в этом узле).\n",
    "#    b. Иначе (если узел можно разбивать дальше):\n",
    "#       i. Найти лучший признак \\(j\\) и лучший порог \\(s\\) для разбиения данных в узле, максимизируя уменьшение MSE (или другого критерия).\n",
    "#       ii. Разделить данные узла на два подмножества: \\(D_{left} = \\{(X_i, y_i) | X_{ij} \\le s\\}\\) и \\(D_{right} = \\{(X_i, y_i) | X_{ij} > s\\}\\).\n",
    "#       iii. Рекурсивно вызвать шаг 2 для левого дочернего узла с данными \\(D_{left}\\).\n",
    "#       iv. Рекурсивно вызвать шаг 2 для правого дочернего узла с данными \\(D_{right}\\).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Реализация в Scikit-learn\n",
    "\n",
    "# Основной класс: `sklearn.tree.DecisionTreeRegressor`\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Инициализация модели с некоторыми параметрами:\n",
    "# tree_reg = DecisionTreeRegressor(\n",
    "#     criterion='squared_error', # Критерий: 'squared_error' (MSE), 'friedman_mse', 'absolute_error' (MAE), 'poisson'\n",
    "#     splitter='best',        # Стратегия выбора разбиения: 'best' или 'random'\n",
    "#     max_depth=None,         # Максимальная глубина (None - без ограничения)\n",
    "#     min_samples_split=2,    # Минимальное число образцов для сплита\n",
    "#     min_samples_leaf=1,     # Минимальное число образцов в листе\n",
    "#     max_features=None,      # Число признаков для поиска лучшего сплита (None - все)\n",
    "#     random_state=None,      # Для воспроизводимости\n",
    "#     max_leaf_nodes=None,    # Макс. число листьев\n",
    "#     min_impurity_decrease=0.0, # Мин. уменьшение нечистоты для сплита\n",
    "#     ccp_alpha=0.0           # Параметр для Cost Complexity Pruning\n",
    "# )\n",
    "\n",
    "# Обучение:\n",
    "# tree_reg.fit(X_train, y_train)\n",
    "\n",
    "# Предсказание:\n",
    "# y_pred = tree_reg.predict(X_test)\n",
    "\n",
    "# Важность признаков:\n",
    "# importances = tree_reg.feature_importances_\n",
    "\n",
    "# Визуализация дерева (требует graphviz и matplotlib):\n",
    "# from sklearn.tree import plot_tree\n",
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plot_tree(tree_reg, filled=True, feature_names=feature_names_list, rounded=True, fontsize=10)\n",
    "# plt.show()\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Примеры Задач Регрессии для Деревьев Решений\n",
    "\n",
    "# Деревья решений подходят для задач регрессии, где:\n",
    "# - Есть нелинейные зависимости между признаками и целью.\n",
    "# - Важна интерпретируемость модели (можно посмотреть на правила в дереве).\n",
    "# - Не хочется тратить время на масштабирование признаков.\n",
    "# - Данные содержат как числовые, так и категориальные признаки (хотя sklearn требует их кодирования).\n",
    "\n",
    "# Примеры:\n",
    "# 1. **Предсказание Цены Недвижимости:** На основе площади, числа комнат, района (категориальный), года постройки. Дерево может выучить правила вроде \"Если район='Центр' И площадь > 100, то цена = X, иначе ...\".\n",
    "# 2. **Оценка Спроса на Товар:** На основе дня недели, времени суток, наличия промо-акции, погоды. Дерево может разделить данные по этим условиям.\n",
    "# 3. **Прогнозирование Количества Арендованных Велосипедов:** На основе часа, дня недели, сезона, погоды (температура, влажность, ветер).\n",
    "# 4. **Оценка Возраста Человека по Физическим Показателям:** (Хотя здесь могут быть и более гладкие зависимости).\n",
    "# 5. **Предсказание Времени Выполнения Задачи:** На основе типа задачи, приоритета, опыта исполнителя.\n",
    "\n",
    "# Важно помнить, что для повышения точности и стабильности часто используют ансамбли деревьев (Random Forest, Gradient Boosting), которые строятся на основе DecisionTreeRegressor.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 7: Пример Задачи и Решения (Предсказание Синусоиды)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Обучить DecisionTreeRegressor предсказывать значение функции \\( y = \\sin(x) \\)\n",
    "# с добавлением шума на отрезке \\( [0, 10] \\). Это покажет, как дерево\n",
    "# аппроксимирует нелинейную функцию кусочно-постоянными значениями.\n",
    "\n",
    "# --- Решение (Полный Код) ---\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 1. Генерация Синтетических Данных\n",
    "np.random.seed(42)\n",
    "X = np.sort(10 * np.random.rand(100, 1), axis=0) # 100 точек от 0 до 10\n",
    "y = np.sin(X).ravel() + np.random.randn(100) * 0.1 # sin(X) + шум\n",
    "\n",
    "# print(\"Shape of X:\", X.shape)\n",
    "# print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# 2. Разделение Данных\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "# print(f\"Train samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# 3. Создание и Обучение Модели Дерева Решений\n",
    "# Обучим два дерева: одно неглубокое, другое глубокое (для демонстрации переобучения)\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "tree_reg_deep = DecisionTreeRegressor(random_state=42) # Без ограничения глубины\n",
    "\n",
    "# print(\"\\nTraining Decision Tree (max_depth=2)...\")\n",
    "tree_reg1.fit(X_train, y_train)\n",
    "# print(\"Training complete.\")\n",
    "\n",
    "# print(\"\\nTraining Decision Tree (max_depth=5)...\")\n",
    "tree_reg2.fit(X_train, y_train)\n",
    "# print(\"Training complete.\")\n",
    "\n",
    "# print(\"\\nTraining Decision Tree (no depth limit)...\")\n",
    "tree_reg_deep.fit(X_train, y_train)\n",
    "# print(\"Training complete.\")\n",
    "\n",
    "\n",
    "# 4. Предсказание на Тестовой Выборке\n",
    "y_pred1 = tree_reg1.predict(X_test)\n",
    "y_pred2 = tree_reg2.predict(X_test)\n",
    "y_pred_deep = tree_reg_deep.predict(X_test)\n",
    "\n",
    "# 5. Оценка Моделей\n",
    "print(\"\\n--- Evaluation (max_depth=2) ---\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred1):.4f}\")\n",
    "print(f\"R2 Score: {r2_score(y_test, y_pred1):.4f}\")\n",
    "\n",
    "print(\"\\n--- Evaluation (max_depth=5) ---\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred2):.4f}\")\n",
    "print(f\"R2 Score: {r2_score(y_test, y_pred2):.4f}\") # Должен быть лучше, чем у depth=2\n",
    "\n",
    "print(\"\\n--- Evaluation (no depth limit) ---\")\n",
    "print(f\"MSE: {mean_squared_error(y_test, y_pred_deep):.4f}\") # Может быть хуже из-за переобучения\n",
    "print(f\"R2 Score: {r2_score(y_test, y_pred_deep):.4f}\")\n",
    "\n",
    "# 6. Визуализация Результатов\n",
    "X_plot = np.arange(0.0, 10.0, 0.01)[:, np.newaxis] # Плотная сетка для отрисовки предсказаний\n",
    "y_pred_plot1 = tree_reg1.predict(X_plot)\n",
    "y_pred_plot2 = tree_reg2.predict(X_plot)\n",
    "y_pred_plot_deep = tree_reg_deep.predict(X_plot)\n",
    "\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.scatter(X_train, y_train, s=20, edgecolor=\"black\", c=\"darkorange\", label=\"Training data\")\n",
    "# plt.scatter(X_test, y_test, s=20, edgecolor=\"black\", c=\"cornflowerblue\", label=\"Test data\", alpha=0.8)\n",
    "# plt.plot(X_plot, y_pred_plot1, color=\"yellowgreen\", label=\"Prediction (max_depth=2)\", linewidth=2)\n",
    "# plt.plot(X_plot, y_pred_plot2, color=\"red\", label=\"Prediction (max_depth=5)\", linewidth=2, linestyle='--')\n",
    "# # plt.plot(X_plot, y_pred_plot_deep, color=\"purple\", label=\"Prediction (no limit)\", linewidth=1, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Ансамблевые Методы на Основе Деревьев\n",
    "\n",
    "# Идея Ансамблей:\n",
    "# Вместо того чтобы полагаться на одну модель (например, одно дерево решений),\n",
    "# ансамблевые методы комбинируют предсказания **нескольких** моделей (часто деревьев)\n",
    "# для получения более точного, стабильного и надежного итогового предсказания.\n",
    "# Комбинирование помогает уменьшить недостатки отдельных моделей (например,\n",
    "# склонность деревьев к переобучению и их нестабильность).\n",
    "\n",
    "# Основные Типы Ансамблей Деревьев:\n",
    "# 1. Методы Усреднения (Averaging Methods):\n",
    "#    - Строят несколько независимых моделей (деревьев) и усредняют их предсказания.\n",
    "#    - Основная цель: Уменьшить **дисперсию** (variance) модели.\n",
    "#    - Пример: Случайный Лес (Random Forest).\n",
    "# 2. Методы Усиления (Boosting Methods):\n",
    "#    - Строят модели последовательно, каждая следующая модель пытается исправить\n",
    "#      ошибки предыдущих.\n",
    "#    - Основная цель: Уменьшить **смещение** (bias) модели, но также могут уменьшать и дисперсию.\n",
    "#    - Примеры: Градиентный Бустинг (GBM), XGBoost, LightGBM, CatBoost.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Бэггинг и Случайный Лес (Random Forest)\n",
    "\n",
    "# Бэггинг (Bagging - Bootstrap Aggregating):\n",
    "# 1. Создается множество (N) обучающих подвыборок из исходного датасета с помощью\n",
    "#    **бутстрэпа** (bootstrap sampling) - выборки с возвращением. Каждая подвыборка\n",
    "#    имеет тот же размер, что и исходная, но некоторые примеры могут повторяться,\n",
    "#    а некоторые - отсутствовать.\n",
    "# 2. На каждой подвыборке обучается своя независимая базовая модель (например, дерево решений).\n",
    "# 3. Предсказания всех N моделей усредняются (для регрессии) или определяется\n",
    "#    наиболее частый класс (для классификации).\n",
    "\n",
    "# Случайный Лес (Random Forest):\n",
    "# - Это усовершенствованный бэггинг, специально для деревьев решений.\n",
    "# - Дополнительно к бутстрэпу данных, при построении каждого узла дерева\n",
    "#   выбирается **случайное подмножество признаков** (`max_features`), и лучшее\n",
    "#   разбиение ищется только среди этих признаков.\n",
    "# - Это вносит дополнительную случайность и уменьшает корреляцию между деревьями,\n",
    "#   что еще сильнее снижает дисперсию ансамбля.\n",
    "\n",
    "# Преимущества Random Forest:\n",
    "# + Высокая точность и стабильность.\n",
    "# + Устойчивость к переобучению (гораздо лучше, чем одно дерево).\n",
    "# + Не требует масштабирования признаков.\n",
    "# + Хорошо работает \"из коробки\" с настройками по умолчанию.\n",
    "# + Позволяет оценить важность признаков.\n",
    "# + Легко параллелизуется.\n",
    "\n",
    "# Недостатки Random Forest:\n",
    "# - Менее интерпретируемый, чем одно дерево (\"черный ящик\").\n",
    "# - Может быть медленнее и требовать больше памяти, чем одно дерево или бустинг.\n",
    "# - Может не очень хорошо экстраполировать за пределы диапазона обучающих данных.\n",
    "\n",
    "# Реализация в Scikit-learn:\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# rf_reg = RandomForestRegressor(\n",
    "#     n_estimators=100,      # Количество деревьев в лесу\n",
    "#     criterion='squared_error', # Критерий для регрессии (MSE)\n",
    "#     max_depth=None,        # Макс. глубина каждого дерева\n",
    "#     min_samples_split=2,   # Мин. число образцов для сплита\n",
    "#     min_samples_leaf=1,    # Мин. число образцов в листе\n",
    "#     max_features='sqrt',   # Число признаков для сплита ('sqrt', 'log2', float, int)\n",
    "#     bootstrap=True,        # Использовать ли бутстрэп\n",
    "#     oob_score=False,       # Считать ли Out-of-Bag ошибку (оценка на неиспользованных данных)\n",
    "#     n_jobs=-1,             # Использовать все доступные ядра CPU\n",
    "#     random_state=None\n",
    "# )\n",
    "# rf_reg.fit(X_train, y_train)\n",
    "# y_pred_rf = rf_reg.predict(X_test)\n",
    "# importances_rf = rf_reg.feature_importances_\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Градиентный Бустинг (Gradient Boosting Machines - GBM)\n",
    "\n",
    "# Принцип Работы:\n",
    "# 1. Начинаем с простой базовой модели (часто просто среднее значение целевой переменной).\n",
    "# 2. Итеративно строим новые модели (обычно неглубокие деревья решений):\n",
    "#    a. Вычисляем **остатки** (residuals) - разницу между истинными значениями и\n",
    "#       предсказаниями текущего ансамбля. Для MSE остатки совпадают с отрицательным\n",
    "#       градиентом функции потерь.\n",
    "#    b. Обучаем новое дерево предсказывать эти остатки (или отрицательный градиент).\n",
    "#    c. Добавляем предсказания нового дерева к предсказаниям ансамбля, умноженные\n",
    "#       на небольшой коэффициент - **скорость обучения** (`learning_rate` или `eta`).\n",
    "#       Этот коэффициент (shrinkage) уменьшает вклад каждого дерева и помогает\n",
    "#       предотвратить переобучение.\n",
    "# 3. Повторяем шаг 2 заданное количество раз (`n_estimators`).\n",
    "\n",
    "# Ключевые Гиперпараметры GBM:\n",
    "# - `n_estimators`: Количество деревьев (итераций бустинга). Слишком большое значение может привести к переобучению.\n",
    "# - `learning_rate` (eta): Скорость обучения (усадка). Меньшие значения (например, 0.01-0.1) требуют большего `n_estimators`, но обычно дают лучшие результаты и более устойчивы к переобучению.\n",
    "# - `max_depth`: Максимальная глубина отдельных деревьев. Обычно используются неглубокие деревья (например, 3-8).\n",
    "# - `subsample`: Доля обучающих примеров, используемая для обучения каждого дерева (стохастический градиентный бустинг). Значение < 1.0 вносит случайность и помогает бороться с переобучением.\n",
    "# - `min_samples_split`, `min_samples_leaf`: Параметры для контроля сложности отдельных деревьев.\n",
    "\n",
    "# Преимущества GBM:\n",
    "# + Часто обеспечивает очень высокую точность предсказаний.\n",
    "# + Гибкость в выборе функции потерь (не только MSE).\n",
    "\n",
    "# Недостатки GBM:\n",
    "# - Более чувствителен к настройке гиперпараметров, чем Random Forest.\n",
    "# - Склонен к переобучению, если параметры не настроены должным образом (особенно `n_estimators` и `learning_rate`).\n",
    "# - Обучение происходит последовательно, что затрудняет параллелизацию (хотя построение отдельных деревьев может быть распараллелено).\n",
    "\n",
    "# Реализация в Scikit-learn:\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# gbm_reg = GradientBoostingRegressor(\n",
    "#     loss='squared_error',   # Функция потерь ('squared_error', 'absolute_error', 'huber', 'quantile')\n",
    "#     learning_rate=0.1,\n",
    "#     n_estimators=100,\n",
    "#     subsample=1.0,\n",
    "#     criterion='friedman_mse', # Критерий для оценки качества сплита\n",
    "#     min_samples_split=2,\n",
    "#     min_samples_leaf=1,\n",
    "#     max_depth=3,            # Обычно неглубокие деревья\n",
    "#     random_state=None,\n",
    "#     max_features=None,\n",
    "#     ccp_alpha=0.0\n",
    "# )\n",
    "# gbm_reg.fit(X_train, y_train)\n",
    "# y_pred_gbm = gbm_reg.predict(X_test)\n",
    "# importances_gbm = gbm_reg.feature_importances_\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Оптимизированные Реализации Градиентного Бустинга\n",
    "\n",
    "# Существуют библиотеки, предлагающие более быстрые и продвинутые реализации GBM.\n",
    "\n",
    "# --- 4.1 XGBoost (Extreme Gradient Boosting) ---\n",
    "# # Улучшения:\n",
    "# # - Регуляризация (L1 и L2) на веса листьев для борьбы с переобучением.\n",
    "# # - Оптимизированный алгоритм поиска разбиений (учитывает разреженность данных, кэширование).\n",
    "# # - Встроенная обработка пропущенных значений.\n",
    "# # - Возможность параллелизации на уровне построения дерева.\n",
    "# # - Встроенная кросс-валидация.\n",
    "# # Установка: `pip install xgboost`\n",
    "import xgboost as xgb\n",
    "\n",
    "# xgb_reg = xgb.XGBRegressor(\n",
    "#     objective='reg:squarederror', # Целевая функция (MSE)\n",
    "#     n_estimators=100,\n",
    "#     learning_rate=0.1,\n",
    "#     max_depth=3,\n",
    "#     subsample=0.8,          # Доля строк для каждого дерева\n",
    "#     colsample_bytree=0.8,   # Доля признаков для каждого дерева\n",
    "#     gamma=0,                # Минимальное уменьшение потерь для сплита (регуляризация)\n",
    "#     reg_alpha=0,            # L1 регуляризация на веса листьев\n",
    "#     reg_lambda=1,           # L2 регуляризация на веса листьев\n",
    "#     random_state=None,\n",
    "#     n_jobs=-1               # Использовать все ядра\n",
    "# )\n",
    "# xgb_reg.fit(X_train, y_train) # Может требовать раннюю остановку (early stopping) на валидационном наборе\n",
    "# y_pred_xgb = xgb_reg.predict(X_test)\n",
    "# importances_xgb = xgb_reg.feature_importances_\n",
    "\n",
    "# --- 4.2 LightGBM (Light Gradient Boosting Machine) ---\n",
    "# # Улучшения:\n",
    "# # - Leaf-wise рост дерева (вместо level-wise): Строит дерево по листьям с наибольшим уменьшением потерь, что часто быстрее и точнее.\n",
    "# # - Gradient-based One-Side Sampling (GOSS): Сохраняет примеры с большими градиентами и случайно отбрасывает примеры с малыми градиентами для ускорения.\n",
    "# # - Exclusive Feature Bundling (EFB): Объединяет взаимоисключающие признаки для уменьшения размерности.\n",
    "# # - Очень быстрый и эффективный по памяти, особенно на больших датасетах.\n",
    "# # Установка: `pip install lightgbm`\n",
    "import lightgbm as lgb\n",
    "\n",
    "# lgb_reg = lgb.LGBMRegressor(\n",
    "#     objective='regression', # или 'regression_l1' (MAE), 'huber', 'quantile'\n",
    "#     metric='rmse',          # Метрика для оценки ('rmse', 'mae')\n",
    "#     n_estimators=100,\n",
    "#     learning_rate=0.1,\n",
    "#     max_depth=-1,           # -1 означает без ограничения\n",
    "#     num_leaves=31,          # Макс. число листьев (важный параметр для контроля сложности)\n",
    "#     subsample=0.8,\n",
    "#     colsample_bytree=0.8,\n",
    "#     reg_alpha=0.0,\n",
    "#     reg_lambda=0.0,\n",
    "#     random_state=None,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# lgb_reg.fit(X_train, y_train) # Также поддерживает early stopping\n",
    "# y_pred_lgb = lgb_reg.predict(X_test)\n",
    "# importances_lgb = lgb_reg.feature_importances_\n",
    "\n",
    "# --- 4.3 CatBoost ---\n",
    "# # Улучшения:\n",
    "# # - Продвинутая обработка категориальных признаков: Использует target encoding с регуляризацией (ordered boosting, permutations), не требует ручного one-hot encoding.\n",
    "# # - Ordered Boosting: Способ построения ансамбля, который помогает бороться со смещением предсказаний и переобучением.\n",
    "# # - Symmetric Trees: Использует одинаковые условия разбиения на одном уровне дерева.\n",
    "# # - Часто дает хорошие результаты с минимальной настройкой гиперпараметров.\n",
    "# # Установка: `pip install catboost`\n",
    "import catboost as cb\n",
    "\n",
    "# cb_reg = cb.CatBoostRegressor(\n",
    "#     iterations=100,         # Аналог n_estimators\n",
    "#     learning_rate=0.1,\n",
    "#     depth=6,                # Аналог max_depth\n",
    "#     l2_leaf_reg=3,          # L2 регуляризация\n",
    "#     loss_function='RMSE',   # Функция потерь/метрика\n",
    "#     verbose=0,              # Уровень вывода информации (0 - тихо)\n",
    "#     random_seed=None,\n",
    "#     # cat_features=[index1, index2, ...] # Индексы категориальных столбцов (если есть)\n",
    "# )\n",
    "# cb_reg.fit(X_train, y_train) # Может автоматически обрабатывать категориальные признаки, если указаны\n",
    "# y_pred_cb = cb_reg.predict(X_test)\n",
    "# importances_cb = cb_reg.feature_importances_\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Сравнение и Выбор\n",
    "\n",
    "# | Метод          | Основная Идея        | Плюсы                                       | Минусы                                        | Когда использовать                                    |\n",
    "# |----------------|----------------------|---------------------------------------------|-----------------------------------------------|-------------------------------------------------------|\n",
    "# | **Random Forest**| Бэггинг + Случ. Призн.| Стабильность, Уст. к переобуч., Параллелизм | Менее точный чем бустинг, \"Черный ящик\"       | Хороший baseline, когда важна стабильность           |\n",
    "# | **GBM (sklearn)**| Посл. испр. ошибок   | Высокая точность, Гибкость потерь           | Склонен к переобуч., Медленнее RF, Параметры | Когда нужна точность, но оптимиз. реализации недоступны |\n",
    "# | **XGBoost**      | Оптимиз. GBM + Регул.| Точность, Скорость, Регуляризация, Пропуски | Больше параметров для настройки               | Стандарт де-факто для соревнований, общие задачи      |\n",
    "# | **LightGBM**     | Оптимиз. GBM (GOSS/EFB)| Очень быстрый, Эфф. память, Точность      | Может переобучиться на малых данных           | Большие датасеты, когда важна скорость               |\n",
    "# | **CatBoost**     | Оптимиз. GBM (Катег.)| Отл. работа с категор., Меньше тюнинга      | Может быть медленнее XGB/LGB на числ. данных | Много категориальных признаков, меньше времени на тюнинг |\n",
    "\n",
    "# **Рекомендация:** Начинайте с Random Forest или одной из оптимизированных реализаций бустинга (XGBoost, LightGBM, CatBoost). Они часто дают лучшие результаты, чем стандартный GBM из sklearn. Выбор между XGBoost, LightGBM и CatBoost зависит от данных (размер, тип признаков) и приоритетов (скорость, точность, простота настройки).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Пример Задачи и Решения (XGBoost для Регрессии)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Предсказать цену на жилье в Калифорнии, используя датасет\n",
    "# California Housing из Scikit-learn. Используем XGBoost.\n",
    "\n",
    "# --- Решение (Полный Код) ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Загрузка Данных\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "X = housing.data\n",
    "y = housing.target # Целевая переменная - медианная стоимость дома в $100,000s\n",
    "\n",
    "# print(\"Dataset Info:\")\n",
    "# print(X.info())\n",
    "# print(\"\\nTarget variable description:\", housing.DESCR.split('\\n')[5]) # Описание цели\n",
    "# print(\"\\nSample Data (X head):\")\n",
    "# print(X.head())\n",
    "# print(\"\\nSample Target (y head):\")\n",
    "# print(y.head())\n",
    "\n",
    "# 2. Разделение Данных\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# print(f\"\\nTrain samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# 3. Масштабирование Признаков (Рекомендуется для XGBoost, хотя он менее чувствителен)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Преобразуем обратно в DataFrame для удобства (опционально)\n",
    "# X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "# X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "\n",
    "# 4. Создание и Обучение Модели XGBoost\n",
    "# Используем параметры по умолчанию или немного настроенные\n",
    "xgb_reg = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,       # Начнем со 100 деревьев\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,            # Увеличим глубину немного\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    early_stopping_rounds=10 # Добавим раннюю остановку\n",
    ")\n",
    "\n",
    "# print(\"\\nTraining XGBoost model...\")\n",
    "# Обучаем с использованием валидационного набора для ранней остановки\n",
    "eval_set = [(X_test_scaled, y_test)] # Используем тестовый набор как валидационный (не идеально, лучше отдельный val set)\n",
    "xgb_reg.fit(X_train_scaled, y_train,\n",
    "            eval_set=eval_set,\n",
    "            verbose=False) # verbose=True покажет процесс обучения\n",
    "# print(\"Training complete.\")\n",
    "# print(f\"Best iteration: {xgb_reg.best_iteration}\")\n",
    "\n",
    "# 5. Предсказание на Тестовой Выборке\n",
    "y_pred_xgb = xgb_reg.predict(X_test_scaled)\n",
    "\n",
    "# 6. Оценка Модели\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"\\n--- XGBoost Evaluation ---\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_xgb:.4f}\")\n",
    "print(f\"R-squared (R2): {r2_xgb:.4f}\")\n",
    "\n",
    "# 7. Важность Признаков\n",
    "# feature_importances = pd.Series(xgb_reg.feature_importances_, index=X.columns)\n",
    "# feature_importances = feature_importances.sort_values(ascending=False)\n",
    "#\n",
    "# print(\"\\nFeature Importances:\")\n",
    "# print(feature_importances)\n",
    "#\n",
    "# # Визуализация важности\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# feature_importances.plot(kind='bar')\n",
    "# plt.title('XGBoost Feature Importances for California Housing')\n",
    "# plt.ylabel('Importance')\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# --- Конец Примера ---\n",
    "\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Задача Титаника и Типы Данных\n",
    "\n",
    "# Задача: Предсказать выживание пассажира (Survived = 1) или гибель (Survived = 0)\n",
    "# на основе данных о пассажирах Титаника. Это задача бинарной классификации.\n",
    "\n",
    "# Типы Данных в Датасете:\n",
    "# - Числовые Непрерывные: Age, Fare\n",
    "# - Числовые Дискретные: SibSp (братья/сестры/супруги), Parch (родители/дети), Pclass (класс пассажира - можно считать порядковым)\n",
    "# - Категориальные Номинальные: Sex (male/female), Embarked (порт посадки C/Q/S)\n",
    "# - Текст/ID: Name, Ticket, Cabin, PassengerId (обычно не используются напрямую как признаки)\n",
    "# - Целевая Переменная: Survived (0 или 1)\n",
    "\n",
    "# Проблема: Деревья решений и их ансамбли (в большинстве реализаций, например, Scikit-learn)\n",
    "# требуют на вход числовые данные без пропусков. Текстовые и категориальные признаки\n",
    "# нужно преобразовать, а пропуски - обработать.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Подготовка Данных для Деревьев (Критически Важный Шаг)\n",
    "\n",
    "# --- 2.1 Обработка Пропущенных Значений ---\n",
    "# # Age: Часто имеет пропуски.\n",
    "# #   Стратегии: Заполнить медианой/средним; заполнить медианой по группе (Pclass+Sex); предсказать регрессией.\n",
    "# # Embarked: Редкие пропуски.\n",
    "# #   Стратегии: Заполнить модой (самым частым значением).\n",
    "# # Cabin: Очень много пропусков.\n",
    "# #   Стратегии: Удалить столбец; создать признак HasCabin (1/0); извлечь палубу (первую букву) и считать категорией (пропуски -> 'Unknown').\n",
    "# # Fare: Редкие пропуски (в тестовом наборе).\n",
    "# #   Стратегии: Заполнить медианой по Pclass.\n",
    "\n",
    "# --- 2.2 Кодирование Категориальных Признаков ---\n",
    "# # Преобразование категорий в числа.\n",
    "# # Sex (male/female):\n",
    "# #   Label Encoding (0/1) - просто и подходит для деревьев.\n",
    "# #   One-Hot Encoding ([1,0]/[0,1]) - тоже работает.\n",
    "# # Embarked (C, Q, S):\n",
    "# #   One-Hot Encoding - **рекомендуется**, т.к. нет порядка. Создаст Embarked_C, Embarked_Q, Embarked_S.\n",
    "# # Pclass (1, 2, 3):\n",
    "# #   Можно оставить как есть (деревья найдут сплиты типа Pclass <= 1.5).\n",
    "# #   Можно использовать One-Hot Encoding, если не хотим предполагать порядок.\n",
    "\n",
    "# --- 2.3 Инжиниринг Признаков (Feature Engineering) ---\n",
    "# # Создание новых признаков из существующих.\n",
    "# # FamilySize: SibSp + Parch + 1.\n",
    "# # IsAlone: 1 если FamilySize == 1, иначе 0.\n",
    "# # Title: Извлечь из Name (Mr, Mrs, Miss, Master, Dr...). Очень полезно. Закодировать (One-Hot или сгруппировать редкие).\n",
    "# # AgeGroup: Разбить Age на категории (ребенок, взрослый...).\n",
    "# # FareBin: Разбить Fare на категории.\n",
    "\n",
    "# --- 2.4 Удаление Ненужных Признаков ---\n",
    "# # PassengerId: Идентификатор, не нужен для модели.\n",
    "# # Name: Удалить после извлечения Title.\n",
    "# # Ticket: Обычно удаляют (сложный для парсинга).\n",
    "# # Cabin: Удалить, если не использовался для инжиниринга.\n",
    "\n",
    "# --- 2.5 Масштабирование Признаков (Feature Scaling) ---\n",
    "# # StandardScaler, MinMaxScaler и т.д.\n",
    "# # **НЕ ТРЕБУЕТСЯ** для деревьев решений, Random Forest, Gradient Boosting.\n",
    "# # Эти алгоритмы не чувствительны к масштабу признаков.\n",
    "\n",
    "# # Итог: После предобработки должна получиться числовая матрица признаков без пропусков.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Применение Деревьев Решений и Ансамблей\n",
    "\n",
    "# После подготовки данных можно обучать модели.\n",
    "\n",
    "# --- 3.1 Decision Tree Classifier (`sklearn.tree.DecisionTreeClassifier`) ---\n",
    "# # - Хороший baseline, интерпретируемый.\n",
    "# # - **Склонен к переобучению!** Нужна регуляризация:\n",
    "# #   - `max_depth`: Ограничить глубину (e.g., 3-7).\n",
    "# #   - `min_samples_leaf`: Мин. число примеров в листе (e.g., 5-10).\n",
    "# #   - `min_samples_split`: Мин. число для разделения узла.\n",
    "# #   - `ccp_alpha`: Для post-pruning.\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# dt_clf = DecisionTreeClassifier(max_depth=5, min_samples_leaf=10, random_state=42)\n",
    "\n",
    "# --- 3.2 Random Forest Classifier (`sklearn.ensemble.RandomForestClassifier`) ---\n",
    "# # - Ансамбль деревьев (бэггинг + случайные признаки).\n",
    "# # - **Уменьшает переобучение**, более стабилен и точен.\n",
    "# # - Основные параметры:\n",
    "# #   - `n_estimators`: Число деревьев (e.g., 100-500).\n",
    "# #   - `max_depth`, `min_samples_leaf`, `min_samples_split`: Контроль деревьев.\n",
    "# #   - `max_features`: Число признаков для сплита ('sqrt', 'log2').\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# rf_clf = RandomForestClassifier(n_estimators=200, max_depth=7, min_samples_leaf=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "# --- 3.3 Gradient Boosting Classifier (`sklearn.ensemble.GradientBoostingClassifier`) ---\n",
    "# # - Последовательное построение деревьев (бустинг).\n",
    "# # - Потенциально высокая точность.\n",
    "# # - **Чувствителен к параметрам**, может переобучиться.\n",
    "# # - Ключевые параметры:\n",
    "# #   - `n_estimators`: Число деревьев.\n",
    "# #   - `learning_rate`: Скорость обучения (e.g., 0.01-0.1).\n",
    "# #   - `max_depth`: Глубина деревьев (обычно неглубокие, 3-5).\n",
    "# #   - `subsample`: Доля данных для дерева (< 1.0).\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# gbm_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, subsample=0.8, random_state=42)\n",
    "\n",
    "# --- 3.4 XGBoost (`xgboost.XGBClassifier`) ---\n",
    "# # - Оптимизированный бустинг с регуляризацией.\n",
    "# # - Часто **state-of-the-art** для табличных данных.\n",
    "# # - **Может обрабатывать пропуски** (NaN) внутри.\n",
    "# # - Параметры: `eta` (learning_rate), `gamma`, `reg_alpha`, `reg_lambda`.\n",
    "import xgboost as xgb\n",
    "# xgb_clf = xgb.XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, subsample=0.8, colsample_bytree=0.8, use_label_encoder=False, eval_metric='logloss', random_state=42, n_jobs=-1)\n",
    "\n",
    "# --- 3.5 LightGBM (`lightgbm.LGBMClassifier`) ---\n",
    "# # - Быстрая реализация бустинга (leaf-wise рост).\n",
    "# # - **Эффективен на больших данных**.\n",
    "# # - **Может обрабатывать пропуски**.\n",
    "# # - Параметры: `num_leaves`, `learning_rate`, `n_estimators`.\n",
    "import lightgbm as lgb\n",
    "# lgb_clf = lgb.LGBMClassifier(n_estimators=100, learning_rate=0.1, num_leaves=31, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1)\n",
    "\n",
    "# --- 3.6 CatBoost (`catboost.CatBoostClassifier`) ---\n",
    "# # - **Отлично обрабатывает категориальные признаки** автоматически (если указать `cat_features`).\n",
    "# # - Устойчив к переобучению.\n",
    "# # - **Может обрабатывать пропуски**.\n",
    "# # - Параметры: `iterations`, `learning_rate`, `depth`.\n",
    "import catboost as cb\n",
    "# cb_clf = cb.CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, l2_leaf_reg=3, loss_function='Logloss', verbose=0, random_state=42) # verbose=0 чтобы не печатал лог\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Оценка Модели (Бинарная Классификация)\n",
    "\n",
    "# Используемые метрики:\n",
    "# - Accuracy: Общая точность (доля правильных ответов).\n",
    "# - Precision: Точность для класса \"выжил\" (TP / (TP + FP)).\n",
    "# - Recall: Полнота для класса \"выжил\" (TP / (TP + FN)).\n",
    "# - F1-Score: Гармоническое среднее Precision и Recall.\n",
    "# - AUC-ROC: Площадь под ROC-кривой (способность разделять классы).\n",
    "# - Confusion Matrix: Матрица ошибок (TP, TN, FP, FN).\n",
    "\n",
    "# Важно использовать Кросс-Валидацию для надежной оценки и подбора гиперпараметров.\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Пример оценки с кросс-валидацией (для Random Forest)\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# rf_model_for_cv = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1) # Пример модели\n",
    "# cv_scores = cross_val_score(rf_model_for_cv, X, y, cv=kf, scoring='accuracy') # X, y - полные обработанные данные\n",
    "# print(f\"Cross-Validation Accuracy Scores: {cv_scores}\")\n",
    "# print(f\"Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Пример Рабочего Процесса (Концептуальный Код)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier # Пример с Random Forest\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# --- 5.1 Загрузка данных ---\n",
    "# # Замените на реальные пути к файлам Титаника\n",
    "# try:\n",
    "#     train_df = pd.read_csv('train.csv')\n",
    "#     test_df = pd.read_csv('test.csv') # Для финального предсказания\n",
    "#     passenger_ids_test = test_df['PassengerId'] # Сохраняем ID для submission\n",
    "#     print(\"Titanic data loaded successfully.\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"Error: train.csv or test.csv not found. Please place them in the correct directory.\")\n",
    "#     # Создадим пустые DataFrame для предотвращения ошибок далее\n",
    "#     train_df = pd.DataFrame()\n",
    "#     test_df = pd.DataFrame()\n",
    "#     passenger_ids_test = pd.Series(dtype='int64')\n",
    "\n",
    "\n",
    "# --- 5.2 Предобработка и Инжиниринг ---\n",
    "def preprocess_titanic_simple(df):\n",
    "    # # Функция для простой предобработки (можно улучшать)\n",
    "    processed_df = df.copy()\n",
    "\n",
    "    # # Заполнение Age медианой\n",
    "    median_age = processed_df['Age'].median()\n",
    "    processed_df['Age'].fillna(median_age, inplace=True)\n",
    "\n",
    "    # # Заполнение Embarked модой\n",
    "    mode_embarked = processed_df['Embarked'].mode()[0]\n",
    "    processed_df['Embarked'].fillna(mode_embarked, inplace=True)\n",
    "\n",
    "    # # Заполнение Fare медианой (на случай пропусков в test)\n",
    "    median_fare = processed_df['Fare'].median()\n",
    "    processed_df['Fare'].fillna(median_fare, inplace=True)\n",
    "\n",
    "    # # Кодирование Sex\n",
    "    le = LabelEncoder()\n",
    "    processed_df['Sex'] = le.fit_transform(processed_df['Sex']) # female: 0, male: 1\n",
    "\n",
    "    # # Кодирование Embarked (One-Hot)\n",
    "    processed_df = pd.get_dummies(processed_df, columns=['Embarked'], prefix='Embarked', drop_first=False) # Не удаляем первый для простоты\n",
    "\n",
    "    # # Feature Engineering: FamilySize, IsAlone\n",
    "    processed_df['FamilySize'] = processed_df['SibSp'] + processed_df['Parch'] + 1\n",
    "    processed_df['IsAlone'] = (processed_df['FamilySize'] == 1).astype(int)\n",
    "\n",
    "    # # Удаление ненужных столбцов\n",
    "    cols_to_drop = ['Name', 'Ticket', 'Cabin', 'PassengerId', 'SibSp', 'Parch']\n",
    "    cols_exist = [col for col in cols_to_drop if col in processed_df.columns]\n",
    "    processed_df = processed_df.drop(columns=cols_exist)\n",
    "\n",
    "    # # Убедимся, что все колонки числовые (на случай ошибок)\n",
    "    for col in processed_df.columns:\n",
    "        if processed_df[col].dtype == 'object':\n",
    "             # Простая обработка - может потребоваться более сложная\n",
    "             processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce').fillna(0)\n",
    "\n",
    "    return processed_df\n",
    "\n",
    "# # Проверяем, что DataFrame не пустые перед обработкой\n",
    "# if not train_df.empty and not test_df.empty:\n",
    "#     train_processed = preprocess_titanic_simple(train_df)\n",
    "#     test_processed = preprocess_titanic_simple(test_df)\n",
    "#\n",
    "#     # # Выравнивание колонок (на случай разных категорий в train/test после one-hot)\n",
    "#     train_labels = train_processed['Survived']\n",
    "#     train_ids = train_processed.index # Или PassengerId, если не удалили\n",
    "#     test_ids = test_processed.index\n",
    "#\n",
    "#     # Удаляем Survived из трейна для создания X\n",
    "#     train_features = train_processed.drop('Survived', axis=1)\n",
    "#     test_features = test_processed # В тесте нет Survived\n",
    "#\n",
    "#     # Выравниваем колонки - добавляем недостающие в тест с нулями, удаляем лишние из теста\n",
    "#     common_cols = list(set(train_features.columns) & set(test_features.columns))\n",
    "#     train_features = train_features[common_cols]\n",
    "#     test_features = test_features[common_cols]\n",
    "#\n",
    "#     # Добавляем недостающие колонки в test_features (если они были только в train)\n",
    "#     missing_cols = set(train_features.columns) - set(test_features.columns)\n",
    "#     for c in missing_cols:\n",
    "#         test_features[c] = 0\n",
    "#     # Убедимся, что порядок колонок одинаковый\n",
    "#     test_features = test_features[train_features.columns]\n",
    "#\n",
    "#     X = train_features\n",
    "#     y = train_labels\n",
    "#     X_submission_test = test_features # Финальный тестовый набор для предсказания\n",
    "#\n",
    "#     print(\"Data preprocessing complete.\")\n",
    "#     print(f\"Shape of X: {X.shape}\")\n",
    "#     print(f\"Shape of X_submission_test: {X_submission_test.shape}\")\n",
    "# else:\n",
    "#     print(\"Skipping processing and training due to missing data.\")\n",
    "#     # Создадим пустые переменные, чтобы код ниже не падал\n",
    "#     X, y, X_submission_test = pd.DataFrame(), pd.Series(dtype='int64'), pd.DataFrame()\n",
    "\n",
    "\n",
    "# --- 5.3 Обучение Модели (Random Forest) ---\n",
    "# if not X.empty: # Проверяем, что есть данные для обучения\n",
    "#     model = RandomForestClassifier(n_estimators=200, max_depth=7, min_samples_leaf=5,\n",
    "#                                    random_state=42, n_jobs=-1, oob_score=True)\n",
    "#     print(\"\\nTraining Random Forest...\")\n",
    "#     model.fit(X, y)\n",
    "#     print(\"Training complete.\")\n",
    "#     print(f\"OOB Score: {model.oob_score_:.4f}\") # Оценка на Out-of-Bag данных\n",
    "# else:\n",
    "#     print(\"Skipping model training.\")\n",
    "#     model = None # Модель не обучена\n",
    "\n",
    "# --- 5.4 Оценка (Пример на обучающих данных - не идеально!) ---\n",
    "# if model:\n",
    "#     print(\"\\nEvaluating on Training Data (for demonstration)...\")\n",
    "#     y_pred_train = model.predict(X)\n",
    "#     print(f\"Training Accuracy: {accuracy_score(y, y_pred_train):.4f}\")\n",
    "#     print(classification_report(y, y_pred_train))\n",
    "#     # В реальной задаче нужна валидация на отложенной выборке или CV\n",
    "\n",
    "# --- 5.5 Предсказание для Тестового Набора (для Kaggle) ---\n",
    "# if model and not X_submission_test.empty:\n",
    "#     print(\"\\nPredicting on Test Set for submission...\")\n",
    "#     test_predictions = model.predict(X_submission_test)\n",
    "#\n",
    "#     # --- 5.6 Формирование Файла для Отправки ---\n",
    "#     submission = pd.DataFrame({'PassengerId': passenger_ids_test, 'Survived': test_predictions})\n",
    "#     submission_filename = 'titanic_submission_rf.csv'\n",
    "#     submission.to_csv(submission_filename, index=False)\n",
    "#     print(f\"\\nSubmission file created: {submission_filename}\")\n",
    "# else:\n",
    "#      print(\"\\nSkipping prediction and submission file creation.\")\n",
    "\n",
    "# --- Конец Примера ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Введение в Обучение Без Учителя (Unsupervised Learning)\n",
    "\n",
    "# Что такое Обучение Без Учителя?\n",
    "# Это парадигма машинного обучения, в которой модели обучаются на данных,\n",
    "# **не имеющих заранее известных правильных ответов или меток** (т.е. нет целевой переменной `y`).\n",
    "# Вместо предсказания конкретного выхода, цель - найти скрытую структуру,\n",
    "# закономерности, группы или представления в самих входных данных (`X`).\n",
    "\n",
    "# Цели Обучения Без Учителя:\n",
    "# - **Понимание данных:** Обнаружение внутренней структуры, групп, выбросов.\n",
    "# - **Сжатие данных:** Уменьшение размерности для визуализации или хранения.\n",
    "# - **Извлечение признаков:** Создание новых, более информативных признаков для\n",
    "#   последующего использования в моделях с учителем.\n",
    "# - **Обнаружение аномалий:** Выявление необычных или подозрительных наблюдений.\n",
    "# - **Генерация данных:** Создание новых данных, похожих на исходные (генеративные модели).\n",
    "\n",
    "# Отличие от Обучения с Учителем:\n",
    "# - **С учителем:** Есть размеченные данные `(X, y)`. Цель - научиться предсказывать `y` по `X`.\n",
    "#   Задачи: Классификация, Регрессия.\n",
    "# - **Без учителя:** Есть только данные `X`. Цель - найти структуру в `X`.\n",
    "#   Задачи: Кластеризация, Снижение размерности, Обнаружение аномалий и т.д.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Основные Задачи Обучения Без Учителя\n",
    "\n",
    "# --- 2.1 Кластеризация (Clustering) ---\n",
    "# # Цель: Разделить набор данных на группы (кластеры) таким образом, чтобы\n",
    "# # объекты внутри одного кластера были максимально похожи друг на друга,\n",
    "# # а объекты из разных кластеров - максимально различны.\n",
    "# # Применение: Сегментация клиентов, группировка документов по темам,\n",
    "# #            обнаружение сообществ в соцсетях, сегментация изображений.\n",
    "# # Алгоритмы: K-Means, DBSCAN, Иерархическая кластеризация, Gaussian Mixture Models (GMM).\n",
    "\n",
    "# --- 2.2 Снижение Размерности (Dimensionality Reduction) ---\n",
    "# # Цель: Уменьшить количество признаков (размерность) в данных, сохраняя\n",
    "# # при этом как можно больше важной информации.\n",
    "# # Применение: Визуализация многомерных данных (в 2D или 3D),\n",
    "# #            уменьшение вычислительной сложности, борьба с \"проклятием размерности\",\n",
    "# #            шумоподавление, извлечение признаков.\n",
    "# # Алгоритмы: PCA (Метод Главных Компонент), t-SNE, UMAP, Автоэнкодеры (Autoencoders).\n",
    "\n",
    "# --- 2.3 Обнаружение Аномалий (Anomaly Detection / Outlier Detection) ---\n",
    "# # Цель: Найти объекты в данных, которые значительно отличаются от \"нормального\"\n",
    "# # большинства данных.\n",
    "# # Применение: Обнаружение мошенничества, сетевых вторжений, дефектов производства,\n",
    "# #            необычных медицинских показателей.\n",
    "# # Алгоритмы: Isolation Forest, Local Outlier Factor (LOF), One-Class SVM, Автоэнкодеры.\n",
    "\n",
    "# --- 2.4 Правила Ассоциации (Association Rule Mining) ---\n",
    "# # Цель: Найти закономерности вида \"Если есть X, то часто есть и Y\" в больших наборах данных.\n",
    "# # Применение: Анализ рыночной корзины (Market Basket Analysis - \"кто покупает хлеб, часто покупает и молоко\"),\n",
    "# #            рекомендательные системы (базовые).\n",
    "# # Алгоритмы: Apriori, FP-Growth.\n",
    "\n",
    "# --- 2.5 Обучение Представлений (Representation Learning) ---\n",
    "# # Цель: Научиться представлять данные в виде, который удобен для дальнейшей обработки\n",
    "# # (например, в виде плотных векторов - эмбеддингов). Часто пересекается со снижением размерности.\n",
    "# # Применение: Извлечение признаков для классификации/регрессии, семантический поиск.\n",
    "# # Алгоритмы: Автоэнкодеры, Word2Vec/GloVe/FastText (для текста, часто обучаются само-супервизией), PCA.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Кластеризация - Алгоритмы и Концепции\n",
    "\n",
    "# --- 3.1 K-Means (Метод k-средних) ---\n",
    "# # Идея: Разделить данные на `k` заранее заданных кластеров.\n",
    "# # Алгоритм:\n",
    "# #   1. Инициализировать `k` центроидов (центров кластеров) случайно или по определенной стратегии.\n",
    "# #   2. Повторять до сходимости:\n",
    "# #      a. Шаг Присвоения: Отнести каждый объект данных к ближайшему центроиду (обычно по евклидову расстоянию).\n",
    "# #      b. Шаг Обновления: Пересчитать положение каждого центроида как среднее всех объектов, отнесенных к нему.\n",
    "# # Плюсы: Простой, быстрый, хорошо масштабируется.\n",
    "# # Минусы: Нужно заранее знать число кластеров `k`, чувствителен к начальной инициализации центроидов,\n",
    "# #         плохо работает с кластерами нешарообразной формы и разной плотности, чувствителен к масштабу признаков.\n",
    "# # Библиотека: `sklearn.cluster.KMeans`\n",
    "from sklearn.cluster import KMeans\n",
    "# kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto') # n_init='auto' для подавления warning\n",
    "\n",
    "# --- 3.2 DBSCAN (Density-Based Spatial Clustering of Applications with Noise) ---\n",
    "# # Идея: Группировать точки, которые плотно расположены, отмечая как выбросы точки,\n",
    "# #       лежащие в областях с низкой плотностью.\n",
    "# # Алгоритм:\n",
    "# #   1. Выбрать точку, найти ее соседей в радиусе `eps`.\n",
    "# #   2. Если соседей достаточно (`min_samples`), создать новый кластер, добавить точку и ее соседей в него.\n",
    "# #   3. Рекурсивно расширять кластер, добавляя соседей соседей и т.д.\n",
    "# #   4. Точки, не попавшие ни в один кластер, считаются шумом.\n",
    "# # Плюсы: Не требует задания числа кластеров, находит кластеры произвольной формы, устойчив к выбросам.\n",
    "# # Минусы: Чувствителен к параметрам `eps` (радиус окрестности) и `min_samples` (мин. число точек),\n",
    "# #         плохо работает с кластерами разной плотности, чувствителен к масштабу признаков.\n",
    "# # Библиотека: `sklearn.cluster.DBSCAN`\n",
    "from sklearn.cluster import DBSCAN\n",
    "# dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "\n",
    "# --- 3.3 Иерархическая Кластеризация (Агломеративная) ---\n",
    "# # Идея: Построить иерархию кластеров (дендрограмму).\n",
    "# # Алгоритм (Агломеративный - \"снизу вверх\"):\n",
    "# #   1. Начать с того, что каждая точка - это отдельный кластер.\n",
    "# #   2. Повторять:\n",
    "# #      a. Найти два ближайших кластера.\n",
    "# #      b. Объединить их в один новый кластер.\n",
    "# #   3. Продолжать до тех пор, пока все точки не окажутся в одном кластере.\n",
    "# # Плюсы: Не требует задания числа кластеров заранее (можно выбрать потом, \"обрезав\" дендрограмму),\n",
    "# #         дает визуализацию иерархии (дендрограмма).\n",
    "# # Минусы: Вычислительно сложен (O(n^2 log n) или O(n^3)), не очень подходит для больших данных,\n",
    "# #         результат зависит от метрики расстояния и критерия связи (linkage).\n",
    "# # Критерии Связи (Linkage): Определяют расстояние между кластерами:\n",
    "# #   - `ward`: Минимизирует дисперсию внутри объединяемых кластеров (часто дает хорошие результаты).\n",
    "# #   - `average`: Среднее расстояние между всеми парами точек из разных кластеров.\n",
    "# #   - `complete` (max): Максимальное расстояние между точками из разных кластеров.\n",
    "# #   - `single` (min): Минимальное расстояние между точками из разных кластеров.\n",
    "# # Библиотека: `sklearn.cluster.AgglomerativeClustering`\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# agg_clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0, linkage='ward') # distance_threshold=0 строит полную дендрограмму\n",
    "\n",
    "# --- 3.4 Оценка Качества Кластеризации ---\n",
    "# # Так как нет истинных меток, используются внутренние метрики:\n",
    "# # - Коэффициент Силуэта (Silhouette Score): Измеряет, насколько объект похож на свой кластер\n",
    "# #   по сравнению с другими кластерами. Значения от -1 до 1. Ближе к 1 - лучше.\n",
    "# #   `sklearn.metrics.silhouette_score`\n",
    "# # - Индекс Дэвиса-Болдина (Davies-Bouldin Index): Отношение внутрикластерного расстояния\n",
    "# #   к межкластерному расстоянию. Чем меньше значение, тем лучше (ближе к 0).\n",
    "# #   `sklearn.metrics.davies_bouldin_score`\n",
    "# # - Визуальный анализ (если данные 2D/3D или после снижения размерности).\n",
    "# # - Метод \"Локтя\" (Elbow Method) для K-Means: Построить график зависимости суммы квадратов\n",
    "# #   расстояний до центроидов (inertia) от числа кластеров `k`. Искать \"изгиб\" (локоть) на графике.\n",
    "\n",
    "# --- 3.5 Важность Масштабирования ---\n",
    "# # Алгоритмы, основанные на расстоянии (K-Means, DBSCAN, Иерархическая), **очень чувствительны**\n",
    "# # к масштабу признаков. Признаки с большими значениями будут доминировать.\n",
    "# # **Необходимо масштабировать данные** перед кластеризацией (например, с помощью `StandardScaler`).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Снижение Размерности - Алгоритмы и Концепции\n",
    "\n",
    "# --- 4.1 Метод Главных Компонент (PCA - Principal Component Analysis) ---\n",
    "# # Идея: Найти новое ортогональное базисное пространство (главные компоненты),\n",
    "# #       в котором дисперсия данных максимальна вдоль первых компонент.\n",
    "# #       Проецирует данные на подпространство меньшей размерности, образованное\n",
    "# #       главными компонентами с наибольшей дисперсией.\n",
    "# # Алгоритм: Основан на сингулярном разложении (SVD) ковариационной матрицы данных.\n",
    "# # Плюсы: Простой, быстрый, детерминированный, хорошо работает для линейного снижения размерности,\n",
    "# #         полезен для шумоподавления.\n",
    "# # Минусы: Чувствителен к масштабу признаков, плохо работает, если зависимости нелинейные,\n",
    "# #         главные компоненты могут быть трудно интерпретируемы.\n",
    "# # Библиотека: `sklearn.decomposition.PCA`\n",
    "from sklearn.decomposition import PCA\n",
    "# pca = PCA(n_components=2) # Снизить до 2 компонент\n",
    "# pca = PCA(n_components=0.95) # Сохранить 95% дисперсии\n",
    "\n",
    "# --- 4.2 t-SNE (t-distributed Stochastic Neighbor Embedding) ---\n",
    "# # Идея: Моделирует сходство между точками в многомерном пространстве и пытается\n",
    "# #       сохранить это локальное сходство при отображении в низкоразмерное пространство (обычно 2D/3D).\n",
    "# # Алгоритм: Вероятностный, использует t-распределение для моделирования сходства в низкой размерности.\n",
    "# # Плюсы: Отлично подходит для **визуализации** сложных многомерных данных, хорошо разделяет кластеры.\n",
    "# # Минусы: **Не подходит для общего снижения размерности** (только для визуализации),\n",
    "# #         вычислительно сложен, результаты могут сильно зависеть от параметров (`perplexity`, `learning_rate`, `n_iter`),\n",
    "# #         не сохраняет глобальную структуру данных (расстояния между кластерами неинформативны).\n",
    "# # Библиотека: `sklearn.manifold.TSNE`\n",
    "from sklearn.manifold import TSNE\n",
    "# tsne = TSNE(n_components=2, perplexity=30.0, n_iter=1000, random_state=42)\n",
    "\n",
    "# --- 4.3 UMAP (Uniform Manifold Approximation and Projection) ---\n",
    "# # Идея: Основан на топологическом анализе данных и теории римановых многообразий.\n",
    "# #       Строит граф ближайших соседей в исходном пространстве и оптимизирует\n",
    "# #       похожий граф в низкоразмерном пространстве.\n",
    "# # Алгоритм: Сложный, но эффективный.\n",
    "# # Плюсы: Часто дает лучшую **визуализацию**, чем t-SNE, сохраняя как локальную, так и\n",
    "# #         (в некоторой степени) глобальную структуру. Значительно **быстрее** t-SNE.\n",
    "# #         Может использоваться и для общего снижения размерности (с осторожностью).\n",
    "# # Минусы: Результаты зависят от параметров (`n_neighbors`, `min_dist`). Теория сложнее PCA.\n",
    "# # Библиотека: `umap-learn` (нужно установить отдельно: `pip install umap-learn`)\n",
    "# import umap # pip install umap-learn\n",
    "# umap_reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1, random_state=42)\n",
    "\n",
    "# --- 4.4 Автоэнкодеры (Autoencoders) ---\n",
    "# # Идея: Нейронная сеть, состоящая из двух частей:\n",
    "# #   - Энкодер: Сжимает входные данные в низкоразмерное представление (латентное пространство, код).\n",
    "# #   - Декодер: Пытается восстановить исходные данные из этого сжатого представления.\n",
    "# # Сеть обучается минимизировать ошибку реконструкции (разницу между входом и выходом).\n",
    "# # Сжатое представление (выход энкодера) используется как результат снижения размерности.\n",
    "# # Плюсы: Могут изучать сложные **нелинейные** зависимости, гибкая архитектура.\n",
    "# # Минусы: Требуют больше данных и вычислительных ресурсов для обучения, сложнее в настройке.\n",
    "# # Библиотека: PyTorch, TensorFlow/Keras.\n",
    "\n",
    "# --- 4.5 Важность Масштабирования ---\n",
    "# # PCA **очень чувствителен** к масштабу признаков. **Необходимо масштабировать** данные перед PCA.\n",
    "# # t-SNE и UMAP менее чувствительны, но масштабирование часто рекомендуется.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Обнаружение Аномалий - Алгоритмы\n",
    "\n",
    "# --- 5.1 Isolation Forest ---\n",
    "# # Идея: Аномалии легче \"изолировать\" (отделить от остальных данных), чем нормальные точки.\n",
    "# # Алгоритм: Строит ансамбль случайных деревьев изоляции. В каждом дереве данные\n",
    "# #          рекурсивно делятся случайным признаком и случайным порогом. Аномальные\n",
    "# #          точки обычно требуют меньше разбиений (имеют меньшую среднюю длину пути\n",
    "# #          от корня до листа).\n",
    "# # Плюсы: Эффективен на многомерных данных, не требует масштабирования, относительно быстрый.\n",
    "# # Минусы: Может быть чувствителен к параметру `contamination` (ожидаемая доля выбросов).\n",
    "# # Библиотека: `sklearn.ensemble.IsolationForest`\n",
    "from sklearn.ensemble import IsolationForest\n",
    "# iso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42) # contamination='auto' или float (e.g., 0.05)\n",
    "\n",
    "# --- 5.2 Local Outlier Factor (LOF) ---\n",
    "# # Идея: Сравнивает локальную плотность точки с плотностью ее соседей.\n",
    "# #       Точки в областях с значительно меньшей плотностью, чем у их соседей, считаются выбросами.\n",
    "# # Плюсы: Учитывает локальную структуру, может находить выбросы в данных с разной плотностью.\n",
    "# # Минусы: Вычислительно сложнее Isolation Forest, чувствителен к выбору числа соседей (`n_neighbors`) и масштабу.\n",
    "# # Библиотека: `sklearn.neighbors.LocalOutlierFactor`\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "# lof = LocalOutlierFactor(n_neighbors=20, contamination='auto') # contamination='auto' или float\n",
    "\n",
    "# --- 5.3 One-Class SVM ---\n",
    "# # Идея: Пытается найти границу, которая охватывает \"нормальные\" данные.\n",
    "# #       Точки, лежащие далеко за этой границей, считаются аномалиями.\n",
    "# #       Использует ядерный трюк для нелинейных границ.\n",
    "# # Плюсы: Может находить сложные границы аномалий.\n",
    "# # Минусы: Чувствителен к выбору ядра и его параметров (`kernel`, `gamma`, `nu`), требует масштабирования.\n",
    "# # Библиотека: `sklearn.svm.OneClassSVM`\n",
    "from sklearn.svm import OneClassSVM\n",
    "# one_svm = OneClassSVM(gamma='auto', nu=0.05) # nu - ожидаемая доля выбросов\n",
    "\n",
    "# --- 5.4 Оценка Обнаружения Аномалий ---\n",
    "# # Сложно без истинных меток.\n",
    "# # Если есть метки (хотя бы для части данных): Precision, Recall, F1 для класса аномалий.\n",
    "# # Если меток нет: Визуальный анализ (если возможно), экспертная оценка, анализ характеристик найденных аномалий.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Пример Задачи и Решения (Кластеризация K-Means)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Сгенерировать 2D данные с несколькими явными группами (блобами)\n",
    "# и применить K-Means для их разделения на кластеры. Визуализировать результат.\n",
    "\n",
    "# --- Решение (Полный Код) ---\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs # Для генерации данных для кластеризации\n",
    "from sklearn.preprocessing import StandardScaler # Для масштабирования\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score # Для оценки\n",
    "\n",
    "# 1. Генерация Синтетических Данных\n",
    "n_samples = 300\n",
    "n_features = 2\n",
    "n_clusters = 4 # Задаем истинное число кластеров для генерации\n",
    "random_state = 42\n",
    "\n",
    "X, y_true = make_blobs(n_samples=n_samples,\n",
    "                       n_features=n_features,\n",
    "                       centers=n_clusters,\n",
    "                       cluster_std=0.8, # Стандартное отклонение внутри кластера\n",
    "                       random_state=random_state)\n",
    "\n",
    "# print(f\"Generated data shape: X={X.shape}, y_true={y_true.shape}\")\n",
    "# print(\"Sample X:\\n\", X[:5])\n",
    "# print(\"True labels (first 10):\", y_true[:10]) # Истинные метки (мы их не будем использовать для обучения)\n",
    "\n",
    "# Визуализация исходных данных\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(X[:, 0], X[:, 1], c=y_true, s=50, cmap='viridis') # Раскрасим по истинным меткам для наглядности\n",
    "# plt.title(\"Сгенерированные данные для кластеризации\")\n",
    "# plt.xlabel(\"Признак 1\")\n",
    "# plt.ylabel(\"Признак 2\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# 2. Масштабирование Данных (Важно для K-Means)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# print(\"\\nScaled data sample (first 5):\\n\", X_scaled[:5])\n",
    "\n",
    "# 3. Обучение Модели K-Means\n",
    "k = n_clusters # Мы знаем истинное K, но в реальности его нужно подбирать (метод локтя, силуэт)\n",
    "kmeans = KMeans(n_clusters=k, random_state=random_state, n_init=10) # n_init=10 для стабильности\n",
    "\n",
    "# print(f\"\\nTraining K-Means with k={k}...\")\n",
    "kmeans.fit(X_scaled)\n",
    "# print(\"Training complete.\")\n",
    "\n",
    "# Получаем результаты\n",
    "cluster_labels = kmeans.labels_ # Метки кластеров, присвоенные каждой точке\n",
    "centroids = kmeans.cluster_centers_ # Координаты центроидов\n",
    "inertia = kmeans.inertia_ # Сумма квадратов расстояний до ближайшего центроида\n",
    "\n",
    "# print(f\"\\nCluster labels assigned (first 10): {cluster_labels[:10]}\")\n",
    "# print(f\"Centroid coordinates:\\n{centroids}\")\n",
    "# print(f\"Inertia (Within-cluster sum-of-squares): {inertia:.2f}\")\n",
    "\n",
    "# 4. Оценка Качества Кластеризации (без истинных меток)\n",
    "silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "print(f\"\\nSilhouette Score: {silhouette_avg:.4f}\") # Ближе к 1 - лучше\n",
    "\n",
    "# 5. Визуализация Результатов Кластеризации\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# # Раскрашиваем точки по предсказанным меткам кластеров\n",
    "# plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=cluster_labels, s=50, cmap='viridis', label='Data Points')\n",
    "# # Рисуем центроиды\n",
    "# plt.scatter(centroids[:, 0], centroids[:, 1], s=200, c='red', marker='X', label='Centroids')\n",
    "# plt.title(f\"K-Means Clustering Results (k={k}, Silhouette={silhouette_avg:.2f})\")\n",
    "# plt.xlabel(\"Масштабированный Признак 1\")\n",
    "# plt.ylabel(\"Масштабированный Признак 2\")\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# --- Конец Примера ---\n",
    "\n",
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Введение в Метод Главных Компонент (PCA - Principal Component Analysis)\n",
    "\n",
    "# Что такое PCA?\n",
    "# PCA - это один из самых популярных методов обучения без учителя для\n",
    "# **снижения размерности** (dimensionality reduction).\n",
    "# Это линейный метод преобразования данных, который находит новое\n",
    "# координатное пространство, где оси (называемые **главными компонентами**)\n",
    "# выровнены вдоль направлений максимальной **дисперсии** (variance) данных.\n",
    "\n",
    "# Цель PCA:\n",
    "# 1. Уменьшить количество признаков (столбцов) в наборе данных, сохраняя\n",
    "#    при этом как можно больше исходной информации (дисперсии).\n",
    "# 2. Создать новые, некоррелированные признаки (главные компоненты) из\n",
    "#    линейных комбинаций исходных признаков.\n",
    "# 3. Упростить данные для визуализации, хранения или последующего использования\n",
    "#    в других моделях машинного обучения.\n",
    "\n",
    "# Ключевая Идея:\n",
    "# PCA предполагает, что направления с наибольшей дисперсией содержат\n",
    "# наиболее важную информацию о структуре данных. Он находит эти направления\n",
    "# и проецирует исходные данные на подпространство, образованное\n",
    "# несколькими первыми (наиболее важными) главными компонентами.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: Зачем Использовать PCA? (Применения)\n",
    "\n",
    "# 1. Снижение Размерности:\n",
    "#    - Уменьшение вычислительной сложности моделей ML (меньше признаков -> быстрее обучение).\n",
    "#    - Борьба с \"проклятием размерности\" (curse of dimensionality), когда\n",
    "#      слишком много признаков при ограниченном количестве данных ухудшают модель.\n",
    "#    - Упрощение модели, иногда улучшение обобщающей способности за счет удаления шума.\n",
    "# 2. Визуализация Данных:\n",
    "#    - Уменьшение размерности данных до 2 или 3 позволяет визуализировать\n",
    "#      многомерные данные на плоскости или в пространстве, чтобы увидеть\n",
    "#      структуру, кластеры, выбросы.\n",
    "# 3. Шумоподавление:\n",
    "#    - Предполагается, что компоненты с малой дисперсией соответствуют шуму.\n",
    "#      Отбрасывая эти компоненты, можно \"очистить\" данные.\n",
    "# 4. Извлечение Признаков (Feature Extraction):\n",
    "#    - Главные компоненты можно рассматривать как новые, синтетические признаки,\n",
    "#      которые могут быть более информативными и менее коррелированными, чем исходные.\n",
    "# 5. Сжатие Данных:\n",
    "#    - Уменьшение размерности приводит к уменьшению объема данных для хранения.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: Как Работает PCA (Интуиция и Концепции)\n",
    "\n",
    "# 1. Дисперсия (Variance):\n",
    "#    - Мера разброса данных вдоль определенного направления. PCA ищет оси,\n",
    "#      вдоль которых этот разброс максимален.\n",
    "# 2. Ковариация (Covariance):\n",
    "#    - Мера того, как две переменные изменяются вместе. PCA использует\n",
    "#      ковариационную матрицу (или матрицу корреляций) для анализа\n",
    "#      взаимосвязей между исходными признаками.\n",
    "# 3. Главные Компоненты (Principal Components - PCs):\n",
    "#    - Это новые оси в пространстве данных.\n",
    "#    - Они являются **линейными комбинациями** исходных признаков.\n",
    "#    - Они **ортогональны** друг другу (некоррелированы).\n",
    "#    - Они упорядочены по убыванию объясняемой ими дисперсии:\n",
    "#      - PC1 объясняет наибольшую долю дисперсии данных.\n",
    "#      - PC2 объясняет наибольшую долю *оставшейся* дисперсии, будучи ортогональной PC1.\n",
    "#      - PC3 объясняет наибольшую долю *оставшейся* дисперсии, будучи ортогональной PC1 и PC2, и т.д.\n",
    "# 4. Собственные Векторы и Собственные Значения (Eigenvectors & Eigenvalues):\n",
    "#    - Математическая основа PCA.\n",
    "#    - Собственные векторы ковариационной матрицы данных определяют **направления** главных компонент.\n",
    "#    - Соответствующие им собственные значения определяют **величину дисперсии** данных вдоль этих направлений.\n",
    "#    - Главные компоненты - это собственные векторы, отсортированные по убыванию собственных значений.\n",
    "# 5. Проекция (Projection):\n",
    "#    - После нахождения главных компонент (новых осей), исходные данные\n",
    "#      проецируются на подпространство, образованное выбранным количеством\n",
    "#      первых (наиболее важных) главных компонент. Это и есть процесс\n",
    "#      снижения размерности.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Математические Шаги (Концептуально)\n",
    "\n",
    "# 1. **Стандартизация Данных:**\n",
    "#    - **Критически важный шаг!** PCA чувствителен к масштабу признаков.\n",
    "#    - Необходимо привести все признаки к одному масштабу, обычно путем\n",
    "#      стандартизации (вычитание среднего и деление на стандартное отклонение),\n",
    "#      чтобы каждый признак имел среднее 0 и дисперсию 1.\n",
    "# 2. **Вычисление Ковариационной Матрицы:**\n",
    "#    - Рассчитать ковариационную матрицу стандартизированных данных \\( \\Sigma \\).\n",
    "#      Размер матрицы будет `(n_features, n_features)`.\n",
    "# 3. **Вычисление Собственных Векторов и Значений:**\n",
    "#    - Найти собственные векторы \\( v \\) и собственные значения \\( \\lambda \\)\n",
    "#      ковариационной матрицы: \\( \\Sigma v = \\lambda v \\).\n",
    "# 4. **Сортировка:**\n",
    "#    - Отсортировать собственные значения по убыванию (\\( \\lambda_1 \\ge \\lambda_2 \\ge ... \\)).\n",
    "#    - Отсортировать соответствующие собственные векторы в том же порядке.\n",
    "# 5. **Выбор Главных Компонент:**\n",
    "#    - Выбрать первые `k` собственных векторов (где `k` - желаемая новая размерность),\n",
    "#      соответствующих `k` наибольшим собственным значениям.\n",
    "#    - Эти `k` векторов образуют матрицу проекции \\( W \\) размером `(n_features, k)`.\n",
    "# 6. **Трансформация Данных:**\n",
    "#    - Спроецировать стандартизированные исходные данные \\( X_{std} \\) на новое\n",
    "#      подпространство с помощью матрицы проекции:\n",
    "#      $$ X_{pca} = X_{std} W $$\n",
    "#    - \\( X_{pca} \\) - это данные в новом пространстве главных компонент,\n",
    "#      имеющие размерность `(n_samples, k)`.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Реализация в Scikit-learn\n",
    "\n",
    "# Основной класс: `sklearn.decomposition.PCA`\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler # Для обязательного масштабирования\n",
    "\n",
    "# --- Инициализация ---\n",
    "# pca = PCA(n_components=...)\n",
    "\n",
    "# Параметр `n_components`:\n",
    "# - `int`: Задать конкретное число компонент (например, `n_components=2`).\n",
    "# - `float` (от 0 до 1): Задать долю дисперсии, которую нужно сохранить\n",
    "#   (например, `n_components=0.95` сохранит компоненты, объясняющие 95% дисперсии).\n",
    "# - `None`: Сохранить все компоненты (`min(n_samples, n_features)`).\n",
    "# - `'mle'`: Использовать оценку максимального правдоподобия для выбора размерности.\n",
    "\n",
    "# --- Обучение и Трансформация ---\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_raw) # X_raw - исходные данные\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(X_scaled) # Обучение PCA (находит компоненты)\n",
    "# X_pca = pca.transform(X_scaled) # Применение преобразования\n",
    "\n",
    "# Или одной командой:\n",
    "# X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# --- Атрибуты Обученной Модели ---\n",
    "# - `pca.components_`: Массив формы `(n_components, n_features)`.\n",
    "#   Строки - это главные компоненты (собственные векторы).\n",
    "# - `pca.explained_variance_`: Массив формы `(n_components,)`.\n",
    "#   Объясненная дисперсия для каждой компоненты (собственные значения).\n",
    "# - `pca.explained_variance_ratio_`: Массив формы `(n_components,)`.\n",
    "#   Доля дисперсии, объясняемая каждой компонентой. Сумма по всем компонентам = 1.\n",
    "# - `pca.singular_values_`: Сингулярные значения (связаны с explained_variance_).\n",
    "# - `pca.mean_`: Средние значения признаков (если стандартизация делалась внутри PCA, что не рекомендуется).\n",
    "# - `pca.n_components_`: Итоговое количество выбранных компонент.\n",
    "\n",
    "# --- Обратное Преобразование ---\n",
    "# Можно попытаться восстановить данные в исходном пространстве (с потерей информации).\n",
    "# X_reconstructed = pca.inverse_transform(X_pca)\n",
    "# # X_reconstructed будет иметь ту же размерность, что и X_scaled, но будет приближением.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Преимущества и Недостатки PCA\n",
    "\n",
    "# Преимущества:\n",
    "# + **Простота и Скорость:** Алгоритм основан на стандартных линейных операциях (SVD), обычно работает быстро.\n",
    "# + **Детерминированность:** Результат всегда одинаков для одних и тех же данных.\n",
    "# + **Ортоганальность Компонент:** Новые признаки (главные компоненты) некоррелированы, что может быть полезно для некоторых моделей ML.\n",
    "# + **Эффективен для Линейных Структур:** Хорошо работает, когда основные зависимости в данных линейны.\n",
    "\n",
    "# Недостатки:\n",
    "# - **Чувствительность к Масштабу:** **Обязательно** требует масштабирования/стандартизации данных.\n",
    "# - **Линейность:** Не способен улавливать сложные нелинейные структуры в данных. Для этого нужны нелинейные методы (Kernel PCA, t-SNE, UMAP, Autoencoders).\n",
    "# - **Интерпретируемость Компонент:** Главные компоненты являются линейными комбинациями исходных признаков и часто теряют интуитивную интерпретируемость. Сложно сказать, что означает PC1 или PC2 в терминах исходных признаков.\n",
    "# - **Потеря Информации:** Снижение размерности всегда сопряжено с некоторой потерей информации (дисперсии).\n",
    "# - **Чувствительность к Выбросам:** Так как PCA максимизирует дисперсию, выбросы могут сильно влиять на направления главных компонент.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 7: Важные Соображения\n",
    "\n",
    "# 1. **Масштабирование Данных:** Повторимся - это **самый важный** шаг перед применением PCA. Используйте `StandardScaler`.\n",
    "# 2. **Выбор `n_components`:**\n",
    "#    - **Визуализация:** Обычно выбирают `n_components=2` или `n_components=3`.\n",
    "#    - **Сохранение Дисперсии:** Часто выбирают `n_components` так, чтобы сохранить определенный процент дисперсии (например, 90%, 95%, 99%). Можно построить график кумулятивной объясненной дисперсии (`np.cumsum(pca.explained_variance_ratio_)`) от числа компонент и найти \"локоть\" или точку, где достигается нужный процент.\n",
    "#    - **Для Моделей ML:** Иногда подбирают `n_components` как гиперпараметр с помощью кросс-валидации, чтобы максимизировать производительность последующей модели.\n",
    "# 3. **Интерпретация:** Помните, что главные компоненты - это абстрактные направления. Анализ весов в `pca.components_` может дать некоторое представление о вкладе исходных признаков в каждую компоненту, но интерпретация остается сложной.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 8: Пример Задачи и Решения (Визуализация Рукописных Цифр)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Использовать PCA для снижения размерности датасета рукописных цифр\n",
    "# (Digits dataset, 64 признака - 8x8 пикселей) до 2 измерений и визуализировать\n",
    "# результат, чтобы увидеть, разделяются ли классы цифр в новом пространстве.\n",
    "\n",
    "# --- Решение (Полный Код) ---\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 1. Загрузка Данных\n",
    "digits = load_digits()\n",
    "X = digits.data # Признаки (64 пикселя)\n",
    "y = digits.target # Метки (цифры от 0 до 9)\n",
    "n_samples, n_features = X.shape\n",
    "n_digits = len(np.unique(y))\n",
    "\n",
    "# print(f\"Dataset shape: {X.shape}\")\n",
    "# print(f\"Number of unique digits: {n_digits}\")\n",
    "# print(f\"Sample data point (first digit, flattened): {X[0]}\")\n",
    "\n",
    "# 2. Масштабирование Данных (ОБЯЗАТЕЛЬНО!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# print(f\"\\nMean after scaling (should be close to 0): {X_scaled.mean():.2f}\")\n",
    "# print(f\"Std dev after scaling (should be close to 1): {X_scaled.std():.2f}\")\n",
    "\n",
    "# 3. Применение PCA для снижения до 2 компонент\n",
    "n_components_pca = 2\n",
    "pca = PCA(n_components=n_components_pca, random_state=42)\n",
    "\n",
    "# print(f\"\\nApplying PCA to reduce dimensions to {n_components_pca}...\")\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "# print(\"PCA transformation complete.\")\n",
    "# print(f\"Shape after PCA: {X_pca.shape}\") # (n_samples, n_components_pca)\n",
    "\n",
    "# 4. Анализ Объясненной Дисперсии\n",
    "# print(f\"\\nExplained variance ratio by component: {pca.explained_variance_ratio_}\")\n",
    "# print(f\"Total explained variance by {n_components_pca} components: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "# # Увидим, что 2 компоненты объясняют лишь часть всей дисперсии\n",
    "\n",
    "# 5. Визуализация Результатов\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap=plt.cm.get_cmap(\"jet\", n_digits), alpha=0.7, s=15)\n",
    "# plt.title(f'Digits Dataset PCA Projection ({n_components_pca} Components)')\n",
    "# plt.xlabel('Principal Component 1')\n",
    "# plt.ylabel('Principal Component 2')\n",
    "# plt.legend(handles=scatter.legend_elements()[0], labels=digits.target_names, title=\"Digits\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "# # На графике должно быть видно, что разные цифры образуют относительно разделенные кластеры\n",
    "# # даже в 2D пространстве, полученном с помощью PCA.\n",
    "\n",
    "# 6. (Опционально) График для выбора n_components\n",
    "# pca_full = PCA(random_state=42) # PCA со всеми компонентами\n",
    "# pca_full.fit(X_scaled)\n",
    "# explained_variance_cumulative = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "#\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(range(1, len(explained_variance_cumulative) + 1), explained_variance_cumulative, marker='.', linestyle='--')\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "# plt.title('Explained Variance by Number of PCA Components')\n",
    "# plt.grid(True)\n",
    "# # Добавим линии для 95% и 99% дисперсии\n",
    "# plt.axhline(y=0.95, color='r', linestyle=':', label='95% Explained Variance')\n",
    "# plt.axhline(y=0.99, color='g', linestyle=':', label='99% Explained Variance')\n",
    "# plt.legend(loc='best')\n",
    "# plt.show()\n",
    "# # Этот график помогает определить, сколько компонент нужно для сохранения нужной доли информации.\n",
    "\n",
    "# --- Конец Примера ---\n",
    "\n",
    "# --------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Блок 1: Зачем Нужно Масштабирование Признаков (Feature Scaling)?\n",
    "\n",
    "# Что такое Масштабирование Признаков?\n",
    "# Это процесс преобразования числовых признаков в наборе данных так, чтобы они\n",
    "# находились в определенном, общем диапазоне или имели определенные статистические свойства.\n",
    "\n",
    "# Зачем это Нужно?\n",
    "# Многие алгоритмы машинного обучения чувствительны к масштабу входных признаков.\n",
    "# Признаки с большими значениями могут оказывать непропорционально большое влияние\n",
    "# на результат обучения и предсказания по сравнению с признаками с малыми значениями.\n",
    "# Масштабирование помогает:\n",
    "# 1. Улучшить Сходимость Алгоритмов: Особенно для тех, что используют градиентный спуск\n",
    "#    (например, Линейная/Логистическая регрессия, Нейронные сети). Сходимость будет быстрее и стабильнее.\n",
    "# 2. Улучшить Производительность Моделей: Для алгоритмов, основанных на расстоянии\n",
    "#    (например, K-Means, KNN, SVM с некоторыми ядрами) или использующих регуляризацию (Ridge, Lasso),\n",
    "#    масштабирование критически важно, так как расстояние или штрафы будут зависеть от масштаба.\n",
    "# 3. Обеспечить Равный Вклад Признаков: Предотвращает доминирование признаков с большими значениями.\n",
    "# 4. Необходимость для PCA: PCA ищет направления максимальной дисперсии, поэтому он очень\n",
    "#    чувствителен к масштабу исходных признаков.\n",
    "\n",
    "# Какие Алгоритмы Менее Чувствительны?\n",
    "# - Деревья Решений и Ансамбли на их основе (Random Forest, Gradient Boosting):\n",
    "#   Они работают путем поиска порогов для разбиения по каждому признаку независимо,\n",
    "#   поэтому масштаб обычно не влияет на их производительность. Масштабирование\n",
    "#   для них, как правило, не требуется (и иногда может даже немного ухудшить результат).\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 2: StandardScaler\n",
    "\n",
    "# Принцип Работы:\n",
    "# Стандартизация преобразует данные так, чтобы они имели **среднее значение 0**\n",
    "# и **стандартное отклонение 1**.\n",
    "# Формула для каждого признака \\( x_j \\):\n",
    "# $$ z_j = \\frac{x_j - \\mu_j}{\\sigma_j} $$\n",
    "# Где:\n",
    "# - \\( z_j \\) - стандартизированное значение признака.\n",
    "# - \\( x_j \\) - исходное значение признака.\n",
    "# - \\( \\mu_j \\) - среднее значение признака \\( j \\) (вычисленное на обучающих данных).\n",
    "# - \\( \\sigma_j \\) - стандартное отклонение признака \\( j \\) (вычисленное на обучающих данных).\n",
    "\n",
    "# Результат:\n",
    "# - Данные центрированы вокруг нуля.\n",
    "# - Распределение данных сохраняется (форма гистограммы не меняется).\n",
    "# - Диапазон значений не ограничен строго (могут быть значения > 1 или < -1).\n",
    "\n",
    "# Когда Использовать:\n",
    "# - Когда данные имеют распределение, близкое к нормальному (Гауссову), хотя это не строгое требование.\n",
    "# - Для алгоритмов, которые предполагают, что признаки центрированы около нуля или имеют единичную дисперсию (например, Линейная регрессия с регуляризацией, SVM, PCA, Нейронные сети).\n",
    "# - Менее чувствителен к выбросам по сравнению с MinMaxScaler, так как не использует min/max.\n",
    "\n",
    "# Реализация в Scikit-learn:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler_std = StandardScaler()\n",
    "# # Обучение (вычисление mean и std) и трансформация обучающих данных:\n",
    "# # X_train_scaled = scaler_std.fit_transform(X_train)\n",
    "# # Трансформация тестовых/новых данных (используя mean и std от X_train):\n",
    "# # X_test_scaled = scaler_std.transform(X_test)\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 3: MinMaxScaler\n",
    "\n",
    "# Принцип Работы:\n",
    "# Нормализация (Min-Max Scaling) масштабирует данные так, чтобы они находились\n",
    "# в заданном диапазоне, обычно **[0, 1]** (или [-1, 1]).\n",
    "# Формула для каждого признака \\( x_j \\) (для диапазона [0, 1]):\n",
    "# $$ x'_{j} = \\frac{x_j - \\min(x_j)}{\\max(x_j) - \\min(x_j)} $$\n",
    "# Где:\n",
    "# - \\( x'_{j} \\) - нормализованное значение признака.\n",
    "# - \\( x_j \\) - исходное значение признака.\n",
    "# - \\( \\min(x_j) \\) - минимальное значение признака \\( j \\) (на обучающих данных).\n",
    "# - \\( \\max(x_j) \\) - максимальное значение признака \\( j \\) (на обучающих данных).\n",
    "\n",
    "# Результат:\n",
    "# - Все значения признаков находятся в диапазоне [0, 1] (или другом заданном).\n",
    "# - Сохраняет форму исходного распределения.\n",
    "# - Сохраняет относительные расстояния между точками в пределах диапазона.\n",
    "\n",
    "# Когда Использовать:\n",
    "# - Когда требуется, чтобы признаки находились в строго определенном диапазоне (например, для некоторых нейронных сетей или визуализаций).\n",
    "# - Для алгоритмов, не делающих предположений о распределении данных (например, KNN).\n",
    "# - **Очень чувствителен к выбросам:** Экстремальные значения (min или max) могут сильно \"сжать\" остальные данные в небольшой части диапазона.\n",
    "\n",
    "# Реализация в Scikit-learn:\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# scaler_minmax = MinMaxScaler(feature_range=(0, 1)) # Диапазон по умолчанию [0, 1]\n",
    "# # Обучение (вычисление min и max) и трансформация обучающих данных:\n",
    "# # X_train_scaled = scaler_minmax.fit_transform(X_train)\n",
    "# # Трансформация тестовых/новых данных (используя min и max от X_train):\n",
    "# # X_test_scaled = scaler_minmax.transform(X_test)\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 4: Как Использовать Скалеры (Workflow)\n",
    "\n",
    "# **Критически Важный Принцип: Fit только на Обучающих Данных!**\n",
    "# Параметры масштабирования (среднее/ст.откл. для StandardScaler, min/max для MinMaxScaler)\n",
    "# должны вычисляться **только** на обучающей выборке (`X_train`).\n",
    "# Затем эти же вычисленные параметры используются для трансформации и обучающей (`X_train`),\n",
    "# и валидационной (`X_val`), и тестовой (`X_test`), и новых данных для предсказания.\n",
    "# Это предотвращает \"утечку данных\" (data leakage) из тестовой выборки в процесс обучения.\n",
    "\n",
    "# Шаги:\n",
    "# 1. Разделить данные на обучающую и тестовую выборки (`train_test_split`).\n",
    "# 2. Создать экземпляр скалера (`StandardScaler()` или `MinMaxScaler()`).\n",
    "# 3. Обучить скалер на обучающих данных: `scaler.fit(X_train)`.\n",
    "# 4. Трансформировать обучающие данные: `X_train_scaled = scaler.transform(X_train)`.\n",
    "#    (Шаги 3 и 4 можно объединить: `X_train_scaled = scaler.fit_transform(X_train)`).\n",
    "# 5. Трансформировать тестовые данные (используя параметры, выученные на шаге 3):\n",
    "#    `X_test_scaled = scaler.transform(X_test)`.\n",
    "# 6. Обучить модель машинного обучения на `X_train_scaled` и `y_train`.\n",
    "# 7. Оценить модель на `X_test_scaled` и `y_test`.\n",
    "# 8. Для предсказания на новых данных: `X_new_scaled = scaler.transform(X_new)`, затем `model.predict(X_new_scaled)`.\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 5: Обратное Преобразование (Inverse Transformation)\n",
    "\n",
    "# Зачем Нужно?\n",
    "# Иногда после получения предсказаний от модели, обученной на масштабированных данных,\n",
    "# или после применения методов типа PCA, может потребоваться вернуть данные\n",
    "# (или результаты) обратно в исходный масштаб для:\n",
    "# - Интерпретации результатов (например, предсказанная цена в долларах, а не в стандартизированных единицах).\n",
    "# - Сравнения с исходными данными.\n",
    "# - Визуализации в исходном масштабе.\n",
    "\n",
    "# Как Сделать?\n",
    "# Обученные скалеры (`StandardScaler`, `MinMaxScaler`) имеют метод `inverse_transform()`.\n",
    "# Он использует параметры, вычисленные во время `fit()` (среднее/ст.откл. или min/max),\n",
    "# чтобы выполнить обратное математическое преобразование.\n",
    "\n",
    "# Пример:\n",
    "# # Предположим, scaler был обучен на X_train\n",
    "# # X_train_scaled = scaler.fit_transform(X_train)\n",
    "# # X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Обратное преобразование масштабированных данных\n",
    "# X_train_original_scale = scaler.inverse_transform(X_train_scaled)\n",
    "# X_test_original_scale = scaler.inverse_transform(X_test_scaled)\n",
    "\n",
    "# # X_train_original_scale должен быть (почти) идентичен X_train\n",
    "# # X_test_original_scale должен быть (почти) идентичен X_test\n",
    "# # (Небольшие различия возможны из-за точности вычислений с плавающей точкой)\n",
    "\n",
    "# # Если у вас есть предсказания Y_pred, сделанные на масштабированных X,\n",
    "# # и вы хотите вернуть их в исходный масштаб Y (если Y тоже масштабировался),\n",
    "# # вам нужен скалер, обученный на Y_train:\n",
    "# # y_scaler = StandardScaler() # или MinMaxScaler\n",
    "# # y_train_scaled = y_scaler.fit_transform(y_train.reshape(-1, 1)) # reshape для 1D -> 2D\n",
    "# # ... обучение модели на X_train_scaled -> y_train_scaled ...\n",
    "# # y_pred_scaled = model.predict(X_test_scaled)\n",
    "# # y_pred_original_scale = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 6: Пример Задачи и Решения (Применение и Обратное Преобразование)\n",
    "\n",
    "# --- Условие Задачи ---\n",
    "# Задача: Сгенерировать 2D данные. Разделить их на train/test.\n",
    "# Применить StandardScaler и MinMaxScaler. Показать результаты масштабирования.\n",
    "# Затем применить обратное преобразование и убедиться, что данные вернулись к исходному масштабу.\n",
    "\n",
    "# --- Решение (Полный Код) ---\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Генерация Синтетических Данных\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame({\n",
    "    'Feature1': np.random.rand(100) * 100, # Диапазон ~[0, 100]\n",
    "    'Feature2': np.random.normal(50, 10, 100) # Нормальное распр. mean=50, std=10\n",
    "})\n",
    "# print(\"Original Data Sample (first 5 rows):\")\n",
    "# print(data.head())\n",
    "# print(\"\\nOriginal Data Description:\")\n",
    "# print(data.describe())\n",
    "\n",
    "# 2. Разделение Данных\n",
    "X_train_df, X_test_df = train_test_split(data, test_size=0.25, random_state=42)\n",
    "# print(f\"\\nTrain set size: {X_train_df.shape[0]}, Test set size: {X_test_df.shape[0]}\")\n",
    "\n",
    "# 3. Применение StandardScaler\n",
    "scaler_std = StandardScaler()\n",
    "# Fit на трейне, transform на трейне и тесте\n",
    "X_train_std_scaled = scaler_std.fit_transform(X_train_df)\n",
    "X_test_std_scaled = scaler_std.transform(X_test_df)\n",
    "\n",
    "# Конвертируем обратно в DataFrame для удобства просмотра\n",
    "X_train_std_scaled_df = pd.DataFrame(X_train_std_scaled, columns=data.columns, index=X_train_df.index)\n",
    "X_test_std_scaled_df = pd.DataFrame(X_test_std_scaled, columns=data.columns, index=X_test_df.index)\n",
    "\n",
    "# print(\"\\n--- StandardScaler Results ---\")\n",
    "# print(\"Scaled Training Data Sample (first 5 rows):\")\n",
    "# print(X_train_std_scaled_df.head())\n",
    "# print(\"\\nScaled Training Data Description:\")\n",
    "# print(X_train_std_scaled_df.describe()) # Mean ~0, Std ~1\n",
    "\n",
    "# 4. Применение MinMaxScaler\n",
    "scaler_mm = MinMaxScaler(feature_range=(0, 1)) # Диапазон [0, 1]\n",
    "# Fit на трейне, transform на трейне и тесте\n",
    "X_train_mm_scaled = scaler_mm.fit_transform(X_train_df)\n",
    "X_test_mm_scaled = scaler_mm.transform(X_test_df)\n",
    "\n",
    "X_train_mm_scaled_df = pd.DataFrame(X_train_mm_scaled, columns=data.columns, index=X_train_df.index)\n",
    "X_test_mm_scaled_df = pd.DataFrame(X_test_mm_scaled, columns=data.columns, index=X_test_df.index)\n",
    "\n",
    "# print(\"\\n--- MinMaxScaler Results ---\")\n",
    "# print(\"Scaled Training Data Sample (first 5 rows):\")\n",
    "# print(X_train_mm_scaled_df.head())\n",
    "# print(\"\\nScaled Training Data Description:\")\n",
    "# print(X_train_mm_scaled_df.describe()) # Min ~0, Max ~1\n",
    "\n",
    "# 5. Обратное Преобразование (Inverse Transform)\n",
    "# StandardScaler\n",
    "X_train_std_inversed = scaler_std.inverse_transform(X_train_std_scaled)\n",
    "X_test_std_inversed = scaler_std.inverse_transform(X_test_std_scaled)\n",
    "X_train_std_inversed_df = pd.DataFrame(X_train_std_inversed, columns=data.columns, index=X_train_df.index)\n",
    "\n",
    "# MinMaxScaler\n",
    "X_train_mm_inversed = scaler_mm.inverse_transform(X_train_mm_scaled)\n",
    "X_test_mm_inversed = scaler_mm.inverse_transform(X_test_mm_scaled)\n",
    "X_train_mm_inversed_df = pd.DataFrame(X_train_mm_inversed, columns=data.columns, index=X_train_df.index)\n",
    "\n",
    "# print(\"\\n--- Inverse Transform Results ---\")\n",
    "# print(\"Original Training Data Sample (first 5 rows):\")\n",
    "# print(X_train_df.head())\n",
    "# print(\"\\nStandardScaler Inversed Training Data Sample (first 5 rows):\")\n",
    "# print(X_train_std_inversed_df.head()) # Должно совпадать с оригиналом\n",
    "# print(\"\\nMinMaxScaler Inversed Training Data Sample (first 5 rows):\")\n",
    "# print(X_train_mm_inversed_df.head()) # Должно совпадать с оригиналом\n",
    "\n",
    "# Проверка совпадения (с учетом точности float)\n",
    "# print(\"\\nCheck if inversed data matches original (StandardScaler):\")\n",
    "# print(np.allclose(X_train_df.values, X_train_std_inversed))\n",
    "# print(\"Check if inversed data matches original (MinMaxScaler):\")\n",
    "# print(np.allclose(X_train_df.values, X_train_mm_inversed))\n",
    "\n",
    "# 6. Визуализация (опционально)\n",
    "# fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "# axes[0].scatter(X_train_df['Feature1'], X_train_df['Feature2'])\n",
    "# axes[0].set_title('Original Data')\n",
    "# axes[0].grid(True)\n",
    "# axes[1].scatter(X_train_std_scaled_df['Feature1'], X_train_std_scaled_df['Feature2'])\n",
    "# axes[1].set_title('StandardScaler Data')\n",
    "# axes[1].grid(True)\n",
    "# axes[2].scatter(X_train_mm_scaled_df['Feature1'], X_train_mm_scaled_df['Feature2'])\n",
    "# axes[2].set_title('MinMaxScaler Data')\n",
    "# axes[2].grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# --- Конец Примера ---\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# Блок 7: Выбор Между StandardScaler и MinMaxScaler\n",
    "\n",
    "# - **StandardScaler:**\n",
    "#   - Предпочтительнее, если алгоритм делает предположения о данных (нулевое среднее, единичная дисперсия), как PCA или линейные модели с регуляризацией.\n",
    "#   - Менее чувствителен к выбросам.\n",
    "#   - Не приводит данные к строгому диапазону.\n",
    "# - **MinMaxScaler:**\n",
    "#   - Полезен, когда нужен строгий диапазон [0, 1] (например, для обработки изображений, некоторых нейросетей).\n",
    "#   - Хорош для алгоритмов, не делающих предположений о распределении.\n",
    "#   - **Сильно подвержен влиянию выбросов.** Если есть выбросы, их лучше обработать до масштабирования или использовать более робастный скалер (например, `RobustScaler` из sklearn, который использует медиану и межквартильный размах).\n",
    "\n",
    "# Часто стоит попробовать оба скалера и посмотреть, какой дает лучшие результаты для вашей конкретной модели и данных. Не забывайте про `RobustScaler` как альтернативу при наличии выбросов.\n",
    "# --------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
